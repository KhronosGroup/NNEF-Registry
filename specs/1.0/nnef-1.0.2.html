<!DOCTYPE html>
<html lang="en">
<head>
<meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Asciidoctor 2.0.10">
<meta name="author" content="The Khronos NNEF Working Group">
<title>Neural Network Exchange Format</title>
<link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700">
<style>
/* Asciidoctor default stylesheet | MIT License | https://asciidoctor.org */
/* Uncomment @import statement to use as custom stylesheet */
/*@import "https://fonts.googleapis.com/css?family=Open+Sans:300,300italic,400,400italic,600,600italic%7CNoto+Serif:400,400italic,700,700italic%7CDroid+Sans+Mono:400,700";*/
article,aside,details,figcaption,figure,footer,header,hgroup,main,nav,section{display:block}
audio,video{display:inline-block}
audio:not([controls]){display:none;height:0}
html{font-family:sans-serif;-ms-text-size-adjust:100%;-webkit-text-size-adjust:100%}
a{background:none}
a:focus{outline:thin dotted}
a:active,a:hover{outline:0}
h1{font-size:2em;margin:.67em 0}
abbr[title]{border-bottom:1px dotted}
b,strong{font-weight:bold}
dfn{font-style:italic}
hr{-moz-box-sizing:content-box;box-sizing:content-box;height:0}
mark{background:#ff0;color:#000}
code,kbd,pre,samp{font-family:monospace;font-size:1em}
pre{white-space:pre-wrap}
q{quotes:"\201C" "\201D" "\2018" "\2019"}
small{font-size:80%}
sub,sup{font-size:75%;line-height:0;position:relative;vertical-align:baseline}
sup{top:-.5em}
sub{bottom:-.25em}
img{border:0}
svg:not(:root){overflow:hidden}
figure{margin:0}
fieldset{border:1px solid silver;margin:0 2px;padding:.35em .625em .75em}
legend{border:0;padding:0}
button,input,select,textarea{font-family:inherit;font-size:100%;margin:0}
button,input{line-height:normal}
button,select{text-transform:none}
button,html input[type="button"],input[type="reset"],input[type="submit"]{-webkit-appearance:button;cursor:pointer}
button[disabled],html input[disabled]{cursor:default}
input[type="checkbox"],input[type="radio"]{box-sizing:border-box;padding:0}
button::-moz-focus-inner,input::-moz-focus-inner{border:0;padding:0}
textarea{overflow:auto;vertical-align:top}
table{border-collapse:collapse;border-spacing:0}
*,*::before,*::after{-moz-box-sizing:border-box;-webkit-box-sizing:border-box;box-sizing:border-box}
html,body{font-size:100%}
body{background:#fff;color:rgba(0,0,0,.8);padding:0;margin:0;font-family:"Noto Serif","DejaVu Serif",serif;font-weight:400;font-style:normal;line-height:1;position:relative;cursor:auto;tab-size:4;-moz-osx-font-smoothing:grayscale;-webkit-font-smoothing:antialiased}
a:hover{cursor:pointer}
img,object,embed{max-width:100%;height:auto}
object,embed{height:100%}
img{-ms-interpolation-mode:bicubic}
.left{float:left!important}
.right{float:right!important}
.text-left{text-align:left!important}
.text-right{text-align:right!important}
.text-center{text-align:center!important}
.text-justify{text-align:justify!important}
.hide{display:none}
img,object,svg{display:inline-block;vertical-align:middle}
textarea{height:auto;min-height:50px}
select{width:100%}
.center{margin-left:auto;margin-right:auto}
.stretch{width:100%}
.subheader,.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{line-height:1.45;color:#7a2518;font-weight:400;margin-top:0;margin-bottom:.25em}
div,dl,dt,dd,ul,ol,li,h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6,pre,form,p,blockquote,th,td{margin:0;padding:0;direction:ltr}
a{color:#2156a5;text-decoration:underline;line-height:inherit}
a:hover,a:focus{color:#1d4b8f}
a img{border:0}
p{font-family:inherit;font-weight:400;font-size:1em;line-height:1.6;margin-bottom:1.25em;text-rendering:optimizeLegibility}
p aside{font-size:.875em;line-height:1.35;font-style:italic}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{font-family:"Open Sans","DejaVu Sans",sans-serif;font-weight:300;font-style:normal;color:#ba3925;text-rendering:optimizeLegibility;margin-top:1em;margin-bottom:.5em;line-height:1.0125em}
h1 small,h2 small,h3 small,#toctitle small,.sidebarblock>.content>.title small,h4 small,h5 small,h6 small{font-size:60%;color:#e99b8f;line-height:0}
h1{font-size:2.125em}
h2{font-size:1.6875em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.375em}
h4,h5{font-size:1.125em}
h6{font-size:1em}
hr{border:solid #dddddf;border-width:1px 0 0;clear:both;margin:1.25em 0 1.1875em;height:0}
em,i{font-style:italic;line-height:inherit}
strong,b{font-weight:bold;line-height:inherit}
small{font-size:60%;line-height:inherit}
code{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;font-weight:400;color:rgba(0,0,0,.9)}
ul,ol,dl{font-size:1em;line-height:1.6;margin-bottom:1.25em;list-style-position:outside;font-family:inherit}
ul,ol{margin-left:1.5em}
ul li ul,ul li ol{margin-left:1.25em;margin-bottom:0;font-size:1em}
ul.square li ul,ul.circle li ul,ul.disc li ul{list-style:inherit}
ul.square{list-style-type:square}
ul.circle{list-style-type:circle}
ul.disc{list-style-type:disc}
ol li ul,ol li ol{margin-left:1.25em;margin-bottom:0}
dl dt{margin-bottom:.3125em;font-weight:bold}
dl dd{margin-bottom:1.25em}
abbr,acronym{text-transform:uppercase;font-size:90%;color:rgba(0,0,0,.8);border-bottom:1px dotted #ddd;cursor:help}
abbr{text-transform:none}
blockquote{margin:0 0 1.25em;padding:.5625em 1.25em 0 1.1875em;border-left:1px solid #ddd}
blockquote cite{display:block;font-size:.9375em;color:rgba(0,0,0,.6)}
blockquote cite::before{content:"\2014 \0020"}
blockquote cite a,blockquote cite a:visited{color:rgba(0,0,0,.6)}
blockquote,blockquote p{line-height:1.6;color:rgba(0,0,0,.85)}
@media screen and (min-width:768px){h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2}
h1{font-size:2.75em}
h2{font-size:2.3125em}
h3,#toctitle,.sidebarblock>.content>.title{font-size:1.6875em}
h4{font-size:1.4375em}}
table{background:#fff;margin-bottom:1.25em;border:solid 1px #dedede}
table thead,table tfoot{background:#f7f8f7}
table thead tr th,table thead tr td,table tfoot tr th,table tfoot tr td{padding:.5em .625em .625em;font-size:inherit;color:rgba(0,0,0,.8);text-align:left}
table tr th,table tr td{padding:.5625em .625em;font-size:inherit;color:rgba(0,0,0,.8)}
table tr.even,table tr.alt{background:#f8f8f7}
table thead tr th,table tfoot tr th,table tbody tr td,table tr td,table tfoot tr td{display:table-cell;line-height:1.6}
h1,h2,h3,#toctitle,.sidebarblock>.content>.title,h4,h5,h6{line-height:1.2;word-spacing:-.05em}
h1 strong,h2 strong,h3 strong,#toctitle strong,.sidebarblock>.content>.title strong,h4 strong,h5 strong,h6 strong{font-weight:400}
.clearfix::before,.clearfix::after,.float-group::before,.float-group::after{content:" ";display:table}
.clearfix::after,.float-group::after{clear:both}
:not(pre):not([class^=L])>code{font-size:.9375em;font-style:normal!important;letter-spacing:0;padding:.1em .5ex;word-spacing:-.15em;background:#f7f7f8;-webkit-border-radius:4px;border-radius:4px;line-height:1.45;text-rendering:optimizeSpeed;word-wrap:break-word}
:not(pre)>code.nobreak{word-wrap:normal}
:not(pre)>code.nowrap{white-space:nowrap}
pre{color:rgba(0,0,0,.9);font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;line-height:1.45;text-rendering:optimizeSpeed}
pre code,pre pre{color:inherit;font-size:inherit;line-height:inherit}
pre>code{display:block}
pre.nowrap,pre.nowrap pre{white-space:pre;word-wrap:normal}
em em{font-style:normal}
strong strong{font-weight:400}
.keyseq{color:rgba(51,51,51,.8)}
kbd{font-family:"Droid Sans Mono","DejaVu Sans Mono",monospace;display:inline-block;color:rgba(0,0,0,.8);font-size:.65em;line-height:1.45;background:#f7f7f7;border:1px solid #ccc;-webkit-border-radius:3px;border-radius:3px;-webkit-box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em white inset;box-shadow:0 1px 0 rgba(0,0,0,.2),0 0 0 .1em #fff inset;margin:0 .15em;padding:.2em .5em;vertical-align:middle;position:relative;top:-.1em;white-space:nowrap}
.keyseq kbd:first-child{margin-left:0}
.keyseq kbd:last-child{margin-right:0}
.menuseq,.menuref{color:#000}
.menuseq b:not(.caret),.menuref{font-weight:inherit}
.menuseq{word-spacing:-.02em}
.menuseq b.caret{font-size:1.25em;line-height:.8}
.menuseq i.caret{font-weight:bold;text-align:center;width:.45em}
b.button::before,b.button::after{position:relative;top:-1px;font-weight:400}
b.button::before{content:"[";padding:0 3px 0 2px}
b.button::after{content:"]";padding:0 2px 0 3px}
p a>code:hover{color:rgba(0,0,0,.9)}
#header,#content,#footnotes,#footer{width:100%;margin-left:auto;margin-right:auto;margin-top:0;margin-bottom:0;max-width:62.5em;*zoom:1;position:relative;padding-left:.9375em;padding-right:.9375em}
#header::before,#header::after,#content::before,#content::after,#footnotes::before,#footnotes::after,#footer::before,#footer::after{content:" ";display:table}
#header::after,#content::after,#footnotes::after,#footer::after{clear:both}
#content{margin-top:1.25em}
#content::before{content:none}
#header>h1:first-child{color:rgba(0,0,0,.85);margin-top:2.25rem;margin-bottom:0}
#header>h1:first-child+#toc{margin-top:8px;border-top:1px solid #dddddf}
#header>h1:only-child,body.toc2 #header>h1:nth-last-child(2){border-bottom:1px solid #dddddf;padding-bottom:8px}
#header .details{border-bottom:1px solid #dddddf;line-height:1.45;padding-top:.25em;padding-bottom:.25em;padding-left:.25em;color:rgba(0,0,0,.6);display:-ms-flexbox;display:-webkit-flex;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap}
#header .details span:first-child{margin-left:-.125em}
#header .details span.email a{color:rgba(0,0,0,.85)}
#header .details br{display:none}
#header .details br+span::before{content:"\00a0\2013\00a0"}
#header .details br+span.author::before{content:"\00a0\22c5\00a0";color:rgba(0,0,0,.85)}
#header .details br+span#revremark::before{content:"\00a0|\00a0"}
#header #revnumber{text-transform:capitalize}
#header #revnumber::after{content:"\00a0"}
#content>h1:first-child:not([class]){color:rgba(0,0,0,.85);border-bottom:1px solid #dddddf;padding-bottom:8px;margin-top:0;padding-top:1rem;margin-bottom:1.25rem}
#toc{border-bottom:1px solid #e7e7e9;padding-bottom:.5em}
#toc>ul{margin-left:.125em}
#toc ul.sectlevel0>li>a{font-style:italic}
#toc ul.sectlevel0 ul.sectlevel1{margin:.5em 0}
#toc ul{font-family:"Open Sans","DejaVu Sans",sans-serif;list-style-type:none}
#toc li{line-height:1.3334;margin-top:.3334em}
#toc a{text-decoration:none}
#toc a:active{text-decoration:underline}
#toctitle{color:#7a2518;font-size:1.2em}
@media screen and (min-width:768px){#toctitle{font-size:1.375em}
body.toc2{padding-left:15em;padding-right:0}
#toc.toc2{margin-top:0!important;background:#f8f8f7;position:fixed;width:15em;left:0;top:0;border-right:1px solid #e7e7e9;border-top-width:0!important;border-bottom-width:0!important;z-index:1000;padding:1.25em 1em;height:100%;overflow:auto}
#toc.toc2 #toctitle{margin-top:0;margin-bottom:.8rem;font-size:1.2em}
#toc.toc2>ul{font-size:.9em;margin-bottom:0}
#toc.toc2 ul ul{margin-left:0;padding-left:1em}
#toc.toc2 ul.sectlevel0 ul.sectlevel1{padding-left:0;margin-top:.5em;margin-bottom:.5em}
body.toc2.toc-right{padding-left:0;padding-right:15em}
body.toc2.toc-right #toc.toc2{border-right-width:0;border-left:1px solid #e7e7e9;left:auto;right:0}}
@media screen and (min-width:1280px){body.toc2{padding-left:20em;padding-right:0}
#toc.toc2{width:20em}
#toc.toc2 #toctitle{font-size:1.375em}
#toc.toc2>ul{font-size:.95em}
#toc.toc2 ul ul{padding-left:1.25em}
body.toc2.toc-right{padding-left:0;padding-right:20em}}
#content #toc{border-style:solid;border-width:1px;border-color:#e0e0dc;margin-bottom:1.25em;padding:1.25em;background:#f8f8f7;-webkit-border-radius:4px;border-radius:4px}
#content #toc>:first-child{margin-top:0}
#content #toc>:last-child{margin-bottom:0}
#footer{max-width:100%;background:rgba(0,0,0,.8);padding:1.25em}
#footer-text{color:rgba(255,255,255,.8);line-height:1.44}
#content{margin-bottom:.625em}
.sect1{padding-bottom:.625em}
@media screen and (min-width:768px){#content{margin-bottom:1.25em}
.sect1{padding-bottom:1.25em}}
.sect1:last-child{padding-bottom:0}
.sect1+.sect1{border-top:1px solid #e7e7e9}
#content h1>a.anchor,h2>a.anchor,h3>a.anchor,#toctitle>a.anchor,.sidebarblock>.content>.title>a.anchor,h4>a.anchor,h5>a.anchor,h6>a.anchor{position:absolute;z-index:1001;width:1.5ex;margin-left:-1.5ex;display:block;text-decoration:none!important;visibility:hidden;text-align:center;font-weight:400}
#content h1>a.anchor::before,h2>a.anchor::before,h3>a.anchor::before,#toctitle>a.anchor::before,.sidebarblock>.content>.title>a.anchor::before,h4>a.anchor::before,h5>a.anchor::before,h6>a.anchor::before{content:"\00A7";font-size:.85em;display:block;padding-top:.1em}
#content h1:hover>a.anchor,#content h1>a.anchor:hover,h2:hover>a.anchor,h2>a.anchor:hover,h3:hover>a.anchor,#toctitle:hover>a.anchor,.sidebarblock>.content>.title:hover>a.anchor,h3>a.anchor:hover,#toctitle>a.anchor:hover,.sidebarblock>.content>.title>a.anchor:hover,h4:hover>a.anchor,h4>a.anchor:hover,h5:hover>a.anchor,h5>a.anchor:hover,h6:hover>a.anchor,h6>a.anchor:hover{visibility:visible}
#content h1>a.link,h2>a.link,h3>a.link,#toctitle>a.link,.sidebarblock>.content>.title>a.link,h4>a.link,h5>a.link,h6>a.link{color:#ba3925;text-decoration:none}
#content h1>a.link:hover,h2>a.link:hover,h3>a.link:hover,#toctitle>a.link:hover,.sidebarblock>.content>.title>a.link:hover,h4>a.link:hover,h5>a.link:hover,h6>a.link:hover{color:#a53221}
details,.audioblock,.imageblock,.literalblock,.listingblock,.stemblock,.videoblock{margin-bottom:1.25em}
details>summary:first-of-type{cursor:pointer;display:list-item;outline:none;margin-bottom:.75em}
.admonitionblock td.content>.title,.audioblock>.title,.exampleblock>.title,.imageblock>.title,.listingblock>.title,.literalblock>.title,.stemblock>.title,.openblock>.title,.paragraph>.title,.quoteblock>.title,table.tableblock>.title,.verseblock>.title,.videoblock>.title,.dlist>.title,.olist>.title,.ulist>.title,.qlist>.title,.hdlist>.title{text-rendering:optimizeLegibility;text-align:left;font-family:"Noto Serif","DejaVu Serif",serif;font-size:1rem;font-style:italic}
table.tableblock.fit-content>caption.title{white-space:nowrap;width:0}
.paragraph.lead>p,#preamble>.sectionbody>[class="paragraph"]:first-of-type p{font-size:1.21875em;line-height:1.6;color:rgba(0,0,0,.85)}
table.tableblock #preamble>.sectionbody>[class="paragraph"]:first-of-type p{font-size:inherit}
.admonitionblock>table{border-collapse:separate;border:0;background:none;width:100%}
.admonitionblock>table td.icon{text-align:center;width:80px}
.admonitionblock>table td.icon img{max-width:none}
.admonitionblock>table td.icon .title{font-weight:bold;font-family:"Open Sans","DejaVu Sans",sans-serif;text-transform:uppercase}
.admonitionblock>table td.content{padding-left:1.125em;padding-right:1.25em;border-left:1px solid #dddddf;color:rgba(0,0,0,.6)}
.admonitionblock>table td.content>:last-child>:last-child{margin-bottom:0}
.exampleblock>.content{border-style:solid;border-width:1px;border-color:#e6e6e6;margin-bottom:1.25em;padding:1.25em;background:#fff;-webkit-border-radius:4px;border-radius:4px}
.exampleblock>.content>:first-child{margin-top:0}
.exampleblock>.content>:last-child{margin-bottom:0}
.sidebarblock{border-style:solid;border-width:1px;border-color:#dbdbd6;margin-bottom:1.25em;padding:1.25em;background:#f3f3f2;-webkit-border-radius:4px;border-radius:4px}
.sidebarblock>:first-child{margin-top:0}
.sidebarblock>:last-child{margin-bottom:0}
.sidebarblock>.content>.title{color:#7a2518;margin-top:0;text-align:center}
.exampleblock>.content>:last-child>:last-child,.exampleblock>.content .olist>ol>li:last-child>:last-child,.exampleblock>.content .ulist>ul>li:last-child>:last-child,.exampleblock>.content .qlist>ol>li:last-child>:last-child,.sidebarblock>.content>:last-child>:last-child,.sidebarblock>.content .olist>ol>li:last-child>:last-child,.sidebarblock>.content .ulist>ul>li:last-child>:last-child,.sidebarblock>.content .qlist>ol>li:last-child>:last-child{margin-bottom:0}
.literalblock pre,.listingblock>.content>pre{-webkit-border-radius:4px;border-radius:4px;word-wrap:break-word;overflow-x:auto;padding:1em;font-size:.8125em}
@media screen and (min-width:768px){.literalblock pre,.listingblock>.content>pre{font-size:.90625em}}
@media screen and (min-width:1280px){.literalblock pre,.listingblock>.content>pre{font-size:1em}}
.literalblock pre,.listingblock>.content>pre:not(.highlight),.listingblock>.content>pre[class="highlight"],.listingblock>.content>pre[class^="highlight "]{background:#f7f7f8}
.literalblock.output pre{color:#f7f7f8;background:rgba(0,0,0,.9)}
.listingblock>.content{position:relative}
.listingblock code[data-lang]::before{display:none;content:attr(data-lang);position:absolute;font-size:.75em;top:.425rem;right:.5rem;line-height:1;text-transform:uppercase;color:inherit;opacity:.5}
.listingblock:hover code[data-lang]::before{display:block}
.listingblock.terminal pre .command::before{content:attr(data-prompt);padding-right:.5em;color:inherit;opacity:.5}
.listingblock.terminal pre .command:not([data-prompt])::before{content:"$"}
.listingblock pre.highlightjs{padding:0}
.listingblock pre.highlightjs>code{padding:1em;-webkit-border-radius:4px;border-radius:4px}
.listingblock pre.prettyprint{border-width:0}
.prettyprint{background:#f7f7f8}
pre.prettyprint .linenums{line-height:1.45;margin-left:2em}
pre.prettyprint li{background:none;list-style-type:inherit;padding-left:0}
pre.prettyprint li code[data-lang]::before{opacity:1}
pre.prettyprint li:not(:first-child) code[data-lang]::before{display:none}
table.linenotable{border-collapse:separate;border:0;margin-bottom:0;background:none}
table.linenotable td[class]{color:inherit;vertical-align:top;padding:0;line-height:inherit;white-space:normal}
table.linenotable td.code{padding-left:.75em}
table.linenotable td.linenos{border-right:1px solid currentColor;opacity:.35;padding-right:.5em}
pre.pygments .lineno{border-right:1px solid currentColor;opacity:.35;display:inline-block;margin-right:.75em}
pre.pygments .lineno::before{content:"";margin-right:-.125em}
.quoteblock{margin:0 1em 1.25em 1.5em;display:table}
.quoteblock:not(.excerpt)>.title{margin-left:-1.5em;margin-bottom:.75em}
.quoteblock blockquote,.quoteblock p{color:rgba(0,0,0,.85);font-size:1.15rem;line-height:1.75;word-spacing:.1em;letter-spacing:0;font-style:italic;text-align:justify}
.quoteblock blockquote{margin:0;padding:0;border:0}
.quoteblock blockquote::before{content:"\201c";float:left;font-size:2.75em;font-weight:bold;line-height:.6em;margin-left:-.6em;color:#7a2518;text-shadow:0 1px 2px rgba(0,0,0,.1)}
.quoteblock blockquote>.paragraph:last-child p{margin-bottom:0}
.quoteblock .attribution{margin-top:.75em;margin-right:.5ex;text-align:right}
.verseblock{margin:0 1em 1.25em}
.verseblock pre{font-family:"Open Sans","DejaVu Sans",sans;font-size:1.15rem;color:rgba(0,0,0,.85);font-weight:300;text-rendering:optimizeLegibility}
.verseblock pre strong{font-weight:400}
.verseblock .attribution{margin-top:1.25rem;margin-left:.5ex}
.quoteblock .attribution,.verseblock .attribution{font-size:.9375em;line-height:1.45;font-style:italic}
.quoteblock .attribution br,.verseblock .attribution br{display:none}
.quoteblock .attribution cite,.verseblock .attribution cite{display:block;letter-spacing:-.025em;color:rgba(0,0,0,.6)}
.quoteblock.abstract blockquote::before,.quoteblock.excerpt blockquote::before,.quoteblock .quoteblock blockquote::before{display:none}
.quoteblock.abstract blockquote,.quoteblock.abstract p,.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{line-height:1.6;word-spacing:0}
.quoteblock.abstract{margin:0 1em 1.25em;display:block}
.quoteblock.abstract>.title{margin:0 0 .375em;font-size:1.15em;text-align:center}
.quoteblock.excerpt>blockquote,.quoteblock .quoteblock{padding:0 0 .25em 1em;border-left:.25em solid #dddddf}
.quoteblock.excerpt,.quoteblock .quoteblock{margin-left:0}
.quoteblock.excerpt blockquote,.quoteblock.excerpt p,.quoteblock .quoteblock blockquote,.quoteblock .quoteblock p{color:inherit;font-size:1.0625rem}
.quoteblock.excerpt .attribution,.quoteblock .quoteblock .attribution{color:inherit;text-align:left;margin-right:0}
table.tableblock{max-width:100%;border-collapse:separate}
p.tableblock:last-child{margin-bottom:0}
td.tableblock>.content>:last-child{margin-bottom:-1.25em}
td.tableblock>.content>:last-child.sidebarblock{margin-bottom:0}
table.tableblock,th.tableblock,td.tableblock{border:0 solid #dedede}
table.grid-all>thead>tr>.tableblock,table.grid-all>tbody>tr>.tableblock{border-width:0 1px 1px 0}
table.grid-all>tfoot>tr>.tableblock{border-width:1px 1px 0 0}
table.grid-cols>*>tr>.tableblock{border-width:0 1px 0 0}
table.grid-rows>thead>tr>.tableblock,table.grid-rows>tbody>tr>.tableblock{border-width:0 0 1px}
table.grid-rows>tfoot>tr>.tableblock{border-width:1px 0 0}
table.grid-all>*>tr>.tableblock:last-child,table.grid-cols>*>tr>.tableblock:last-child{border-right-width:0}
table.grid-all>tbody>tr:last-child>.tableblock,table.grid-all>thead:last-child>tr>.tableblock,table.grid-rows>tbody>tr:last-child>.tableblock,table.grid-rows>thead:last-child>tr>.tableblock{border-bottom-width:0}
table.frame-all{border-width:1px}
table.frame-sides{border-width:0 1px}
table.frame-topbot,table.frame-ends{border-width:1px 0}
table.stripes-all tr,table.stripes-odd tr:nth-of-type(odd),table.stripes-even tr:nth-of-type(even),table.stripes-hover tr:hover{background:#f8f8f7}
th.halign-left,td.halign-left{text-align:left}
th.halign-right,td.halign-right{text-align:right}
th.halign-center,td.halign-center{text-align:center}
th.valign-top,td.valign-top{vertical-align:top}
th.valign-bottom,td.valign-bottom{vertical-align:bottom}
th.valign-middle,td.valign-middle{vertical-align:middle}
table thead th,table tfoot th{font-weight:bold}
tbody tr th{display:table-cell;line-height:1.6;background:#f7f8f7}
tbody tr th,tbody tr th p,tfoot tr th,tfoot tr th p{color:rgba(0,0,0,.8);font-weight:bold}
p.tableblock>code:only-child{background:none;padding:0}
p.tableblock{font-size:1em}
ol{margin-left:1.75em}
ul li ol{margin-left:1.5em}
dl dd{margin-left:1.125em}
dl dd:last-child,dl dd:last-child>:last-child{margin-bottom:0}
ol>li p,ul>li p,ul dd,ol dd,.olist .olist,.ulist .ulist,.ulist .olist,.olist .ulist{margin-bottom:.625em}
ul.checklist,ul.none,ol.none,ul.no-bullet,ol.no-bullet,ol.unnumbered,ul.unstyled,ol.unstyled{list-style-type:none}
ul.no-bullet,ol.no-bullet,ol.unnumbered{margin-left:.625em}
ul.unstyled,ol.unstyled{margin-left:0}
ul.checklist{margin-left:.625em}
ul.checklist li>p:first-child>.fa-square-o:first-child,ul.checklist li>p:first-child>.fa-check-square-o:first-child{width:1.25em;font-size:.8em;position:relative;bottom:.125em}
ul.checklist li>p:first-child>input[type="checkbox"]:first-child{margin-right:.25em}
ul.inline{display:-ms-flexbox;display:-webkit-box;display:flex;-ms-flex-flow:row wrap;-webkit-flex-flow:row wrap;flex-flow:row wrap;list-style:none;margin:0 0 .625em -1.25em}
ul.inline>li{margin-left:1.25em}
.unstyled dl dt{font-weight:400;font-style:normal}
ol.arabic{list-style-type:decimal}
ol.decimal{list-style-type:decimal-leading-zero}
ol.loweralpha{list-style-type:lower-alpha}
ol.upperalpha{list-style-type:upper-alpha}
ol.lowerroman{list-style-type:lower-roman}
ol.upperroman{list-style-type:upper-roman}
ol.lowergreek{list-style-type:lower-greek}
.hdlist>table,.colist>table{border:0;background:none}
.hdlist>table>tbody>tr,.colist>table>tbody>tr{background:none}
td.hdlist1,td.hdlist2{vertical-align:top;padding:0 .625em}
td.hdlist1{font-weight:bold;padding-bottom:1.25em}
.literalblock+.colist,.listingblock+.colist{margin-top:-.5em}
.colist td:not([class]):first-child{padding:.4em .75em 0;line-height:1;vertical-align:top}
.colist td:not([class]):first-child img{max-width:none}
.colist td:not([class]):last-child{padding:.25em 0}
.thumb,.th{line-height:0;display:inline-block;border:solid 4px #fff;-webkit-box-shadow:0 0 0 1px #ddd;box-shadow:0 0 0 1px #ddd}
.imageblock.left{margin:.25em .625em 1.25em 0}
.imageblock.right{margin:.25em 0 1.25em .625em}
.imageblock>.title{margin-bottom:0}
.imageblock.thumb,.imageblock.th{border-width:6px}
.imageblock.thumb>.title,.imageblock.th>.title{padding:0 .125em}
.image.left,.image.right{margin-top:.25em;margin-bottom:.25em;display:inline-block;line-height:0}
.image.left{margin-right:.625em}
.image.right{margin-left:.625em}
a.image{text-decoration:none;display:inline-block}
a.image object{pointer-events:none}
sup.footnote,sup.footnoteref{font-size:.875em;position:static;vertical-align:super}
sup.footnote a,sup.footnoteref a{text-decoration:none}
sup.footnote a:active,sup.footnoteref a:active{text-decoration:underline}
#footnotes{padding-top:.75em;padding-bottom:.75em;margin-bottom:.625em}
#footnotes hr{width:20%;min-width:6.25em;margin:-.25em 0 .75em;border-width:1px 0 0}
#footnotes .footnote{padding:0 .375em 0 .225em;line-height:1.3334;font-size:.875em;margin-left:1.2em;margin-bottom:.2em}
#footnotes .footnote a:first-of-type{font-weight:bold;text-decoration:none;margin-left:-1.05em}
#footnotes .footnote:last-of-type{margin-bottom:0}
#content #footnotes{margin-top:-.625em;margin-bottom:0;padding:.75em 0}
.gist .file-data>table{border:0;background:#fff;width:100%;margin-bottom:0}
.gist .file-data>table td.line-data{width:99%}
div.unbreakable{page-break-inside:avoid}
.big{font-size:larger}
.small{font-size:smaller}
.underline{text-decoration:underline}
.overline{text-decoration:overline}
.line-through{text-decoration:line-through}
.aqua{color:#00bfbf}
.aqua-background{background:#00fafa}
.black{color:#000}
.black-background{background:#000}
.blue{color:#0000bf}
.blue-background{background:#0000fa}
.fuchsia{color:#bf00bf}
.fuchsia-background{background:#fa00fa}
.gray{color:#606060}
.gray-background{background:#7d7d7d}
.green{color:#006000}
.green-background{background:#007d00}
.lime{color:#00bf00}
.lime-background{background:#00fa00}
.maroon{color:#600000}
.maroon-background{background:#7d0000}
.navy{color:#000060}
.navy-background{background:#00007d}
.olive{color:#606000}
.olive-background{background:#7d7d00}
.purple{color:#600060}
.purple-background{background:#7d007d}
.red{color:#bf0000}
.red-background{background:#fa0000}
.silver{color:#909090}
.silver-background{background:#bcbcbc}
.teal{color:#006060}
.teal-background{background:#007d7d}
.white{color:#bfbfbf}
.white-background{background:#fafafa}
.yellow{color:#bfbf00}
.yellow-background{background:#fafa00}
span.icon>.fa{cursor:default}
a span.icon>.fa{cursor:inherit}
.admonitionblock td.icon [class^="fa icon-"]{font-size:2.5em;text-shadow:1px 1px 2px rgba(0,0,0,.5);cursor:default}
.admonitionblock td.icon .icon-note::before{content:"\f05a";color:#19407c}
.admonitionblock td.icon .icon-tip::before{content:"\f0eb";text-shadow:1px 1px 2px rgba(155,155,0,.8);color:#111}
.admonitionblock td.icon .icon-warning::before{content:"\f071";color:#bf6900}
.admonitionblock td.icon .icon-caution::before{content:"\f06d";color:#bf3400}
.admonitionblock td.icon .icon-important::before{content:"\f06a";color:#bf0000}
.conum[data-value]{display:inline-block;color:#fff!important;background:rgba(0,0,0,.8);-webkit-border-radius:100px;border-radius:100px;text-align:center;font-size:.75em;width:1.67em;height:1.67em;line-height:1.67em;font-family:"Open Sans","DejaVu Sans",sans-serif;font-style:normal;font-weight:bold}
.conum[data-value] *{color:#fff!important}
.conum[data-value]+b{display:none}
.conum[data-value]::after{content:attr(data-value)}
pre .conum[data-value]{position:relative;top:-.125em}
b.conum *{color:inherit!important}
.conum:not([data-value]):empty{display:none}
dt,th.tableblock,td.content,div.footnote{text-rendering:optimizeLegibility}
h1,h2,p,td.content,span.alt{letter-spacing:-.01em}
p strong,td.content strong,div.footnote strong{letter-spacing:-.005em}
p,blockquote,dt,td.content,span.alt{font-size:1.0625rem}
p{margin-bottom:1.25rem}
.sidebarblock p,.sidebarblock dt,.sidebarblock td.content,p.tableblock{font-size:1em}
.exampleblock>.content{background:#fffef7;border-color:#e0e0dc;-webkit-box-shadow:0 1px 4px #e0e0dc;box-shadow:0 1px 4px #e0e0dc}
.print-only{display:none!important}
@page{margin:1.25cm .75cm}
@media print{*{-webkit-box-shadow:none!important;box-shadow:none!important;text-shadow:none!important}
html{font-size:80%}
a{color:inherit!important;text-decoration:underline!important}
a.bare,a[href^="#"],a[href^="mailto:"]{text-decoration:none!important}
a[href^="http:"]:not(.bare)::after,a[href^="https:"]:not(.bare)::after{content:"(" attr(href) ")";display:inline-block;font-size:.875em;padding-left:.25em}
abbr[title]::after{content:" (" attr(title) ")"}
pre,blockquote,tr,img,object,svg{page-break-inside:avoid}
thead{display:table-header-group}
svg{max-width:100%}
p,blockquote,dt,td.content{font-size:1em;orphans:3;widows:3}
h2,h3,#toctitle,.sidebarblock>.content>.title{page-break-after:avoid}
#toc,.sidebarblock,.exampleblock>.content{background:none!important}
#toc{border-bottom:1px solid #dddddf!important;padding-bottom:0!important}
body.book #header{text-align:center}
body.book #header>h1:first-child{border:0!important;margin:2.5em 0 1em}
body.book #header .details{border:0!important;display:block;padding:0!important}
body.book #header .details span:first-child{margin-left:0!important}
body.book #header .details br{display:block}
body.book #header .details br+span::before{content:none!important}
body.book #toc{border:0!important;text-align:left!important;padding:0!important;margin:0!important}
body.book #toc,body.book #preamble,body.book h1.sect0,body.book .sect1>h2{page-break-before:always}
.listingblock code[data-lang]::before{display:block}
#footer{padding:0 .9375em}
.hide-on-print{display:none!important}
.print-only{display:block!important}
.hide-for-print{display:none!important}
.show-for-print{display:inherit!important}}
@media print,amzn-kf8{#header>h1:first-child{margin-top:1.25rem}
.sect1{padding:0!important}
.sect1+.sect1{border:0}
#footer{background:none}
#footer-text{color:rgba(0,0,0,.6);font-size:.9em}}
@media amzn-kf8{#header,#content,#footnotes,#footer{padding:0}}
</style>
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
</head>
<body class="book" style="max-width: 100;">
<div id="header">
<h1>Neural Network Exchange Format</h1>
<div class="details">
<span id="author" class="author">The Khronos NNEF Working Group</span><br>
<span id="revnumber">version 1.0.2, Revision 2,</span>
<span id="revdate">2019-09-16</span>
</div>
<div id="toc" class="toc">
<div id="toctitle">Table of Contents</div>
<ul class="sectlevel1">
<li><a href="#introduction">1. Introduction</a>
<ul class="sectlevel2">
<li><a href="#_what_is_nnef">1.1. What is NNEF</a>
<ul class="sectlevel3">
<li><a href="#_the_exporters_view_of_nnef">1.1.1. The Exporter&#8217;s view of NNEF</a></li>
<li><a href="#_the_importers_view_of_nnef">1.1.2. The Importer&#8217;s view of NNEF</a></li>
<li><a href="#_the_application_programmers_view_of_nnef">1.1.3. The Application Programmer&#8217;s view of NNEF</a></li>
<li><a href="#_what_nnef_is_not">1.1.4. What NNEF is not</a></li>
</ul>
</li>
<li><a href="#terminology">1.2. Specification Terminology</a></li>
</ul>
</li>
<li><a href="#fundamentals">2. Fundamentals</a>
<ul class="sectlevel2">
<li><a href="#computational-graphs">2.1. Computational Graphs</a></li>
<li><a href="#tensor-description">2.2. Description of Data</a></li>
<li><a href="#operation-description">2.3. Description of Operations</a></li>
<li><a href="#overview">2.4. Overview of Graph Description and Usage</a>
<ul class="sectlevel3">
<li><a href="#graph-compilation">2.4.1. Graph Compilation and Execution</a></li>
</ul>
</li>
<li><a href="#glossary">2.5. Glossary of Terms</a></li>
</ul>
</li>
<li><a href="#formal-description">3. Formal Description</a>
<ul class="sectlevel2">
<li><a href="#lexical-elements">3.1. Lexical Elements</a></li>
<li><a href="#syntax">3.2. Syntax</a>
<ul class="sectlevel3">
<li><a href="#graph">3.2.1. Graph Definition</a></li>
<li><a href="#op-declaration">3.2.2. Fragment Definition</a></li>
<li><a href="#operator-expressions">3.2.3. Operator Expressions</a></li>
<li><a href="#document">3.2.4. The Whole Document</a></li>
</ul>
</li>
<li><a href="#semantics">3.3. Semantics</a>
<ul class="sectlevel3">
<li><a href="#type-system">3.3.1. Type System and Type Checking</a></li>
<li><a href="#fragment-definition-invocation">3.3.2. Graph and Fragment Definition</a></li>
<li><a href="#expression-building">3.3.3. Building Expressions</a></li>
<li><a href="#exported-ids">3.3.4. Exported Identifiers</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#primitives">4. Operations</a>
<ul class="sectlevel2">
<li><a href="#introduction-ops">4.1. Tensor Introducing Operations</a>
<ul class="sectlevel3">
<li><a href="#external-op">4.1.1. External Data Sources</a></li>
<li><a href="#constant-op">4.1.2. Constants</a></li>
<li><a href="#variable-op">4.1.3. Variables</a></li>
</ul>
</li>
<li><a href="#elementwise-ops">4.2. Element-wise Operations</a>
<ul class="sectlevel3">
<li><a href="#unary-ops">4.2.1. Unary Operations</a></li>
<li><a href="#binary-ops">4.2.2. Binary Operations</a></li>
<li><a href="#select-op">4.2.3. Select Operation</a></li>
<li><a href="#_simplifier_operations">4.2.4. Simplifier Operations</a></li>
</ul>
</li>
<li><a href="#sliding-ops">4.3. Sliding-Window Operations</a>
<ul class="sectlevel3">
<li><a href="#convolution-ops">4.3.1. Convolution and Deconvolution</a></li>
<li><a href="#box-ops">4.3.2. Box Filter</a></li>
<li><a href="#_index_based_sampling">4.3.3. Index Based Sampling</a></li>
<li><a href="#up-down-sample">4.3.4. Up and Down-Sampling</a></li>
</ul>
</li>
<li><a href="#reduce-ops">4.4. Reduce Operations</a></li>
<li><a href="#shape-ops">4.5. Tensor Shape Operations</a>
<ul class="sectlevel3">
<li><a href="#reshape">4.5.1. Reshaping</a></li>
<li><a href="#transpose-op">4.5.2. Transposing</a></li>
<li><a href="#_splitting_and_concatenation">4.5.3. Splitting and Concatenation</a></li>
<li><a href="#_slicing">4.5.4. Slicing</a></li>
<li><a href="#_padding">4.5.5. Padding</a></li>
<li><a href="#_tiling">4.5.6. Tiling</a></li>
</ul>
</li>
<li><a href="#roi-ops">4.6. Region-of-Interest Operations</a>
<ul class="sectlevel3">
<li><a href="#roi-pool">4.6.1. RoI Pooling</a></li>
<li><a href="#roi-resize">4.6.2. RoI Align</a></li>
</ul>
</li>
<li><a href="#matmul-op">4.7. Matrix Multiplication</a></li>
<li><a href="#update-op">4.8. Variable Updates</a></li>
<li><a href="#stdlib">4.9. Compound Operations</a>
<ul class="sectlevel3">
<li><a href="#activation-functions">4.9.1. Activation Functions</a></li>
<li><a href="#linear-operations">4.9.2. Linear Operations</a></li>
<li><a href="#pooling-operations">4.9.3. Pooling Operations</a></li>
<li><a href="#normalization-operations">4.9.4. Normalization Operations</a></li>
<li><a href="#quantization-operations">4.9.5. Quantization Operations</a></li>
<li><a href="#misc-operations">4.9.6. Miscellaneous Operations</a></li>
</ul>
</li>
</ul>
</li>
<li><a href="#storing-data">5. Storing Network Data</a>
<ul class="sectlevel2">
<li><a href="#container-structure">5.1. Container Organization</a></li>
<li><a href="#tensor-data-format">5.2. Tensor File Format</a></li>
<li><a href="#quantization-format">5.3. Quantization File Format</a></li>
</ul>
</li>
<li><a href="#document-validity">6. Document Validity</a></li>
<li><a href="#quantization">7. Quantization</a>
<ul class="sectlevel2">
<li><a href="#_incorporating_quantization_info">7.1. Incorporating Quantization Info</a></li>
<li><a href="#_dynamic_quantization">7.2. Dynamic Quantization</a></li>
</ul>
</li>
<li><a href="#grammar-definitions">Appendix A: Grammar Definitions</a>
<ul class="sectlevel2">
<li><a href="#flat-grammar">A.1. Flat Grammar</a></li>
<li><a href="#compositional-grammar">A.2. Compositional Grammar</a></li>
</ul>
</li>
<li><a href="#example-exports">Appendix B: Example Export</a>
<ul class="sectlevel2">
<li><a href="#_alexnet">B.1. AlexNet</a></li>
</ul>
</li>
<li><a href="#credits">Appendix C: Credits</a></li>
<li><a href="#changes">Appendix D: List of Changes</a></li>
</ul>
</div>
</div>
<div id="content">
<div id="preamble">
<div class="sectionbody">
<!-- toc disabled -->
<div class="paragraph">
<p><strong>Copyright 2017 The Khronos Group Inc.</strong></p>
</div>
<div class="paragraph">
<p>This specification is protected by copyright laws and contains material proprietary
to Khronos. Except as described by these terms, it or any components
may not be reproduced, republished, distributed, transmitted, displayed, broadcast
or otherwise exploited in any manner without the express prior written permission
of Khronos.</p>
</div>
<div class="paragraph">
<p>This specification has been created under the Khronos Intellectual Property Rights
Policy, which is Attachment A of the Khronos Group Membership Agreement available at
www.khronos.org/files/member_agreement.pdf. Khronos grants a conditional
copyright license to use and reproduce the unmodified specification for any purpose,
without fee or royalty, EXCEPT no licenses to any patent, trademark or other
intellectual property rights are granted under these terms. Parties desiring to
implement the specification and make use of Khronos trademarks in relation to that
implementation, and receive reciprocal patent license protection under the Khronos
IP Policy must become Adopters and confirm the implementation as conformant under
the process defined by Khronos for this specification;
see <a href="https://www.khronos.org/adopters" class="bare">https://www.khronos.org/adopters</a>.</p>
</div>
<div class="paragraph">
<p>Khronos makes no, and expressly disclaims any, representations or warranties,
express or implied, regarding this specification, including, without limitation:
merchantability, fitness for a particular purpose, non-infringement of any
intellectual property, correctness, accuracy, completeness, timeliness, and
reliability. Under no circumstances will Khronos, or any of its Promoters,
Contributors or Members, or their respective partners, officers, directors,
employees, agents or representatives be liable for any damages, whether direct,
indirect, special or consequential damages for lost revenues, lost profits, or
otherwise, arising from or in connection with these materials.</p>
</div>
<div class="paragraph">
<p>Vulkan is a registered trademark and Khronos, OpenXR, SPIR, SPIR-V, SYCL, WebGL,
WebCL, OpenVX, OpenVG, EGL, COLLADA, glTF, NNEF, OpenKODE, OpenKCAM, StreamInput,
OpenWF, OpenSL ES, OpenMAX, OpenMAX AL, OpenMAX IL, OpenMAX DL, OpenML and DevU are
trademarks of The Khronos Group Inc. ASTC is a trademark of ARM Holdings PLC,
OpenCL is a trademark of Apple Inc. and OpenGL and OpenML are registered trademarks
and the OpenGL ES and OpenGL SC logos are trademarks of Silicon Graphics
International used under license by Khronos. All other product names, trademarks,
and/or company names are used solely for identification and belong to their
respective owners.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="introduction">1. Introduction</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This chapter is Informative except for the section on Terminology.</p>
</div>
<div class="paragraph">
<p>This document, referred to as the “NNEF Specification” or just the “Specification” hereafter, describes the Neural Network Exchange Format: what it is, what it is intended to be used for, and what is required to produce or consume it. We assume that the reader has at least a rudimentary understanding of neural networks and deep learning. This means familiarity with the essentials of neural network operations and terminology.</p>
</div>
<div class="sect2">
<h3 id="_what_is_nnef">1.1. What is NNEF</h3>
<div class="paragraph">
<p>NNEF is a data format for exchanging information about (trained) neural networks. Exchanging such information in a standardized format has become inevitable with the spreading of deep learning, as neural networks found their way from academic research to real-world industrial applications. With the proliferation of open-source deep learning frameworks and hardware support emerging for the acceleration of neural networks, the field faces the problem of fragmentation, as different accelerators are compatible with different frameworks. The goal of NNEF is to provide a standard platform for connecting accelerated neural network execution engines and available deep learning tools. Ideally, neural networks trained in deep learning frameworks would be exported to NNEF, and neural network accelerator libraries could consume it without worrying about compatibility with all deep learning frameworks.</p>
</div>
<div class="paragraph">
<p>NNEF aims to encapsulate two key aspects of neural networks: <em>network structure</em> and <em>network data</em>. To describe network structure in a flexible way, NNEF introduces a simple syntax similar to existing scripting languages, along with a set of standardized operations to express common neural network architectures. The syntax is designed to be both human readable and editable, and also easy to parse by consumer libraries. The network data, which form parameters of the structure, is stored in a simple format that supports flexible production and consumption of networks. NNEF can be thought of as a simple language with which a neural network accelerator can be programmed, while being independent of many details of the training and inference process, such as how the network is fed with data, or the data representations and algorithms of the underlying hardware.</p>
</div>
<div class="paragraph">
<p>Although the main focus of NNEF is to be a central chain in the pipeline from deep learning frameworks to neural network accelerator libraries, we envision that the format may be used by intermediate tools in the future, for transforming neural networks in ways that are independent both from the training and the execution process. Therefore, producers and consumers of NNEF may be various, however, two important sub-categories are exporters and importers, explained below.</p>
</div>
<div class="sect3">
<h4 id="_the_exporters_view_of_nnef">1.1.1. The Exporter&#8217;s view of NNEF</h4>
<div class="paragraph">
<p>For an exporter of NNEF, such as a deep learning framework, NNEF is a format to which its internal network representation can be converted, and afterwards all accelerator libraries that are able to consume NNEF will be able to execute the trained network (if the network matches its capabilities). The task of an exporter is to map its operations and data representation to the operations and data representation of NNEF, given that this mapping is possible.</p>
</div>
<div class="paragraph">
<p>NNEF also aims to be a distilled collection of deep learning operations that are widespread in successful neural architectures. It is the result of studying open-source deep learning frameworks such as Caffe, Torch, Theano, TensorFlow, CNTK, Chainer, and abstracting out the computations and data structures common to them. It mainly focuses on operations that are possibly efficiently implementable on various target hardware, such as massively parallelizable operations, especially 'local' ones that require a locally concentrated subset of the input data to compute outputs.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_importers_view_of_nnef">1.1.2. The Importer&#8217;s view of NNEF</h4>
<div class="paragraph">
<p>The importer of NNEF, such as a neural network accelerator library (and its underlying hardware), is able to import an NNEF document, and compile it to its internal representation ready for execution. This compilation may happen offline or online. During offline compilation, NNEF may be converted to an optimized, hardware specific representation format, which may be saved and later be quickly loaded for execution. During online compilation, the conversion and optimization may happen without saving it into a hardware specific format, but immediately executing the converted network.</p>
</div>
<div class="paragraph">
<p>NNEF collects operations into groups to indicate relatedness of operations. This may serve as a hint or guideline for hardware implementations, as related operations may require similar hardware capabilities.</p>
</div>
</div>
<div class="sect3">
<h4 id="_the_application_programmers_view_of_nnef">1.1.3. The Application Programmer&#8217;s view of NNEF</h4>
<div class="paragraph">
<p>For an application programmer, NNEF is a standardized way to store and transfer neural networks. Given a neural network in NNEF format, and a driver or library that is able to import it, the application programmer need not worry about where the network came from or what kind of underlying hardware will execute it, as long as it has the capabilities to do so. The application programmer may query the driver of the underlying hardware whether it is capable of executing the given network.</p>
</div>
</div>
<div class="sect3">
<h4 id="_what_nnef_is_not">1.1.4. What NNEF is not</h4>
<div class="paragraph">
<p>NNEF is not an API (Application Programming Interface). It does not define an execution model for neural networks, and hence it does not define what it means to correctly execute a neural network described in NNEF. Although it does define the semantics of operations supposing infinite arithmetics, defining correct execution of actual implementations would require finite arithmetics and underlying representations to be taken into account, which is out of the scope of NNEF.</p>
</div>
<div class="paragraph">
<p>Libraries that produce or consume NNEF may have various APIs. However, importantly for application programmers, an NNEF consumer that intends to execute a neural network will most probably have functionalities to import and compile a network described in NNEF, and feed that network with data afterwards. However, the exact nature of this API is out of the scope of NNEF. One such API is described by the OpenVX Khronos standard&#8217;s neural network extension, along with an execution model of neural networks.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="terminology">1.2. Specification Terminology</h3>
<div class="paragraph">
<p>The key words <strong>must</strong>, <strong>required</strong>, <strong>should</strong>, <strong>recommend</strong>, <strong>may</strong>, and
<strong>optional</strong> in this document are to be interpreted as described in RFC 2119:</p>
</div>
<div class="paragraph">
<p><a href="http://www.ietf.org/rfc/rfc2119.txt" class="bare">http://www.ietf.org/rfc/rfc2119.txt</a></p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>must</strong></dt>
<dd>
<p>When used alone, this word, or the term <strong>required</strong>, means that the
definition is an absolute requirement of the specification.
When followed by <strong>not</strong> (&#8220;<strong>must not</strong>&#8221; ), the phrase means that the
definition is an absolute prohibition of the specification.</p>
</dd>
<dt class="hdlist1"><strong>should</strong></dt>
<dd>
<p>When used alone, this word means that there may exist valid reasons in
particular circumstances to ignore a particular item, but the full
implications must be understood and carefully weighed before choosing a
different course.
When followed by <strong>not</strong> (&#8220;<strong>should not</strong>&#8221;), the phrase means that there may
exist valid reasons in particular circumstances when the particular behavior
is acceptable or even useful, but the full implications should be
understood and the case carefully weighed before implementing any behavior
described with this label.
In cases where grammatically appropriate, the terms <strong>recommend</strong> or
<strong>recommendation</strong> may be used instead of <strong>should</strong>.</p>
</dd>
<dt class="hdlist1"><strong>may</strong></dt>
<dd>
<p>This word, or the adjective <strong>optional</strong>, means that an item is truly
optional.
One vendor may choose to include the item because a particular marketplace
requires it or because the vendor feels that it enhances the product while
another vendor may omit the same item.
An implementation that does not include a particular option must be
prepared to interoperate with another implementation, which does include the
option, though perhaps with reduced functionality.
In the same vein an implementation that does include a particular option
must be prepared to interoperate with another implementation, which does not
include the option (except, of course, for the feature the option provides).</p>
</dd>
</dl>
</div>
<div class="paragraph">
<p>The additional terms <strong>can</strong> and <strong>cannot</strong> are to be interpreted as follows:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>can</strong></dt>
<dd>
<p>This word means that the particular behavior described is a valid choice for
an application, and is never used to refer to implementation behavior.</p>
</dd>
<dt class="hdlist1"><strong>cannot</strong></dt>
<dd>
<p>This word means that the particular behavior described is not achievable by
an application.</p>
</dd>
</dl>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>There is an important distinction between <strong>cannot</strong> and <strong>must not</strong>, as used
in this Specification.
<strong>Cannot</strong> means something the format literally is unable to express, while <strong>must not</strong> means something that the format is capable of expressing, but that the consequences of doing so are undefined and potentially unrecoverable for an implementation.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="fundamentals">2. Fundamentals</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This chapter introduces fundamental concepts including computational graphs, operations and tensors, NNEF syntax and data description. It provides a framework for interpreting more specific descriptions of operations and network architectures in the remainder of the Specification.</p>
</div>
<div class="sect2">
<h3 id="computational-graphs">2.1. Computational Graphs</h3>
<div class="paragraph">
<p>A neural network can be described by a computational graph. The computational graph is a directed graph that has two types of nodes: <em>data</em> nodes and <em>operation</em> nodes. A directed edge from a data node to an operation node means that the operation takes the data as its input, while a directed edge from an operation node to a data node means that the operation produces the data as its output. Edges from data node to data node or from operation node to operation node are not allowed.</p>
</div>
<div class="paragraph">
<p>As an example, a simple multi-layer feedforward network can be described by a linear graph, starting from an input data node, where each layer corresponds to an operation node producing a new intermediate data node, while taking the previous (intermediate) data as input, finally producing some output data.</p>
</div>
<div class="paragraph">
<p>Data nodes represent multi-dimensional arrays (such as vectors, matrices, or arrays of higher dimension), called <em>tensors</em>. The computation starts from data nodes that represent externally provided data or constant or variable tensors internal to the graph. In order to make the description uniform, such data nodes are the results of special <a href="#introduction-ops">tensor introducing operations</a>. Thus, the whole computational graph is described by a set of operations, interconnected by data nodes.</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg version="1.2" width="323.6mm" height="94.48mm" viewBox="2168 4556 32360 9448" preserveAspectRatio="xMidYMid" fill-rule="evenodd" stroke-width="28.222" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg" xmlns:ooo="http://xml.openoffice.org/svg/export" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:presentation="http://sun.com/xmlns/staroffice/presentation" xmlns:smil="http://www.w3.org/2001/SMIL20/" xmlns:anim="urn:oasis:names:tc:opendocument:xmlns:animation:1.0" xml:space="preserve">
 <defs class="ClipPathGroup">
  <clipPath id="presentation_clip_path" clipPathUnits="userSpaceOnUse">
   <rect x="2168" y="4556" width="32360" height="9448"/>
  </clipPath>
  <clipPath id="presentation_clip_path_shrink" clipPathUnits="userSpaceOnUse">
   <rect x="2200" y="4565" width="32296" height="9430"/>
  </clipPath>
 </defs>
 <defs>
  <font id="EmbeddedFont_1" horiz-adv-x="2048">
   <font-face font-family="Liberation Sans embedded" units-per-em="2048" font-weight="normal" font-style="normal" ascent="1847" descent="416"/>
   <missing-glyph horiz-adv-x="2048" d="M 0,0 L 2047,0 2047,2047 0,2047 0,0 Z"/>
   <glyph unicode="p" horiz-adv-x="948" d="M 1053,546 C 1053,169 920,-20 655,-20 488,-20 376,43 319,168 L 314,168 C 317,163 318,106 318,-2 L 318,-425 138,-425 138,861 C 138,972 136,1046 132,1082 L 306,1082 C 307,1079 308,1070 309,1054 310,1037 312,1012 314,978 315,944 316,921 316,908 L 320,908 C 352,975 394,1024 447,1055 500,1086 569,1101 655,1101 788,1101 888,1056 954,967 1020,878 1053,737 1053,546 Z M 864,542 C 864,693 844,800 803,865 762,930 698,962 609,962 538,962 482,947 442,917 401,887 371,840 350,777 329,713 318,630 318,528 318,386 341,281 386,214 431,147 505,113 607,113 696,113 762,146 803,212 844,277 864,387 864,542 Z"/>
   <glyph unicode="V" horiz-adv-x="1363" d="M 782,0 L 584,0 9,1409 210,1409 600,417 684,168 768,417 1156,1409 1357,1409 Z"/>
   <glyph unicode="T" horiz-adv-x="1202" d="M 720,1253 L 720,0 530,0 530,1253 46,1253 46,1409 1204,1409 1204,1253 Z"/>
   <glyph unicode="O" horiz-adv-x="1410" d="M 1495,711 C 1495,564 1467,435 1411,324 1354,213 1273,128 1168,69 1063,10 938,-20 795,-20 650,-20 526,9 421,68 316,127 235,212 180,323 125,434 97,563 97,711 97,936 159,1113 282,1240 405,1367 577,1430 797,1430 940,1430 1065,1402 1170,1345 1275,1288 1356,1205 1412,1096 1467,987 1495,859 1495,711 Z M 1300,711 C 1300,886 1256,1024 1169,1124 1081,1224 957,1274 797,1274 636,1274 511,1225 423,1126 335,1027 291,889 291,711 291,534 336,394 425,291 514,187 637,135 795,135 958,135 1083,185 1170,286 1257,386 1300,528 1300,711 Z"/>
   <glyph unicode="E" horiz-adv-x="1132" d="M 168,0 L 168,1409 1237,1409 1237,1253 359,1253 359,801 1177,801 1177,647 359,647 359,156 1278,156 1278,0 Z"/>
   <glyph unicode="C" horiz-adv-x="1318" d="M 792,1274 C 636,1274 515,1224 428,1124 341,1023 298,886 298,711 298,538 343,400 434,295 524,190 646,137 800,137 997,137 1146,235 1245,430 L 1401,352 C 1343,231 1262,138 1157,75 1052,12 930,-20 791,-20 649,-20 526,10 423,69 319,128 240,212 186,322 131,431 104,561 104,711 104,936 165,1112 286,1239 407,1366 575,1430 790,1430 940,1430 1065,1401 1166,1342 1267,1283 1341,1196 1388,1081 L 1207,1021 C 1174,1103 1122,1166 1050,1209 977,1252 891,1274 792,1274 Z"/>
  </font>
 </defs>
 <defs class="TextShapeIndex">
  <g ooo:slide="id1" ooo:id-list="id3 id4 id5 id6 id7 id8 id9 id10 id11 id12 id13 id14 id15 id16 id17 id18 id19 id20 id21 id22 id23 id24 id25 id26 id27 id28 id29 id30 id31 id32 id33 id34"/>
 </defs>
 <defs class="EmbeddedBulletChars">
  <g id="bullet-char-template-57356" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M 580,1141 L 1163,571 580,0 -4,571 580,1141 Z"/>
  </g>
  <g id="bullet-char-template-57354" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M 8,1128 L 1137,1128 1137,0 8,0 8,1128 Z"/>
  </g>
  <g id="bullet-char-template-10146" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M 174,0 L 602,739 174,1481 1456,739 174,0 Z M 1358,739 L 309,1346 659,739 1358,739 Z"/>
  </g>
  <g id="bullet-char-template-10132" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M 2015,739 L 1276,0 717,0 1260,543 174,543 174,936 1260,936 717,1481 1274,1481 2015,739 Z"/>
  </g>
  <g id="bullet-char-template-10007" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M 0,-2 C -7,14 -16,27 -25,37 L 356,567 C 262,823 215,952 215,954 215,979 228,992 255,992 264,992 276,990 289,987 310,991 331,999 354,1012 L 381,999 492,748 772,1049 836,1024 860,1049 C 881,1039 901,1025 922,1006 886,937 835,863 770,784 769,783 710,716 594,584 L 774,223 C 774,196 753,168 711,139 L 727,119 C 717,90 699,76 672,76 641,76 570,178 457,381 L 164,-76 C 142,-110 111,-127 72,-127 30,-127 9,-110 8,-76 1,-67 -2,-52 -2,-32 -2,-23 -1,-13 0,-2 Z"/>
  </g>
  <g id="bullet-char-template-10004" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M 285,-33 C 182,-33 111,30 74,156 52,228 41,333 41,471 41,549 55,616 82,672 116,743 169,778 240,778 293,778 328,747 346,684 L 369,508 C 377,444 397,411 428,410 L 1163,1116 C 1174,1127 1196,1133 1229,1133 1271,1133 1292,1118 1292,1087 L 1292,965 C 1292,929 1282,901 1262,881 L 442,47 C 390,-6 338,-33 285,-33 Z"/>
  </g>
  <g id="bullet-char-template-9679" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M 813,0 C 632,0 489,54 383,161 276,268 223,411 223,592 223,773 276,916 383,1023 489,1130 632,1184 813,1184 992,1184 1136,1130 1245,1023 1353,916 1407,772 1407,592 1407,412 1353,268 1245,161 1136,54 992,0 813,0 Z"/>
  </g>
  <g id="bullet-char-template-8226" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M 346,457 C 273,457 209,483 155,535 101,586 74,649 74,723 74,796 101,859 155,911 209,963 273,989 346,989 419,989 480,963 531,910 582,859 608,796 608,723 608,648 583,586 532,535 482,483 420,457 346,457 Z"/>
  </g>
  <g id="bullet-char-template-8211" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M -4,459 L 1135,459 1135,606 -4,606 -4,459 Z"/>
  </g>
  <g id="bullet-char-template-61548" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M 173,740 C 173,903 231,1043 346,1159 462,1274 601,1332 765,1332 928,1332 1067,1274 1183,1159 1299,1043 1357,903 1357,740 1357,577 1299,437 1183,322 1067,206 928,148 765,148 601,148 462,206 346,322 231,437 173,577 173,740 Z"/>
  </g>
 </defs>
 <defs class="TextEmbeddedBitmaps"/>
 <g class="SlideGroup">
  <g>
   <g id="container-id1">
    <g id="id1" class="Slide" clip-path="url(#presentation_clip_path)">
     <g class="Page">
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id3">
        <rect class="BoundingBox" stroke="none" fill="none" x="2168" y="4556" width="32361" height="9449"/>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id4">
        <rect class="BoundingBox" stroke="none" fill="none" x="2521" y="5570" width="1018" height="2035"/>
        <path fill="rgb(255,255,255)" stroke="none" d="M 3030,7603 L 2522,7603 2522,5571 3537,5571 3537,7603 3030,7603 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 3030,7603 L 2522,7603 2522,5571 3537,5571 3537,7603 3030,7603 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="635px" font-weight="400"><tspan class="TextPosition" x="2818" y="6808"><tspan fill="rgb(0,0,0)" stroke="none">E</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id5">
        <rect class="BoundingBox" stroke="none" fill="none" x="5571" y="5570" width="3052" height="2036"/>
        <path fill="rgb(255,255,255)" stroke="none" d="M 7096,5571 C 7960,5571 8620,6011 8620,6587 8620,7163 7960,7603 7096,7603 6232,7603 5572,7163 5572,6587 5572,6011 6232,5571 7096,5571 Z M 5572,5571 L 5572,5571 Z M 8621,7604 L 8621,7604 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 7096,5571 C 7960,5571 8620,6011 8620,6587 8620,7163 7960,7603 7096,7603 6232,7603 5572,7163 5572,6587 5572,6011 6232,5571 7096,5571 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 5572,5571 L 5572,5571 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 8621,7604 L 8621,7604 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="635px" font-weight="400"><tspan class="TextPosition" x="6673" y="6808"><tspan fill="rgb(0,0,0)" stroke="none">Op</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.LineShape">
       <g id="id6">
        <rect class="BoundingBox" stroke="none" fill="none" x="3537" y="6437" width="2034" height="301"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 3538,6587 L 5140,6587"/>
        <path fill="rgb(0,0,0)" stroke="none" d="M 5570,6587 L 5120,6437 5120,6737 5570,6587 Z"/>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id7">
        <rect class="BoundingBox" stroke="none" fill="none" x="6078" y="8617" width="2035" height="1020"/>
        <path fill="rgb(255,255,255)" stroke="none" d="M 7095,9635 L 6079,9635 6079,8618 8111,8618 8111,9635 7095,9635 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 7095,9635 L 6079,9635 6079,8618 8111,8618 8111,9635 7095,9635 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="635px" font-weight="400"><tspan class="TextPosition" x="6867" y="9347"><tspan fill="rgb(0,0,0)" stroke="none">C</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.LineShape">
       <g id="id8">
        <rect class="BoundingBox" stroke="none" fill="none" x="6945" y="7603" width="301" height="1018"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 7095,8619 L 7095,8033"/>
        <path fill="rgb(0,0,0)" stroke="none" d="M 7095,7603 L 6945,8053 7245,8053 7095,7603 Z"/>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id9">
        <rect class="BoundingBox" stroke="none" fill="none" x="10141" y="5570" width="1018" height="2035"/>
        <path fill="rgb(255,255,255)" stroke="none" d="M 10650,7603 L 10142,7603 10142,5571 11157,5571 11157,7603 10650,7603 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 10650,7603 L 10599,7603"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 10472,7603 L 10421,7603"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 10294,7603 L 10142,7603 10142,7501"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 10142,7374 L 10142,7120"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 10142,6993 L 10142,6739"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 10142,6612 L 10142,6561"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 10142,6434 L 10142,6383"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 10142,6256 L 10142,6002"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 10142,5875 L 10142,5621"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 10220,5571 L 10474,5571"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 10601,5571 L 10652,5571"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 10779,5571 L 10830,5571"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 10957,5571 L 11157,5571 11157,5625"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 11157,5752 L 11157,6006"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 11157,6133 L 11157,6387"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 11157,6514 L 11157,6565"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 11157,6692 L 11157,6743"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 11157,6870 L 11157,7124"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 11157,7251 L 11157,7505"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 11129,7603 L 10875,7603"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 10748,7603 L 10697,7603"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="635px" font-weight="400"><tspan class="TextPosition" x="10455" y="6808"><tspan fill="rgb(0,0,0)" stroke="none">T</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.LineShape">
       <g id="id10">
        <rect class="BoundingBox" stroke="none" fill="none" x="8618" y="6437" width="1526" height="301"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 8619,6587 L 9713,6587"/>
        <path fill="rgb(0,0,0)" stroke="none" d="M 10143,6587 L 9693,6437 9693,6737 10143,6587 Z"/>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id11">
        <rect class="BoundingBox" stroke="none" fill="none" x="13190" y="8619" width="3052" height="2036"/>
        <path fill="rgb(255,255,255)" stroke="none" d="M 14715,8620 C 15579,8620 16239,9060 16239,9636 16239,10212 15579,10652 14715,10652 13851,10652 13191,10212 13191,9636 13191,9060 13851,8620 14715,8620 Z M 13191,8620 L 13191,8620 Z M 16240,10653 L 16240,10653 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 14715,8620 C 15579,8620 16239,9060 16239,9636 16239,10212 15579,10652 14715,10652 13851,10652 13191,10212 13191,9636 13191,9060 13851,8620 14715,8620 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 13191,8620 L 13191,8620 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 16240,10653 L 16240,10653 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="635px" font-weight="400"><tspan class="TextPosition" x="14292" y="9857"><tspan fill="rgb(0,0,0)" stroke="none">Op</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id12">
        <rect class="BoundingBox" stroke="none" fill="none" x="17761" y="8670" width="1018" height="2035"/>
        <path fill="rgb(255,255,255)" stroke="none" d="M 18270,10703 L 17762,10703 17762,8671 18777,8671 18777,10703 18270,10703 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 18270,10703 L 18219,10703"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 18092,10703 L 18041,10703"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 17914,10703 L 17762,10703 17762,10601"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 17762,10474 L 17762,10220"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 17762,10093 L 17762,9839"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 17762,9712 L 17762,9661"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 17762,9534 L 17762,9483"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 17762,9356 L 17762,9102"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 17762,8975 L 17762,8721"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 17840,8671 L 18094,8671"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 18221,8671 L 18272,8671"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 18399,8671 L 18450,8671"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 18577,8671 L 18777,8671 18777,8725"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 18777,8852 L 18777,9106"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 18777,9233 L 18777,9487"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 18777,9614 L 18777,9665"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 18777,9792 L 18777,9843"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 18777,9970 L 18777,10224"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 18777,10351 L 18777,10605"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 18749,10703 L 18495,10703"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 18368,10703 L 18317,10703"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="635px" font-weight="400"><tspan class="TextPosition" x="18075" y="9908"><tspan fill="rgb(0,0,0)" stroke="none">T</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.LineShape">
       <g id="id13">
        <rect class="BoundingBox" stroke="none" fill="none" x="16238" y="9537" width="1526" height="301"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 16239,9687 L 17333,9687"/>
        <path fill="rgb(0,0,0)" stroke="none" d="M 17763,9687 L 17313,9537 17313,9837 17763,9687 Z"/>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id14">
        <rect class="BoundingBox" stroke="none" fill="none" x="2521" y="10650" width="1018" height="2035"/>
        <path fill="rgb(255,255,255)" stroke="none" d="M 3030,12683 L 2522,12683 2522,10651 3537,10651 3537,12683 3030,12683 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 3030,12683 L 2522,12683 2522,10651 3537,10651 3537,12683 3030,12683 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="635px" font-weight="400"><tspan class="TextPosition" x="2818" y="11888"><tspan fill="rgb(0,0,0)" stroke="none">E</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.ConnectorShape">
       <g id="id15">
        <rect class="BoundingBox" stroke="none" fill="none" x="11157" y="6586" width="2631" height="2332"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 11158,6587 L 13637,6587 13637,8487"/>
        <path fill="rgb(0,0,0)" stroke="none" d="M 13637,8917 L 13787,8467 13487,8467 13637,8917 Z"/>
       </g>
      </g>
      <g class="com.sun.star.drawing.ConnectorShape">
       <g id="id16">
        <rect class="BoundingBox" stroke="none" fill="none" x="3537" y="10355" width="10251" height="1314"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 3538,11667 L 13637,11667 13637,10785"/>
        <path fill="rgb(0,0,0)" stroke="none" d="M 13637,10355 L 13487,10805 13787,10805 13637,10355 Z"/>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id17">
        <rect class="BoundingBox" stroke="none" fill="none" x="14461" y="12766" width="2035" height="1020"/>
        <path fill="rgb(255,255,255)" stroke="none" d="M 15478,13784 L 14462,13784 14462,12767 16494,12767 16494,13784 15478,13784 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 15478,13784 L 14462,13784 14462,12767 16494,12767 16494,13784 15478,13784 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="635px" font-weight="400"><tspan class="TextPosition" x="15250" y="13496"><tspan fill="rgb(0,0,0)" stroke="none">C</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.LineShape">
       <g id="id18">
        <rect class="BoundingBox" stroke="none" fill="none" x="15323" y="10584" width="301" height="2186"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 15473,12768 L 15473,11014"/>
        <path fill="rgb(0,0,0)" stroke="none" d="M 15473,10584 L 15323,11034 15623,11034 15473,10584 Z"/>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id19">
        <rect class="BoundingBox" stroke="none" fill="none" x="20811" y="7102" width="3052" height="2036"/>
        <path fill="rgb(255,255,255)" stroke="none" d="M 22336,7103 C 23200,7103 23860,7543 23860,8119 23860,8695 23200,9135 22336,9135 21472,9135 20812,8695 20812,8119 20812,7543 21472,7103 22336,7103 Z M 20812,7103 L 20812,7103 Z M 23861,9136 L 23861,9136 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 22336,7103 C 23200,7103 23860,7543 23860,8119 23860,8695 23200,9135 22336,9135 21472,9135 20812,8695 20812,8119 20812,7543 21472,7103 22336,7103 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 20812,7103 L 20812,7103 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 23861,9136 L 23861,9136 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="635px" font-weight="400"><tspan class="TextPosition" x="21913" y="8340"><tspan fill="rgb(0,0,0)" stroke="none">Op</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id20">
        <rect class="BoundingBox" stroke="none" fill="none" x="21318" y="10149" width="2035" height="1020"/>
        <path fill="rgb(255,255,255)" stroke="none" d="M 22335,11167 L 21319,11167 21319,10150 23351,10150 23351,11167 22335,11167 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 22335,11167 L 21319,11167 21319,10150 23351,10150 23351,11167 22335,11167 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="635px" font-weight="400"><tspan class="TextPosition" x="22107" y="10879"><tspan fill="rgb(0,0,0)" stroke="none">C</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.LineShape">
       <g id="id21">
        <rect class="BoundingBox" stroke="none" fill="none" x="22185" y="9135" width="301" height="1018"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 22335,10151 L 22335,9565"/>
        <path fill="rgb(0,0,0)" stroke="none" d="M 22335,9135 L 22185,9585 22485,9585 22335,9135 Z"/>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id22">
        <rect class="BoundingBox" stroke="none" fill="none" x="25381" y="7102" width="1018" height="2035"/>
        <path fill="rgb(255,255,255)" stroke="none" d="M 25890,9135 L 25382,9135 25382,7103 26397,7103 26397,9135 25890,9135 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 25890,9135 L 25839,9135"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 25712,9135 L 25661,9135"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 25534,9135 L 25382,9135 25382,9033"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 25382,8906 L 25382,8652"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 25382,8525 L 25382,8271"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 25382,8144 L 25382,8093"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 25382,7966 L 25382,7915"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 25382,7788 L 25382,7534"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 25382,7407 L 25382,7153"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 25460,7103 L 25714,7103"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 25841,7103 L 25892,7103"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 26019,7103 L 26070,7103"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 26197,7103 L 26397,7103 26397,7157"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 26397,7284 L 26397,7538"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 26397,7665 L 26397,7919"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 26397,8046 L 26397,8097"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 26397,8224 L 26397,8275"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 26397,8402 L 26397,8656"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 26397,8783 L 26397,9037"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 26369,9135 L 26115,9135"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 25988,9135 L 25937,9135"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="635px" font-weight="400"><tspan class="TextPosition" x="25695" y="8340"><tspan fill="rgb(0,0,0)" stroke="none">T</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.LineShape">
       <g id="id23">
        <rect class="BoundingBox" stroke="none" fill="none" x="23858" y="7969" width="1526" height="301"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 23859,8119 L 24953,8119"/>
        <path fill="rgb(0,0,0)" stroke="none" d="M 25383,8119 L 24933,7969 24933,8269 25383,8119 Z"/>
       </g>
      </g>
      <g class="com.sun.star.drawing.LineShape">
       <g id="id24">
        <rect class="BoundingBox" stroke="none" fill="none" x="26399" y="8037" width="2034" height="301"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 26400,8187 L 28002,8187"/>
        <path fill="rgb(0,0,0)" stroke="none" d="M 28432,8187 L 27982,8037 27982,8337 28432,8187 Z"/>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id25">
        <rect class="BoundingBox" stroke="none" fill="none" x="28433" y="7202" width="3052" height="2036"/>
        <path fill="rgb(255,255,255)" stroke="none" d="M 29958,7203 C 30822,7203 31482,7643 31482,8219 31482,8795 30822,9235 29958,9235 29094,9235 28434,8795 28434,8219 28434,7643 29094,7203 29958,7203 Z M 28434,7203 L 28434,7203 Z M 31483,9236 L 31483,9236 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 29958,7203 C 30822,7203 31482,7643 31482,8219 31482,8795 30822,9235 29958,9235 29094,9235 28434,8795 28434,8219 28434,7643 29094,7203 29958,7203 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 28434,7203 L 28434,7203 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 31483,9236 L 31483,9236 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="635px" font-weight="400"><tspan class="TextPosition" x="29535" y="8440"><tspan fill="rgb(0,0,0)" stroke="none">Op</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id26">
        <rect class="BoundingBox" stroke="none" fill="none" x="28940" y="10249" width="2035" height="1020"/>
        <path fill="rgb(255,255,255)" stroke="none" d="M 29957,11267 L 28941,11267 28941,10250 30973,10250 30973,11267 29957,11267 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 29957,11267 L 28941,11267 28941,10250 30973,10250 30973,11267 29957,11267 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="635px" font-weight="400"><tspan class="TextPosition" x="29729" y="10979"><tspan fill="rgb(0,0,0)" stroke="none">C</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.LineShape">
       <g id="id27">
        <rect class="BoundingBox" stroke="none" fill="none" x="29807" y="9235" width="301" height="1018"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 29957,10251 L 29957,9665"/>
        <path fill="rgb(0,0,0)" stroke="none" d="M 29957,9235 L 29807,9685 30107,9685 29957,9235 Z"/>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id28">
        <rect class="BoundingBox" stroke="none" fill="none" x="33003" y="7202" width="1018" height="2035"/>
        <path fill="rgb(255,255,255)" stroke="none" d="M 33512,9235 L 33004,9235 33004,7203 34019,7203 34019,9235 33512,9235 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 33512,9235 L 33461,9235"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 33334,9235 L 33283,9235"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 33156,9235 L 33004,9235 33004,9133"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 33004,9006 L 33004,8752"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 33004,8625 L 33004,8371"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 33004,8244 L 33004,8193"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 33004,8066 L 33004,8015"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 33004,7888 L 33004,7634"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 33004,7507 L 33004,7253"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 33082,7203 L 33336,7203"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 33463,7203 L 33514,7203"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 33641,7203 L 33692,7203"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 33819,7203 L 34019,7203 34019,7257"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 34019,7384 L 34019,7638"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 34019,7765 L 34019,8019"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 34019,8146 L 34019,8197"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 34019,8324 L 34019,8375"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 34019,8502 L 34019,8756"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 34019,8883 L 34019,9137"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 33991,9235 L 33737,9235"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 33610,9235 L 33559,9235"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="635px" font-weight="400"><tspan class="TextPosition" x="33317" y="8440"><tspan fill="rgb(0,0,0)" stroke="none">T</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.LineShape">
       <g id="id29">
        <rect class="BoundingBox" stroke="none" fill="none" x="31480" y="8069" width="1526" height="301"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 31481,8219 L 32575,8219"/>
        <path fill="rgb(0,0,0)" stroke="none" d="M 33005,8219 L 32555,8069 32555,8369 33005,8219 Z"/>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id30">
        <rect class="BoundingBox" stroke="none" fill="none" x="24875" y="5070" width="2035" height="1020"/>
        <path fill="rgb(255,255,255)" stroke="none" d="M 25892,6088 L 24876,6088 24876,5071 26908,5071 26908,6088 25892,6088 Z"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 25892,6088 L 24876,6088 24876,5071 26908,5071 26908,6088 25892,6088 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="635px" font-weight="400"><tspan class="TextPosition" x="25681" y="5800"><tspan fill="rgb(0,0,0)" stroke="none">V</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.LineShape">
       <g id="id31">
        <rect class="BoundingBox" stroke="none" fill="none" x="23401" y="5580" width="1476" height="1830"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 23402,7408 L 24606,5915"/>
        <path fill="rgb(0,0,0)" stroke="none" d="M 24876,5580 L 24477,5836 24710,6024 24876,5580 Z"/>
       </g>
      </g>
      <g class="com.sun.star.drawing.LineShape">
       <g id="id32">
        <rect class="BoundingBox" stroke="none" fill="none" x="26907" y="5579" width="2034" height="1882"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 26908,5580 L 28624,7168"/>
        <path fill="rgb(0,0,0)" stroke="none" d="M 28940,7460 L 28712,7044 28508,7265 28940,7460 Z"/>
       </g>
      </g>
      <g class="com.sun.star.drawing.ConnectorShape">
       <g id="id33">
        <rect class="BoundingBox" stroke="none" fill="none" x="18777" y="8838" width="2632" height="851"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 18778,9687 L 21258,9687 21258,9268"/>
        <path fill="rgb(0,0,0)" stroke="none" d="M 21258,8838 L 21108,9288 21408,9288 21258,8838 Z"/>
       </g>
      </g>
      <g class="com.sun.star.drawing.ConnectorShape">
       <g id="id34">
        <rect class="BoundingBox" stroke="none" fill="none" x="11157" y="6586" width="10252" height="815"/>
        <path fill="none" stroke="rgb(0,0,0)" d="M 11158,6587 L 21258,6587 21258,6970"/>
        <path fill="rgb(0,0,0)" stroke="none" d="M 21258,7400 L 21408,6950 21108,6950 21258,7400 Z"/>
       </g>
      </g>
     </g>
    </g>
   </g>
  </g>
 </g>
</svg>" alt="computational graph">
</div>
<div class="title">Figure 1. An example computational graph: squares denote tensor data (E: external, C: constant, V: variable, T: regular tensor), ellipses denote operations</div>
</div>
<div class="paragraph">
<p>Computational graphs describe a <em>single step</em> of neural network computation, that is, the computation performed upon a <em>single input</em> (or single batch of inputs) is fed to the graph. This trivially describes feedforward networks, but not all networks are that simple. Recurrent networks introduce dependencies among steps by allowing some tensor values computed in one step be used in the next step, therefore letting the result of the overall computation depend on a <em>sequence of inputs</em>. In order to maintain a clear and easy-to-validate description, the computation of each step is described by an acyclic graph, and the amount of cyclic dependency required for recurrent networks is achieved via variable tensors that can be updated in each step and retain their values between consecutive steps. To achieve this, a special operation is introduced to update variable tensors (see <a href="#update-op">Variable Updates</a>).</p>
</div>
</div>
<div class="sect2">
<h3 id="tensor-description">2.2. Description of Data</h3>
<div class="paragraph">
<p>A multi-dimensional array can be described by its number of dimensions (rank) and its <em>extent</em> in each dimension (its <em>shape</em>). Conceptually, a multi-dimensional array is infinite dimensional, the irrelevant (trailing) dimensions being of extent 1 (hereafter referred to as <em>singleton</em> dimension). In this description, we do not restrict the number of dimensions, and consider the dimensions that are not explicitly described to be singleton. Of course, in practice, an implementation restricts the number of <em>supported</em> dimensions. The minimum number of supported dimensions is 2. Dimensions are indexed starting from 0. The product of all extents is called the <em>volume</em> of the tensor (note that trailing singleton dimensions do not effect the volume).</p>
</div>
<div class="paragraph">
<p>In the computational graph, each tensor must have a well-defined shape. The operations <code>external</code>, <code>constant</code> and <code>variable</code> define the shape of the introduced tensors explicitly, thus providing shape for those tensors that constitute the starting points of the computation. All other operations define the shape of their result tensors as a function of the shape of their input tensors, this way propagating shape information through the computational graph.</p>
</div>
<div class="paragraph">
<p>In order to describe the structure of a computational graph, no actual data is needed. However, when the graph is to be executed, actual data is required for those tensors that serve as parameters to the computation, such as variables that have a previously computed value. Thus, apart from the structural aspects of data nodes, their actual data also needs to be defined somewhere. The Specification introduces a binary format for describing the contents of individual tensors.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Consumers of an NNEF document are free to replace the shape of external tensors and propagate shapes accordingly as long as it does not lead to invalid arguments of operations in the computational graph.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect2">
<h3 id="operation-description">2.3. Description of Operations</h3>
<div class="paragraph">
<p>Apart from data nodes, an operation node may have attributes that detail the exact computation performed. Operation nodes have well-defined semantics that specify the mapping of input(s) to output(s) including the shape and content of the output tensors depending on those of the input tensors.</p>
</div>
<div class="paragraph">
<p>Computational graphs may be described in a <em>flat</em> or in a <em>compositional</em> manner. In a compositional description an operation may be expressed in terms of other, simpler operations. Hence, the description of a computational graph may be organized into a hierarchy. A flat description does not group operations into larger units.</p>
</div>
<div class="paragraph">
<p>NNEF graph description syntax can be divided into two parts: a core part that is required for a flat description, and an extension part that can describe compound operations. In case of a compositional description, operations fall into two major categories:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><em>primitive</em> : operations that cannot be expressed in terms of other operations</p>
</li>
<li>
<p><em>compound</em> : operations that can be built from other operations</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Compound operations can still be considered valid by a flat description, however, they are also treated as atomic primitives.</p>
</div>
<div class="paragraph">
<p>Below is a graphical illustration of how larger and larger sub-graphs are built, until the whole graph is defined:</p>
</div>
<div class="imageblock text-center">
<div class="content">
<img src="data:image/svg+xml;base64,<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE svg PUBLIC "-//W3C//DTD SVG 1.1//EN" "http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd">
<svg version="1.2" width="256.43mm" height="56.38mm" viewBox="1567 4556 25643 5638" preserveAspectRatio="xMidYMid" fill-rule="evenodd" stroke-width="28.222" stroke-linejoin="round" xmlns="http://www.w3.org/2000/svg" xmlns:ooo="http://xml.openoffice.org/svg/export" xmlns:xlink="http://www.w3.org/1999/xlink" xmlns:presentation="http://sun.com/xmlns/staroffice/presentation" xmlns:smil="http://www.w3.org/2001/SMIL20/" xmlns:anim="urn:oasis:names:tc:opendocument:xmlns:animation:1.0" xml:space="preserve">
 <defs class="ClipPathGroup">
  <clipPath id="presentation_clip_path" clipPathUnits="userSpaceOnUse">
   <rect x="1567" y="4556" width="25643" height="5638"/>
  </clipPath>
  <clipPath id="presentation_clip_path_shrink" clipPathUnits="userSpaceOnUse">
   <rect x="1592" y="4561" width="25592" height="5627"/>
  </clipPath>
 </defs>
 <defs>
  <font id="EmbeddedFont_1" horiz-adv-x="2048">
   <font-face font-family="Liberation Sans embedded" units-per-em="2048" font-weight="normal" font-style="normal" ascent="1847" descent="416"/>
   <missing-glyph horiz-adv-x="2048" d="M 0,0 L 2047,0 2047,2047 0,2047 0,0 Z"/>
   <glyph unicode="v" horiz-adv-x="1040" d="M 613,0 L 400,0 7,1082 199,1082 437,378 C 446,351 469,272 506,141 L 541,258 580,376 826,1082 1017,1082 Z"/>
   <glyph unicode="u" horiz-adv-x="902" d="M 314,1082 L 314,396 C 314,325 321,269 335,230 349,191 371,162 402,145 433,128 478,119 537,119 624,119 692,149 742,208 792,267 817,350 817,455 L 817,1082 997,1082 997,231 C 997,105 999,28 1003,0 L 833,0 C 832,3 832,12 831,27 830,42 830,59 829,78 828,97 826,132 825,185 L 822,185 C 781,110 733,58 679,27 624,-5 557,-20 476,-20 357,-20 271,10 216,69 161,128 133,225 133,361 L 133,1082 Z"/>
   <glyph unicode="t" horiz-adv-x="532" d="M 554,8 C 495,-8 434,-16 372,-16 228,-16 156,66 156,229 L 156,951 31,951 31,1082 163,1082 216,1324 336,1324 336,1082 536,1082 536,951 336,951 336,268 C 336,216 345,180 362,159 379,138 408,127 450,127 474,127 509,132 554,141 Z"/>
   <glyph unicode="r" horiz-adv-x="556" d="M 142,0 L 142,830 C 142,906 140,990 136,1082 L 306,1082 C 311,959 314,886 314,861 L 318,861 C 347,954 380,1017 417,1051 454,1085 507,1102 575,1102 599,1102 623,1099 648,1092 L 648,927 C 624,934 592,937 552,937 477,937 420,905 381,841 342,776 322,684 322,564 L 322,0 Z"/>
   <glyph unicode="p" horiz-adv-x="948" d="M 1053,546 C 1053,169 920,-20 655,-20 488,-20 376,43 319,168 L 314,168 C 317,163 318,106 318,-2 L 318,-425 138,-425 138,861 C 138,972 136,1046 132,1082 L 306,1082 C 307,1079 308,1070 309,1054 310,1037 312,1012 314,978 315,944 316,921 316,908 L 320,908 C 352,975 394,1024 447,1055 500,1086 569,1101 655,1101 788,1101 888,1056 954,967 1020,878 1053,737 1053,546 Z M 864,542 C 864,693 844,800 803,865 762,930 698,962 609,962 538,962 482,947 442,917 401,887 371,840 350,777 329,713 318,630 318,528 318,386 341,281 386,214 431,147 505,113 607,113 696,113 762,146 803,212 844,277 864,387 864,542 Z"/>
   <glyph unicode="o" horiz-adv-x="994" d="M 1053,542 C 1053,353 1011,212 928,119 845,26 724,-20 565,-20 407,-20 288,28 207,125 126,221 86,360 86,542 86,915 248,1102 571,1102 736,1102 858,1057 936,966 1014,875 1053,733 1053,542 Z M 864,542 C 864,691 842,800 798,868 753,935 679,969 574,969 469,969 393,935 346,866 299,797 275,689 275,542 275,399 298,292 345,221 391,149 464,113 563,113 671,113 748,148 795,217 841,286 864,395 864,542 Z"/>
   <glyph unicode="n" horiz-adv-x="902" d="M 825,0 L 825,686 C 825,757 818,813 804,852 790,891 768,920 737,937 706,954 661,963 602,963 515,963 447,933 397,874 347,815 322,732 322,627 L 322,0 142,0 142,851 C 142,977 140,1054 136,1082 L 306,1082 C 307,1079 307,1070 308,1055 309,1040 310,1024 311,1005 312,986 313,950 314,897 L 317,897 C 358,972 406,1025 461,1056 515,1087 582,1102 663,1102 782,1102 869,1073 924,1014 979,955 1006,857 1006,721 L 1006,0 Z"/>
   <glyph unicode="m" horiz-adv-x="1479" d="M 768,0 L 768,686 C 768,791 754,863 725,903 696,943 645,963 570,963 493,963 433,934 388,875 343,816 321,734 321,627 L 321,0 142,0 142,851 C 142,977 140,1054 136,1082 L 306,1082 C 307,1079 307,1070 308,1055 309,1040 310,1024 311,1005 312,986 313,950 314,897 L 317,897 C 356,974 400,1027 450,1057 500,1087 561,1102 633,1102 715,1102 780,1086 828,1053 875,1020 908,968 927,897 L 930,897 C 967,970 1013,1022 1066,1054 1119,1086 1183,1102 1258,1102 1367,1102 1447,1072 1497,1013 1546,954 1571,856 1571,721 L 1571,0 1393,0 1393,686 C 1393,791 1379,863 1350,903 1321,943 1270,963 1195,963 1116,963 1055,934 1012,876 968,817 946,734 946,627 L 946,0 Z"/>
   <glyph unicode="i" horiz-adv-x="209" d="M 137,1312 L 137,1484 317,1484 317,1312 Z M 137,0 L 137,1082 317,1082 317,0 Z"/>
   <glyph unicode="h" horiz-adv-x="878" d="M 317,897 C 356,968 402,1020 457,1053 511,1086 580,1102 663,1102 780,1102 867,1073 923,1015 978,956 1006,858 1006,721 L 1006,0 825,0 825,686 C 825,762 818,819 804,856 790,893 767,920 735,937 703,954 659,963 602,963 517,963 450,934 399,875 348,816 322,737 322,638 L 322,0 142,0 142,1484 322,1484 322,1098 C 322,1057 321,1015 319,972 316,929 315,904 314,897 Z"/>
   <glyph unicode="e" horiz-adv-x="994" d="M 276,503 C 276,379 302,283 353,216 404,149 479,115 578,115 656,115 719,131 766,162 813,193 844,233 861,281 L 1019,236 C 954,65 807,-20 578,-20 418,-20 296,28 213,123 129,218 87,360 87,548 87,727 129,864 213,959 296,1054 416,1102 571,1102 889,1102 1048,910 1048,527 L 1048,503 Z M 862,641 C 852,755 823,838 775,891 727,943 658,969 568,969 481,969 412,940 361,882 310,823 282,743 278,641 Z"/>
   <glyph unicode="d" horiz-adv-x="948" d="M 821,174 C 788,105 744,55 689,25 634,-5 565,-20 484,-20 347,-20 247,26 183,118 118,210 86,349 86,536 86,913 219,1102 484,1102 566,1102 634,1087 689,1057 744,1027 788,979 821,914 L 823,914 821,1035 821,1484 1001,1484 1001,223 C 1001,110 1003,36 1007,0 L 835,0 C 833,11 831,35 829,74 826,113 825,146 825,174 Z M 275,542 C 275,391 295,282 335,217 375,152 440,119 530,119 632,119 706,154 752,225 798,296 821,405 821,554 821,697 798,802 752,869 706,936 633,969 532,969 441,969 376,936 336,869 295,802 275,693 275,542 Z"/>
   <glyph unicode="a" horiz-adv-x="1087" d="M 414,-20 C 305,-20 224,9 169,66 114,123 87,202 87,302 87,414 124,500 198,560 271,620 390,652 554,656 L 797,660 797,719 C 797,807 778,870 741,908 704,946 645,965 565,965 484,965 426,951 389,924 352,897 330,853 323,793 L 135,810 C 166,1005 310,1102 569,1102 705,1102 807,1071 876,1009 945,946 979,856 979,738 L 979,272 C 979,219 986,179 1000,152 1014,125 1041,111 1080,111 1097,111 1117,113 1139,118 L 1139,6 C 1094,-5 1047,-10 1000,-10 933,-10 885,8 855,43 824,78 807,132 803,207 L 797,207 C 751,124 698,66 637,32 576,-3 501,-20 414,-20 Z M 455,115 C 521,115 580,130 631,160 682,190 723,231 753,284 782,336 797,390 797,445 L 797,534 600,530 C 515,529 451,520 408,504 364,488 330,463 307,430 284,397 272,353 272,299 272,240 288,195 320,163 351,131 396,115 455,115 Z"/>
   <glyph unicode="P" horiz-adv-x="1109" d="M 1258,985 C 1258,852 1215,746 1128,667 1041,588 922,549 773,549 L 359,549 359,0 168,0 168,1409 761,1409 C 919,1409 1041,1372 1128,1298 1215,1224 1258,1120 1258,985 Z M 1066,983 C 1066,1165 957,1256 738,1256 L 359,1256 359,700 746,700 C 959,700 1066,794 1066,983 Z"/>
   <glyph unicode="G" horiz-adv-x="1364" d="M 103,711 C 103,940 164,1117 287,1242 410,1367 582,1430 804,1430 960,1430 1087,1404 1184,1351 1281,1298 1356,1214 1409,1098 L 1227,1044 C 1187,1124 1132,1182 1062,1219 991,1256 904,1274 799,1274 636,1274 512,1225 426,1127 340,1028 297,890 297,711 297,533 343,393 434,290 525,187 652,135 813,135 905,135 991,149 1071,177 1150,205 1215,243 1264,291 L 1264,545 843,545 843,705 1440,705 1440,219 C 1365,143 1274,84 1166,43 1057,1 940,-20 813,-20 666,-20 539,9 432,68 325,127 244,211 188,322 131,432 103,562 103,711 Z"/>
   <glyph unicode="C" horiz-adv-x="1318" d="M 792,1274 C 636,1274 515,1224 428,1124 341,1023 298,886 298,711 298,538 343,400 434,295 524,190 646,137 800,137 997,137 1146,235 1245,430 L 1401,352 C 1343,231 1262,138 1157,75 1052,12 930,-20 791,-20 649,-20 526,10 423,69 319,128 240,212 186,322 131,431 104,561 104,711 104,936 165,1112 286,1239 407,1366 575,1430 790,1430 940,1430 1065,1401 1166,1342 1267,1283 1341,1196 1388,1081 L 1207,1021 C 1174,1103 1122,1166 1050,1209 977,1252 891,1274 792,1274 Z"/>
  </font>
 </defs>
 <defs class="TextShapeIndex">
  <g ooo:slide="id1" ooo:id-list="id3 id4 id5 id6 id7 id8 id9 id10 id11 id12 id13 id14 id15 id16 id17 id18 id19"/>
 </defs>
 <defs class="EmbeddedBulletChars">
  <g id="bullet-char-template-57356" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M 580,1141 L 1163,571 580,0 -4,571 580,1141 Z"/>
  </g>
  <g id="bullet-char-template-57354" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M 8,1128 L 1137,1128 1137,0 8,0 8,1128 Z"/>
  </g>
  <g id="bullet-char-template-10146" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M 174,0 L 602,739 174,1481 1456,739 174,0 Z M 1358,739 L 309,1346 659,739 1358,739 Z"/>
  </g>
  <g id="bullet-char-template-10132" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M 2015,739 L 1276,0 717,0 1260,543 174,543 174,936 1260,936 717,1481 1274,1481 2015,739 Z"/>
  </g>
  <g id="bullet-char-template-10007" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M 0,-2 C -7,14 -16,27 -25,37 L 356,567 C 262,823 215,952 215,954 215,979 228,992 255,992 264,992 276,990 289,987 310,991 331,999 354,1012 L 381,999 492,748 772,1049 836,1024 860,1049 C 881,1039 901,1025 922,1006 886,937 835,863 770,784 769,783 710,716 594,584 L 774,223 C 774,196 753,168 711,139 L 727,119 C 717,90 699,76 672,76 641,76 570,178 457,381 L 164,-76 C 142,-110 111,-127 72,-127 30,-127 9,-110 8,-76 1,-67 -2,-52 -2,-32 -2,-23 -1,-13 0,-2 Z"/>
  </g>
  <g id="bullet-char-template-10004" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M 285,-33 C 182,-33 111,30 74,156 52,228 41,333 41,471 41,549 55,616 82,672 116,743 169,778 240,778 293,778 328,747 346,684 L 369,508 C 377,444 397,411 428,410 L 1163,1116 C 1174,1127 1196,1133 1229,1133 1271,1133 1292,1118 1292,1087 L 1292,965 C 1292,929 1282,901 1262,881 L 442,47 C 390,-6 338,-33 285,-33 Z"/>
  </g>
  <g id="bullet-char-template-9679" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M 813,0 C 632,0 489,54 383,161 276,268 223,411 223,592 223,773 276,916 383,1023 489,1130 632,1184 813,1184 992,1184 1136,1130 1245,1023 1353,916 1407,772 1407,592 1407,412 1353,268 1245,161 1136,54 992,0 813,0 Z"/>
  </g>
  <g id="bullet-char-template-8226" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M 346,457 C 273,457 209,483 155,535 101,586 74,649 74,723 74,796 101,859 155,911 209,963 273,989 346,989 419,989 480,963 531,910 582,859 608,796 608,723 608,648 583,586 532,535 482,483 420,457 346,457 Z"/>
  </g>
  <g id="bullet-char-template-8211" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M -4,459 L 1135,459 1135,606 -4,606 -4,459 Z"/>
  </g>
  <g id="bullet-char-template-61548" transform="scale(0.00048828125,-0.00048828125)">
   <path d="M 173,740 C 173,903 231,1043 346,1159 462,1274 601,1332 765,1332 928,1332 1067,1274 1183,1159 1299,1043 1357,903 1357,740 1357,577 1299,437 1183,322 1067,206 928,148 765,148 601,148 462,206 346,322 231,437 173,577 173,740 Z"/>
  </g>
 </defs>
 <defs class="TextEmbeddedBitmaps"/>
 <g class="SlideGroup">
  <g>
   <g id="container-id1">
    <g id="id1" class="Slide" clip-path="url(#presentation_clip_path)">
     <g class="Page">
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id3">
        <rect class="BoundingBox" stroke="none" fill="none" x="2307" y="7349" width="5843" height="1017"/>
        <path fill="rgb(153,153,153)" stroke="none" d="M 5228,8365 L 2307,8365 2307,7349 8149,7349 8149,8365 5228,8365 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="494px" font-weight="400"><tspan class="TextPosition" x="4020" y="8029"><tspan fill="rgb(255,255,255)" stroke="none">Compound</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="Group">
       <g class="com.sun.star.drawing.CustomShape">
        <g id="id4">
         <rect class="BoundingBox" stroke="none" fill="none" x="8403" y="6080" width="14987" height="1017"/>
         <path fill="rgb(153,153,153)" stroke="none" d="M 15896,7096 L 8403,7096 8403,6080 23389,6080 23389,7096 15896,7096 Z"/>
         <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="494px" font-weight="400"><tspan class="TextPosition" x="14688" y="6760"><tspan fill="rgb(255,255,255)" stroke="none">Compound</tspan></tspan></tspan></text>
        </g>
       </g>
       <g class="com.sun.star.drawing.CustomShape">
        <g id="id5">
         <rect class="BoundingBox" stroke="none" fill="none" x="20595" y="7095" width="2795" height="1270"/>
         <path fill="rgb(153,153,153)" stroke="none" d="M 21992,8364 L 20595,8364 20595,7095 23389,7095 23389,8364 21992,8364 Z"/>
        </g>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id6">
        <rect class="BoundingBox" stroke="none" fill="none" x="2307" y="8619" width="2795" height="1017"/>
        <path fill="rgb(204,204,204)" stroke="none" d="M 3704,9635 L 2307,9635 2307,8619 5101,8619 5101,9635 3704,9635 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="494px" font-weight="400"><tspan class="TextPosition" x="2754" y="9299"><tspan fill="rgb(0,0,0)" stroke="none">Primitive</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id7">
        <rect class="BoundingBox" stroke="none" fill="none" x="5355" y="8619" width="2795" height="1017"/>
        <path fill="rgb(204,204,204)" stroke="none" d="M 6752,9635 L 5355,9635 5355,8619 8149,8619 8149,9635 6752,9635 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="494px" font-weight="400"><tspan class="TextPosition" x="5802" y="9299"><tspan fill="rgb(0,0,0)" stroke="none">Primitive</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id8">
        <rect class="BoundingBox" stroke="none" fill="none" x="8403" y="8619" width="2795" height="1017"/>
        <path fill="rgb(204,204,204)" stroke="none" d="M 9800,9635 L 8403,9635 8403,8619 11197,8619 11197,9635 9800,9635 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="494px" font-weight="400"><tspan class="TextPosition" x="8850" y="9299"><tspan fill="rgb(0,0,0)" stroke="none">Primitive</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id9">
        <rect class="BoundingBox" stroke="none" fill="none" x="11451" y="8619" width="2795" height="1017"/>
        <path fill="rgb(204,204,204)" stroke="none" d="M 12848,9635 L 11451,9635 11451,8619 14245,8619 14245,9635 12848,9635 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="494px" font-weight="400"><tspan class="TextPosition" x="11898" y="9299"><tspan fill="rgb(0,0,0)" stroke="none">Primitive</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id10">
        <rect class="BoundingBox" stroke="none" fill="none" x="14499" y="8619" width="2795" height="1017"/>
        <path fill="rgb(204,204,204)" stroke="none" d="M 15896,9635 L 14499,9635 14499,8619 17293,8619 17293,9635 15896,9635 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="494px" font-weight="400"><tspan class="TextPosition" x="14946" y="9299"><tspan fill="rgb(0,0,0)" stroke="none">Primitive</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id11">
        <rect class="BoundingBox" stroke="none" fill="none" x="17547" y="8619" width="2795" height="1017"/>
        <path fill="rgb(204,204,204)" stroke="none" d="M 18944,9635 L 17547,9635 17547,8619 20341,8619 20341,9635 18944,9635 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="494px" font-weight="400"><tspan class="TextPosition" x="17994" y="9299"><tspan fill="rgb(0,0,0)" stroke="none">Primitive</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id12">
        <rect class="BoundingBox" stroke="none" fill="none" x="20594" y="8619" width="2795" height="1017"/>
        <path fill="rgb(204,204,204)" stroke="none" d="M 21991,9635 L 20594,9635 20594,8619 23388,8619 23388,9635 21991,9635 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="494px" font-weight="400"><tspan class="TextPosition" x="21041" y="9299"><tspan fill="rgb(0,0,0)" stroke="none">Primitive</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id13">
        <rect class="BoundingBox" stroke="none" fill="none" x="23642" y="8619" width="2795" height="1017"/>
        <path fill="rgb(204,204,204)" stroke="none" d="M 25039,9635 L 23642,9635 23642,8619 26436,8619 26436,9635 25039,9635 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="494px" font-weight="400"><tspan class="TextPosition" x="24089" y="9299"><tspan fill="rgb(0,0,0)" stroke="none">Primitive</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id14">
        <rect class="BoundingBox" stroke="none" fill="none" x="8403" y="7349" width="5843" height="1017"/>
        <path fill="rgb(153,153,153)" stroke="none" d="M 11324,8365 L 8403,8365 8403,7349 14245,7349 14245,8365 11324,8365 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="494px" font-weight="400"><tspan class="TextPosition" x="10116" y="8029"><tspan fill="rgb(255,255,255)" stroke="none">Compound</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id15">
        <rect class="BoundingBox" stroke="none" fill="none" x="14499" y="7349" width="5843" height="1017"/>
        <path fill="rgb(153,153,153)" stroke="none" d="M 17420,8365 L 14499,8365 14499,7349 20341,7349 20341,8365 17420,8365 Z"/>
        <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="494px" font-weight="400"><tspan class="TextPosition" x="16212" y="8029"><tspan fill="rgb(255,255,255)" stroke="none">Compound</tspan></tspan></tspan></text>
       </g>
      </g>
      <g class="Group">
       <g class="com.sun.star.drawing.CustomShape">
        <g id="id16">
         <rect class="BoundingBox" stroke="none" fill="none" x="2306" y="4810" width="24132" height="1017"/>
         <path fill="rgb(102,102,102)" stroke="none" d="M 14372,5826 L 2306,5826 2306,4810 26437,4810 26437,5826 14372,5826 Z"/>
         <text class="TextShape"><tspan class="TextParagraph" font-family="Liberation Sans, sans-serif" font-size="494px" font-weight="400"><tspan class="TextPosition" x="13684" y="5490"><tspan fill="rgb(255,255,255)" stroke="none">Graph</tspan></tspan></tspan></text>
        </g>
       </g>
       <g class="com.sun.star.drawing.CustomShape">
        <g id="id17">
         <rect class="BoundingBox" stroke="none" fill="none" x="23642" y="5826" width="2796" height="2541"/>
         <path fill="rgb(102,102,102)" stroke="none" d="M 25040,8366 L 23642,8366 23642,5826 26437,5826 26437,8366 25040,8366 Z"/>
        </g>
       </g>
       <g class="com.sun.star.drawing.CustomShape">
        <g id="id18">
         <rect class="BoundingBox" stroke="none" fill="none" x="2307" y="5825" width="5843" height="1270"/>
         <path fill="rgb(102,102,102)" stroke="none" d="M 5228,7094 L 2307,7094 2307,5825 8149,5825 8149,7094 5228,7094 Z"/>
        </g>
       </g>
      </g>
      <g class="com.sun.star.drawing.CustomShape">
       <g id="id19">
        <rect class="BoundingBox" stroke="none" fill="none" x="1567" y="4556" width="25644" height="5639"/>
       </g>
      </g>
     </g>
    </g>
   </g>
  </g>
 </g>
</svg>" alt="operation hierarchy">
</div>
<div class="title">Figure 2. Hierarchical building of compound fragments and the graph</div>
</div>
<div class="paragraph">
<p>A compositional description is useful as it conveys higher order structure in the graph, grouping frequently occurring sub-graphs. This facilitates a compact description, conveys more information than a flat description, and execution engines may leverage such structural information. However, a compositional description is more complex, more difficult to compile. Thus, where a flat description is sufficient, it is possible to use just the appropriate sub-set of NNEF syntax.</p>
</div>
<div class="paragraph">
<p>When describing operations, primitives need to specify their input-output mapping, including the computation they perform and the shapes and data-types of their results as a function of their inputs. The semantics of these primitive operations are expressed by mathematical formulae. On the other hand, compound operations are built from primitives, and the way they are built provides them with semantics that can be derived from the primitives. This way, the whole computational graph will have well-defined semantics.</p>
</div>
<div class="paragraph">
<p>In order to be able to describe compound operations in terms of other operations (primitive or compound), a <em>procedural</em> notation is used. Popular deep learning frameworks typically utilize a general-purpose scripting language (such as Python). The present description of graph structure mimics a simple subset of such scripting languages, built around a graph <em>fragment</em> construction which lets a parameterized operation be specified in terms of other lower level operations. This methodology can be applied to describe graphs of whole neural networks in a compact manner.</p>
</div>
</div>
<div class="sect2">
<h3 id="overview">2.4. Overview of Graph Description and Usage</h3>
<div class="paragraph">
<p>The purpose of this format is to describe a computational graph that can ultimately be executed, however, the format itself does not define an execution model, only the structure and the data parameters of the graph. In order to do so, a simple textual format describes the structural aspects of the computational graph. Accompanying this textual format is a data storage format that describes external tensor parameters of the computational graph.</p>
</div>
<div class="paragraph">
<p>This document is structured as follows. Chapter <a href="#formal-description">Formal Description</a> details the <a href="#syntax">syntax</a> and the <a href="#semantics">semantics</a> of the textual format. Chapter <a href="#primitives">Operations</a> describes a set of primitive and compound operations from which computational graphs can be built, along with their parametrization and semantics. Chapter <a href="#storing-data">Storing Network Data</a> describes the binary format to store network weights.</p>
</div>
<div class="paragraph">
<p>To give a clearer picture of how the format may be used, we describe how these pieces may be fit together to compile and execute a computational graph.</p>
</div>
<div class="sect3">
<h4 id="graph-compilation">2.4.1. Graph Compilation and Execution</h4>
<div class="paragraph">
<p>The compilation of a computational graph starts from:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A byte stream (textual) describing the computational graph as a sequence of <a href="#primitives">operations</a>.</p>
</li>
<li>
<p>An optional byte stream (binary) containing the data of <a href="#storing-data">serialized</a> external tensor parameters to the graph.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>On a conceptual level, the compilation process may have the following steps:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Parse the structural description, check its validity according to the rules in <a href="#formal-description">Formal Description</a>.</p>
</li>
<li>
<p>If the description is compositional, expand the hierarchy of operations into primitives, by evaluating compile-time expressions of attributes, resulting in a flattened structure.</p>
</li>
<li>
<p>Propagate tensor shapes, perform argument checking for operations as described in <a href="#primitives">Operations</a>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>An implementation is not restricted to compile a graph of primitive operations as listed in <a href="#primitives">Operations</a>, but instead it may implement operations such as those in the <a href="#stdlib">Compound Operations</a> as atomic ones for improved efficiency. After building an executable graph, an implementation may optimize it for example by removing intermediate buffers and unnecessary operations or merging sequences of operations.</p>
</div>
<div class="paragraph">
<p>After the compilation process, the graph execution may have the following steps:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Load previously serialized data or feed initial values to tensors declared as variables.</p>
</li>
<li>
<p>In each cycle, feed values to tensors declared as external inputs, execute required operations, read out outputs.</p>
</li>
<li>
<p>Save updated variables if necessary.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Note again that the exchange format does not define an execution model of the computational graph being described. The above is just an example of how it might be used.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="glossary">2.5. Glossary of Terms</h3>
<div class="paragraph">
<p>The following terms are used frequently in the Specification and are listed explicitly here:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><strong>attribute</strong></dt>
<dd>
<p>A non-tensor parameter to operations that define further details of the operation. Attributes are of primitive types whose values are known at graph compilation-time, and hence expressions of attributes can be evaluated at graph compilation-time.</p>
</dd>
<dt class="hdlist1"><strong>compound operation</strong></dt>
<dd>
<p>An operation that is defined in terms of other operations. Its semantics are defined via the composition of the operations that it is defined by.</p>
</dd>
<dt class="hdlist1"><strong>computational graph</strong></dt>
<dd>
<p>A graph with nodes that are either operations or tensors. Operation nodes are connected to tensor nodes only, and vice versa.</p>
</dd>
<dt class="hdlist1"><strong>custom operation</strong></dt>
<dd>
<p>A primitive operation defined without a body in an NNEF document. It is up to the consumer to know the semantics (mathematical formula and shape information) of such an operation. These are not defined by the specification or the NNEF document itself.</p>
</dd>
<dt class="hdlist1"><strong>graph fragment</strong></dt>
<dd>
<p>A sub-graph in the whole graph (network). A fragment can be described by a set of operations interconnected by tensors.</p>
</dd>
<dt class="hdlist1"><strong>graph compilation-time</strong></dt>
<dd>
<p>The time when the graph is built before execution. Analogous to compilation-time for programming languages.</p>
</dd>
<dt class="hdlist1"><strong>graph execution-time</strong></dt>
<dd>
<p>The time when the graph is run (possibly multiple times) after building. Analogous to run-time for programming languages.</p>
</dd>
<dt class="hdlist1"><strong>operation</strong></dt>
<dd>
<p>A mapping of input tensors to output tensors.</p>
</dd>
<dt class="hdlist1"><strong>primitive operation</strong></dt>
<dd>
<p>An operation that is not defined in terms of other operations. Its semantics are defined via mathematical formulae.</p>
</dd>
<dt class="hdlist1"><strong>rank (of a tensor)</strong></dt>
<dd>
<p>The number dimensions of a tensor explicitly specified in a shape or implicitly defined by shape propagation. Note that a shape explicitly defined as (5,1) has rank 2, even though its last dimension is singular.</p>
</dd>
<dt class="hdlist1"><strong>row-major order</strong></dt>
<dd>
<p>A generalization of row-major data layout of matrices to multi-dimensional arrays. Data is laid out in an order where multi-indexing varies fastest along the last dimension, and slowest along dimension 0. Note that the definition is valid for conceptually infinite dimensional data as well, since the trailing singleton dimensions only introduce trailing 0 indices in the conceptually infinite  multi-index.</p>
</dd>
<dt class="hdlist1"><strong>shape (of a tensor)</strong></dt>
<dd>
<p>A list of integers defining the extents of a tensor in each relevant dimension.</p>
</dd>
<dt class="hdlist1"><strong>tensor</strong></dt>
<dd>
<p>A multi-dimensional array of scalars that represents data flow in the graph. The number of dimensions is conceptually infinite; the insignificant trailing dimensions are 1 (singleton dimension). The minimal number of actually supported dimensions by an implementation is 2.</p>
</dd>
<dt class="hdlist1"><strong>variable tensor</strong></dt>
<dd>
<p>A tensor whose value can be updated by a designated operation. All other tensors are conceptually immutable; each operation generates a new tensor.</p>
</dd>
<dt class="hdlist1"><strong>volume (of a tensor)</strong></dt>
<dd>
<p>An integer value that is the product of extents of a shape.</p>
</dd>
</dl>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="formal-description">3. Formal Description</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This chapter provides a formal description of the exchange format for the structural aspects of the computational graph. It is a simple notation with the goal to describe the building of computational graph fragments from lower level building blocks, ultimately arriving to the description of whole network graphs.</p>
</div>
<div class="paragraph">
<p>A grammar in Backus-Naur Form (BNF) is used to describe the <a href="#syntax">syntax</a> of the notation. First, the <a href="#lexical-elements">lexical elements</a> of the grammar are defined, and the <a href="#semantics">constraints</a> associated with valid computational graphs are also enumerated.</p>
</div>
<div class="sect2">
<h3 id="lexical-elements">3.1. Lexical Elements</h3>
<div class="paragraph">
<p>The description format is made up from the following lexical entities:</p>
</div>
<div class="dlist">
<dl>
<dt class="hdlist1"><code>&lt;identifier&gt;</code></dt>
<dd>
<p>An identifier is an alphanumeric sequence of ASCII characters that may also contain the underscore character. More specifically, identifiers <strong>must</strong> consist of the following ASCII characters: <code>_</code>, <code>[a-z]</code>, <code>[A-Z]</code>, <code>[0-9]</code>. The identifier <strong>must not</strong> start with a digit.</p>
</dd>
<dt class="hdlist1"><code>&lt;numeric-literal&gt;</code></dt>
<dd>
<p>A numeric literal consists of an integer part, an optional decimal point (<code>.</code>) and a fractional part, an <code>e</code> or <code>E</code> and an optionally signed integer exponent. The integer, fractional and the exponent parts <strong>must</strong> each consist of a sequence of decimal (base ten) digits (<code>[0-9]</code>). The literal may be preceded by an optional - (minus) sign. In case of flat syntax, the minus sign is interpreted as being part of the literal value. In compositional syntax, the minus sign is interpreted as a unary minus operator that precedes the numeric literal, and hence is not part of the literal value. The end result in either case is the same.</p>
</dd>
<dt class="hdlist1"><code>&lt;string-literal&gt;</code></dt>
<dd>
<p>A string literal is a sequence of characters enclosed within <code>'</code> or <code>"</code> characters. The end and start quotes <strong>must</strong> match. Any printable ASCII character may appear within the string, except for the start quote character, which <strong>must</strong> be escaped by the <code>\</code> character. The <code>\</code> character <strong>must</strong> also be escaped with the <code>\</code> character.</p>
</dd>
<dt class="hdlist1"><code>&lt;logical-literal&gt;</code></dt>
<dd>
<p>Logical literals are the values <code>true</code> and <code>false</code>.</p>
</dd>
<dt class="hdlist1"><code>&lt;keyword&gt;</code></dt>
<dd>
<p>The following alphabetic character sequences have special meaning with respect to the description syntax and thus <strong>must not</strong> be used as identifiers: <code>version</code>, <code>extension</code>, <code>graph</code>, <code>fragment</code>, <code>tensor</code>, <code>integer</code>, <code>scalar</code>, <code>logical</code>, <code>string</code>, <code>shape_of</code>, <code>length_of</code>, <code>range_of</code>, <code>for</code>, <code>in</code>, <code>yield</code>, <code>if</code>, <code>else</code>.</p>
</dd>
<dt class="hdlist1"><code>&lt;operator&gt;</code></dt>
<dd>
<p>The following character sequences have special meaning as operators in mathematical expressions: <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, <code>^</code>, <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>==</code>, <code>!=</code>, <code>&amp;&amp;</code>, <code>||</code>, <code>!</code>.</p>
</dd>
<dt class="hdlist1">Syntactic characters</dt>
<dd>
<p>The following characters have special syntactic meaning: <code>(</code>, <code>)</code>, <code>[</code>, <code>]</code>, <code>{</code>, <code>}</code>, <code>:</code>, <code>=</code>, <code>,</code>, <code>;</code>, <code>-&gt;</code>.</p>
</dd>
<dt class="hdlist1">White spaces</dt>
<dd>
<p>White space characters may be inserted between any lexical entities. They include the space character, the control characters representing horizontal tab, vertical tab, form feed and new-line.</p>
</dd>
<dt class="hdlist1">Comments</dt>
<dd>
<p>Comments are introduced by the <code>#</code> symbol, and last until the end of the line (either until form feed or new line characters).</p>
</dd>
</dl>
</div>
</div>
<div class="sect2">
<h3 id="syntax">3.2. Syntax</h3>
<div class="paragraph">
<p>The central concept in computational graphs is an operation that maps input tensors into output tensors. Operations need to be declared, and a computational graph description is built from the declared operations. Therefore, we separate the description into two key parts:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The declaration of possible operations for building graphs. Operations have a name and a list of parameters and results. Formal parameters are <a href="#type-system">typed</a>, identifying what kind of expressions can be substituted in their place.</p>
</li>
<li>
<p>The actual graph description consists of a list of operation invocations that are validated against the declarations. The operations are invoked by referencing their names and supplying arguments in place of their formal parameters.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Furthermore, the description of syntax is separated into the constructions sufficient for a <em>flat</em> description and extensions required for <em>compositional</em> descriptions.</p>
</div>
<div class="paragraph">
<p>The following BNF notation introduces the description syntax in a more formal manner. Everything defined by <code>::=</code> below is part of the BNF description, and constitutes valid syntax. Anything outside of the grammar defined by these BNF rules is considered invalid syntax.</p>
</div>
<div class="sect3">
<h4 id="graph">3.2.1. Graph Definition</h4>
<div class="paragraph">
<p>A graph definition consists of a graph declaration and its body. The graph declaration has a list of parameters and results.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;graph-definition&gt; ::= &lt;graph-declaration&gt; &lt;body&gt;
&lt;graph-declaration&gt; ::= "<strong>graph</strong>" &lt;identifier&gt; "<strong>(</strong>" &lt;identifier-list&gt; "<strong>)</strong>"
                        "<strong>-&gt;</strong>" "<strong>(</strong>" &lt;identifier-list&gt; "<strong>)</strong>"
&lt;identifier-list&gt; ::= &lt;identifier&gt; ("<strong>,</strong>" &lt;identifier&gt;)*</code></pre>
</div>
</div>
<div class="paragraph">
<p>The graph definition itself consists of a list of assignments, where the left-hand-side of the assignment is an identifier expression (single identifier, tuple or array), and the right-hand-side is an operation invocation.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;body&gt; ::= "<strong>{</strong>" &lt;assignment&gt;+ "<strong>}</strong>"
&lt;assignment&gt; ::= &lt;lvalue-expr&gt; "<strong>=</strong>" &lt;invocation&gt; "<strong>;</strong>"</code></pre>
</div>
</div>
<div class="paragraph">
<p>An invocation consists of an identifier and a list of arguments:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;invocation&gt; ::= &lt;identifier&gt; ["<strong>&lt;</strong>" &lt;type-name&gt; "<strong>&gt;</strong>"] "<strong>(</strong>" &lt;argument-list&gt; "<strong>)</strong>"
&lt;argument-list&gt; ::= &lt;argument&gt; ("<strong>,</strong>" &lt;argument&gt;)*
&lt;argument&gt; ::= &lt;rvalue-expr&gt; | &lt;identifier&gt; "<strong>=</strong>" &lt;rvalue-expr&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Expressions may be literals, identifiers, arrays and tuples. It is necessary to differentiate between left-value and right-value expressions.</p>
</div>
<div class="paragraph">
<p>Left-value expressions are allowed on the left-hand-side of assignments:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;array-lvalue-expr&gt; ::= "<strong>[</strong>" [&lt;lvalue-expr&gt; ("<strong>,</strong>" &lt;lvalue-expr&gt;)* ] "<strong>]</strong>"
&lt;tuple-lvalue-expr&gt; ::= "<strong>(</strong>" &lt;lvalue-expr&gt; ("<strong>,</strong>" &lt;lvalue-expr&gt;)+ "<strong>)</strong>" |
                            &lt;lvalue-expr&gt; ("<strong>,</strong>" &lt;lvalue-expr&gt;)+
&lt;lvalue-expr&gt; ::= &lt;identifier&gt; | &lt;array-lvalue-expr&gt; | &lt;tuple-lvalue-expr&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Right-value expressions are allowed on the right-hand-side of assignments (as argument values):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;array-rvalue-expr&gt; ::= "<strong>[</strong>" [&lt;rvalue-expr&gt; ("<strong>,</strong>" &lt;rvalue-expr&gt;)* ] "<strong>]</strong>"
&lt;tuple-rvalue-expr&gt; ::= "<strong>(</strong>" &lt;rvalue-expr&gt; ("<strong>,</strong>" &lt;rvalue-expr&gt;)+ "<strong>)</strong>"
&lt;rvalue-expr&gt; ::= &lt;identifier&gt; | &lt;literal&gt; | &lt;array-rvalue-expr&gt; | &lt;tuple-rvalue-expr&gt;

&lt;literal&gt; ::= &lt;numeric-literal&gt; | &lt;string-literal&gt; | &lt;logical-literal&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Invocations may have multiple results (if the operation defines multiple results). In this case, the returned expression is a tuple, and the left-hand-size expression must also be a tuple.</p>
</div>
<div class="paragraph">
<p>As an example, using the declarations above, we may define part of a graph as:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>graph barfoo( input ) -&gt; ( output )
{
    input = external(shape = [1,10]);
    intermediate, extra = bar(input, alpha = 2);
    output = foo(intermediate, size = [3,5]);
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the above example, <code>external</code> is an operation used to introduce tensors that receive their data from an external source (see <a href="#introduction-ops">Tensor Introducing Operations</a>), and exemplary operations <code>bar</code> and <code>foo</code> are defined below.</p>
</div>
</div>
<div class="sect3">
<h4 id="op-declaration">3.2.2. Fragment Definition</h4>
<div class="paragraph">
<p>The following syntax elements <strong>must</strong> be enabled by the extension <code>KHR_enable_fragment_definitions</code>.</p>
</div>
<div class="paragraph">
<p>A fragment is similar to the graph in that its body is defined by a list of assignments, but its declaration allows typed formal parameters and results.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;fragment-definition&gt; ::= &lt;fragment-declaration&gt; (&lt;body&gt; | "<strong>;</strong>")</code></pre>
</div>
</div>
<div class="paragraph">
<p>An fragment declaration is introduced by the <code>fragment</code> keyword, has a name, a parameter list and a result list. Parameters are explicitly typed and may have default values. Results are introduced after the <code>-&gt;</code> symbol.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;fragment-declaration&gt; ::= "<strong>fragment</strong>" &lt;identifier&gt; [&lt;generic-declaration&gt;]
                           "<strong>(</strong>" &lt;parameter-list&gt; "<strong>)</strong>" "<strong>-&gt;</strong>" "<strong>(</strong>" &lt;result-list&gt; "<strong>)</strong>"
&lt;generic-declaration&gt; ::= "<strong>&lt;</strong>" "<strong>?</strong>" ["<strong>=</strong>" &lt;type-name&gt;] "<strong>&gt;</strong>"
&lt;parameter-list&gt; ::= &lt;parameter&gt; ("<strong>,</strong>" &lt;parameter&gt;)*
&lt;parameter&gt; ::= &lt;identifier&gt; "<strong>:</strong>" &lt;type-spec&gt; ["<strong>=</strong>" &lt;literal-expr&gt;]
&lt;result-list&gt; ::= &lt;result&gt; ("<strong>,</strong>" &lt;result&gt;)*
&lt;result&gt; ::= &lt;identifier&gt; "<strong>:</strong>" &lt;type-spec&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Default values are literal expressions built from literals only:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;array-literal-expr&gt; ::= "<strong>[</strong>" [&lt;literal-expr&gt; ("<strong>,</strong>" &lt;literal-expr&gt;)* ] "<strong>]</strong>"
&lt;tuple-literal-expr&gt; ::= "<strong>(</strong>" &lt;literal-expr&gt; ("<strong>,</strong>" &lt;literal-expr&gt;)+ "<strong>)</strong>"
&lt;literal-expr&gt; ::= &lt;literal&gt; | &lt;array-literal-expr&gt; | &lt;tuple-literal-expr&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>A type specification may denote a primitive type, an array type or a tuple type.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;type-name&gt; ::= "<strong>integer</strong>" | "<strong>scalar</strong>" | "<strong>logical</strong>" | "<strong>string</strong>" | "<strong>?</strong>"
&lt;tensor-type-spec&gt; ::= "<strong>tensor</strong>" "<strong>&lt;</strong>" [&lt;type-name&gt;] "<strong>&gt;</strong>"
&lt;array-type-spec&gt; ::= &lt;type-spec&gt; "<strong>[]</strong>"
&lt;tuple-type-spec&gt; ::= "<strong>(</strong>" &lt;type-spec&gt; ("<strong>,</strong>" &lt;type-spec&gt;)+ "<strong>)</strong>"
&lt;type-spec&gt; ::= &lt;type-name&gt; | &lt;tensor-type-spec&gt; |
                &lt;array-type-spec&gt; | &lt;tuple-type-spec&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>For example, the following lines show some operation declarations:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment foo( input: tensor&lt;scalar&gt;, size: integer[] = [1] )
-&gt; ( output: tensor&lt;scalar&gt; )

fragment bar( input: tensor&lt;scalar&gt;, alpha: scalar = 0.5 )
-&gt; ( output: tensor&lt;scalar&gt;, extra: tensor&lt;scalar&gt;[] )</code></pre>
</div>
</div>
<div class="paragraph">
<p>The result types of fragments <strong>must</strong> be all tensors.</p>
</div>
</div>
<div class="sect3">
<h4 id="operator-expressions">3.2.3. Operator Expressions</h4>
<div class="paragraph">
<p>The following syntax elements <strong>must</strong> be enabled by the extension <code>KHR_enable_operator_expressions</code>.</p>
</div>
<div class="paragraph">
<p>The syntax enables more complex expressions to be used as right-value expressions. These expressions allow for compile-time arithmetic and argument composition.</p>
</div>
<div class="paragraph">
<p>Various arithmetic, comparison and logical operators can be used to build binary expressions:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;comparison-operator&gt; ::= "<strong>&lt;</strong>" | "<strong>&lt;=</strong>" | "<strong>&gt;</strong>" | "<strong>&gt;=</strong>" | "<strong>==</strong>" | "<strong>!=</strong>" | "<strong>in</strong>"
&lt;binary-arithmetic-operator&gt; ::= "<strong>+</strong>" | "<strong>-</strong>" | "<strong>*</strong>" | "<strong>/</strong>" | "<strong>^</strong>"
&lt;binary-logical-operator&gt; ::= "<strong>&amp;&amp;</strong>" | "<strong>||</strong>"
&lt;binary-operator&gt; ::= &lt;comparison-operator&gt;
                    | &lt;binary-arithmetic-operator&gt;
                    | &lt;binary-logical-operator&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>A handful of unary operators are also available for arithmetic and logical expressions:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;unary-arithmetic-operator&gt; ::= "<strong>+</strong>" | "<strong>-</strong>"
&lt;unary-logical-operator&gt; ::= "<strong>!</strong>"
&lt;unary-operator&gt; ::= &lt;unary-arithmetic-operator&gt;
                   | &lt;unary-logical-operator&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>Operator expressions can then be built using unary and binary operators and parenthesizing:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;unary-expr&gt; ::= &lt;unary-operator&gt; &lt;rvalue-expr&gt;
&lt;binary-expr&gt; ::= &lt;rvalue-expr&gt; &lt;binary-operator&gt; &lt;rvalue-expr&gt;
&lt;paren-expr&gt; ::= "<strong>(</strong>" &lt;rvalue-expr&gt; "<strong>)</strong>"</code></pre>
</div>
</div>
<div class="paragraph">
<p>The if-else expression implements branching by selecting one of two expressions depending on a condition.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;if-else-expr&gt; ::= &lt;rvalue-expr&gt; "<strong>if</strong>" &lt;rvalue-expr&gt; "<strong>else</strong>" &lt;rvalue-expr&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>The array comprehension expression implements a form of looping. It generates an array by iterating one or more others, optionally filtering the resulting items.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;loop-iter&gt; ::= &lt;identifier&gt; "<strong>in</strong>" &lt;rvalue-expr&gt;
&lt;loop-iter-list&gt; ::= &lt;loop-iter&gt; ("<strong>,</strong>" &lt;loop-iter&gt;)*
&lt;comprehension-expr&gt; ::= "<strong>[</strong>" "<strong>for</strong>" &lt;loop-iter-list&gt; ["<strong>if</strong>" &lt;rvalue-expr&gt;]
                         "<strong>yield</strong>" &lt;rvalue-expr&gt; "<strong>]</strong>"</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>When a comprehension expression contains an <code>if</code> condition, the <code>if</code> is interpreted as part of the comprehension expression and not as part of an if-else expression inside the comprehension expression following the <code>in</code> keyword.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Subscripting expressions can reference a single entry in an array, or a range of entries, in which case the start (inclusive) and end (exclusive) of the range is separated by <code>:</code>. Both the start and the end are optional, in which case 0 or the length of the array is taken, respectively.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;subscript-expr&gt; ::= &lt;rvalue-expr&gt; "<strong>[</strong>" (&lt;rvalue-expr&gt; |
                     [&lt;rvalue-expr&gt;] "<strong>:</strong>" [&lt;rvalue-expr&gt;]) "<strong>]</strong>"</code></pre>
</div>
</div>
<div class="paragraph">
<p>A few special keywords can be used for built-in functions.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;builtin-name&gt; ::= "<strong>shape_of</strong>" | "<strong>length_of</strong>" | "<strong>range_of</strong>"
                 | "<strong>integer</strong>" | "<strong>scalar</strong>" | "<strong>logical</strong>" | "<strong>string</strong>"
&lt;builtin-expr&gt; ::= &lt;builtin-name&gt; "<strong>(</strong>" &lt;rvalue-expr&gt; "<strong>)</strong>"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Finally, extended right-value expressions are the union of all the above constructions (including the ones in the definition of basic right-value expressions and invocations). Assignments may contain any right-value expression, not only invocations:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;rvalue-expr&gt; ::= &lt;identifier&gt;
                | &lt;literal&gt;
                | &lt;binary-expr&gt;
                | &lt;unary-expr&gt;
                | &lt;paren-expr&gt;
                | &lt;array-rvalue-expr&gt;
                | &lt;tuple-rvalue-expr&gt;
                | &lt;subscript-expr&gt;
                | &lt;if-else-expr&gt;
                | &lt;comprehension-expr&gt;
                | &lt;builtin-expr&gt;
                | &lt;invocation&gt;

&lt;assignment&gt; ::= &lt;lvalue-expr&gt; "<strong>=</strong>" &lt;rvalue-expr&gt; "<strong>;</strong>"</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="document">3.2.4. The Whole Document</h4>
<div class="paragraph">
<p>The NNEF structure description consists of a version info, an optional list of extensions used, an optional list of operation definitions and a top-level graph definition. An NNEF document that does not contain operation definitions and extended expressions is said to be <em>flat</em>. A graph definition must be always present.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;document&gt; ::= &lt;version&gt; &lt;extension&gt;* &lt;fragment-definition&gt;* &lt;graph-definition&gt;</code></pre>
</div>
</div>
<div class="paragraph">
<p>The version info is introduced by the <code>version</code> keyword, and is defined by a real-number numeric literal as major and minor versions separated by a dot:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;version&gt; ::= "<strong>version</strong>" &lt;numeric-literal&gt; "<strong>;</strong>"</code></pre>
</div>
</div>
<div class="paragraph">
<p>Extensions can be specified after the <code>extension</code> keyword:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;extension&gt; ::= "<strong>extension</strong>" &lt;identifier&gt;+ "<strong>;</strong>"</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Three types of extensions are distinguished, and this is reflected in naming conventions:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Khronos extensions use the format: <code>KHR_extension_name</code>.</p>
</li>
<li>
<p>Cross-vendor extensions use the format <code>EXT_extension_name</code>.</p>
</li>
<li>
<p>Vendor-specific extensions use the format <code>VENDOR_extension_name</code>, where VENDOR is the actual name of the vendor.</p>
</li>
</ul>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Here is a short example for illustration:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>version 1.0;
extension KHR_enable_fragment_definitions;

fragment foo( input: tensor&lt;scalar&gt;, flag: logical ) -&gt; ( output: tensor&lt;scalar&gt; )
{
    output = ...
}

fragment bar( input: tensor&lt;scalar&gt;, param: scalar ) -&gt; ( output: tensor&lt;scalar&gt; )
{
    output = ...
}

graph foobar( input ) -&gt; ( output )
{
    input = external(shape = [4,10]);
    hidden = foo(input, flag = true);
    output = bar(hidden, param = 3.14);
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note, that it is possible to build networks solely from predefined operations (see <a href="#primitives">Operations</a>), which need not be defined in an actual document description. Therefore, fragment definitions are usually unnecessary. Furthermore, the graph definition body can usually be written without using operator expressions; they are most useful for defining compound operations. Hence, networks without custom operations can usually be written using flat syntax only.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="semantics">3.3. Semantics</h3>
<div class="paragraph">
<p>The following subsections define when a <a href="#syntax">syntactically</a> valid document made up of fragment definitions is also semantically well-defined. Semantic validity rules describe the proper definition and invocation of fragments, including proper naming and referencing, argument number, argument types, argument ranges, proper usage of declared parameters and local identifiers.</p>
</div>
<div class="sect3">
<h4 id="type-system">3.3.1. Type System and Type Checking</h4>
<div class="paragraph">
<p>In the grammar defined in <a href="#syntax">Syntax</a>, formal parameters of operations are explicitly typed. Furthermore, literal constants also have an implicitly defined type. The types of expressions are derived from the types of their arguments.</p>
</div>
<div class="paragraph">
<p>Types can either be primitive or compound. Compound types are composed from primitives or from other compound types.</p>
</div>
<div class="sect4">
<h5 id="primitive-types">Primitive Types</h5>
<div class="paragraph">
<p>The following primitive-types are defined:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>integer</code> : a signed integer value used to describe tensor shapes, dimensions and indices into tensors</p>
</li>
<li>
<p><code>scalar</code> : a real value used to describe data and parameters in operations</p>
</li>
<li>
<p><code>logical</code> : a logical value typically used to describe flags and branching conditions</p>
</li>
<li>
<p><code>string</code> : a general character sequence, used to denote enumerations and names</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The primitive types are used to denote parameters whose values are known in compile-time.</p>
</div>
</div>
<div class="sect4">
<h5 id="compound-types">Compound Types</h5>
<div class="paragraph">
<p>Compound types follow a few construction templates:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><em>Tensor types</em> are built from a single data-type, which <strong>must</strong> be a primitive type. For example <code>tensor&lt;scalar&gt;</code> is a tensor of scalars. Tensors represent data on which run-time computations is performed. A tensor type may be <em>unbound</em>, that is, without data-type, using the <code>tensor&lt;&gt;</code> syntax.</p>
</li>
<li>
<p><em>Array types</em> are built from a single item type. Each item in the array has the same type. For example, <code>integer[]</code> is an array of integers.</p>
</li>
<li>
<p><em>Tuple types</em> are built from multiple item types and always have a fixed number of items. For example, <code>(integer,scalar)</code> is a tuple of an integer and a scalar.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>A type is said to be an <em>attribute</em> type if it does not contain tensor types. Tuples <strong>must not</strong> contain both tensor and non-tensor item types at the same time to separate run-time data parameters and compile-time attribute values.</p>
</div>
<div class="paragraph">
<p>The item type of a compound type may also be a compound type (except for tensors), thus enabling arrays of arrays, arrays of tuples or a tuple that contains an array. For example <code>scalar[][]</code> is a 2-dimensional scalar array (where each sub-array may be of different length), <code>(integer,scalar)[]</code> is an array of tuples, and <code>(integer[],scalar)</code> is a tuple that contains an array and a scalar.</p>
</div>
<div class="paragraph">
<p>Unbound tensors cannot be the result of an operation, they can only be used to declare inputs whose data-types do not need to be checked or agreed with other tensors.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>It is up to the implementation of inference engines how tensor data types are represented.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect4">
<h5 id="generic-types">Generic Types</h5>
<div class="paragraph">
<p>The syntax offers a limited way of constructing generic types via the use of <code>?</code> symbol in place of a primitive type. It may also be used in compound types to denote for example generic tensors as <code>tensor&lt;?&gt;</code> or generic arrays as <code>?[]</code>. The generic symbol can be used in fragment declarations to declare that an operation can be applied to tensors of any data-type. For example</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment buzz&lt;?&gt;( input: tensor&lt;?&gt;, param: ? ) -&gt; ( output: tensor&lt;?&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p>declares an operation <code>buzz</code> to be applicable to tensor of any data-type. The definition also implies that the data-type of the parameter <code>input</code> must be the same as the type of parameter <code>param</code>, and that the data-type of result <code>output</code> will be inherited from input.</p>
</div>
<div class="paragraph">
<p>When invoked, the generic data-type <code>?</code> may be inferred from the actual arguments. However, there may be fragments that do not have inputs of generic type, only the output is generic. For this case, the generic type may have an optional default value.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment fizz&lt;? = scalar&gt;( shape: integer[] ) -&gt; ( output: tensor&lt;?&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p>In this case, the generic parameter must be explicitly supplied, or if not, the default value is used.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>a = fizz&lt;integer&gt;(shape = [...])    # explicit integer data-type
b = fizz(shape = [...])             # default scalar data-type</code></pre>
</div>
</div>
<div class="paragraph">
<p>In case the generic parameter needs to be propagated in a compound operation, it can be done using <code>&lt;?&gt;</code> after the name of the operation:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment fizz&lt;? = scalar&gt;( shape: integer[] ) -&gt; ( output: tensor&lt;?&gt; )
{
    output = buzz&lt;?&gt;(...);
}</code></pre>
</div>
</div>
</div>
<div class="sect4">
<h5 id="literal-constant-types">Types of Literal Constants</h5>
<div class="paragraph">
<p>The types of literal constants are as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The type of <code>&lt;numeric-literal&gt;</code> is either <code>integer</code> or <code>scalar</code> depending on whether it denotes an integer or a real value, respectively.</p>
</li>
<li>
<p>The type of <code>&lt;string-literal&gt;</code> is <code>string</code>.</p>
</li>
<li>
<p>The type of the constants <code>true</code> and <code>false</code> is <code>logical</code>.</p>
</li>
<li>
<p>The type of the literal <code>[]</code> is the special type <em>empty-array-type</em>. This type cannot be declared explicitly.</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="type-casting">Type Casting</h5>
<div class="paragraph">
<p>Only the following implicit type casts are allowed (all others are disallowed):</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Primitive types can be cast to the tensor types of the corresponding data-type.</p>
</li>
<li>
<p>Any primitive can be cast to the generic type <code>?</code>, and any (bound) tensor type can be cast to the generic type <code>tensor&lt;?&gt;</code>.</p>
</li>
<li>
<p>Any tensor type can be cast to the unbound tensor type <code>tensor&lt;&gt;</code>.</p>
</li>
<li>
<p>An array type can be cast to another array type if its item type can be cast. The empty-array-type can be cast to any other array type, but no other array type can be cast to the empty-array-type.</p>
</li>
<li>
<p>A tuple type can be cast to another tuple type if they have the same number of items and the corresponding item types can be cast to those of the other tuple type.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Two types have a common type if either of the two can be cast to the other one.</p>
</div>
<div class="paragraph">
<p>When parameters of primitive type are substituted in place of tensor parameters, they behave as constant tensors of singleton shape.</p>
</div>
<div class="paragraph">
<p>When an expression is substituted in place of a formal parameter in an invocation of an operation or is assigned to the result of a fragment, the expression type <strong>must</strong> be equal or castable to the formal parameter or result type; otherwise the invocation is invalid.</p>
</div>
<div class="paragraph">
<p>Furthermore, explicit type casting between primitive types can be forced using <a href="#built-in-functions">built-in functions</a> where necessary.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="fragment-definition-invocation">3.3.2. Graph and Fragment Definition</h4>
<div class="sect4">
<h5 id="fragment-definition">Declarations</h5>
<div class="paragraph">
<p>Each fragment declaration <strong>must</strong> have a unique name. Fragments are identified only based on their name. A valid declaration <strong>must</strong> satisfy the following criteria:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Formal parameter names and result names <strong>must</strong> be unique within a declaration, that is, valid declarations <strong>must not</strong> contain multiple formal parameters or results with the same name (of course, different declarations may use the same formal parameter names).</p>
</li>
<li>
<p>Parameters of tensor type <strong>must</strong> precede attributes.</p>
</li>
<li>
<p>Fragment results <strong>must</strong> be all tensors.</p>
</li>
<li>
<p>The default value expression type <strong>must</strong> match the type of the formal parameter.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Generic fragments may be defined by appending <code>&lt;?&gt;</code> after the name of the fragment. The symbol <code>?</code> is used to refer to the generic type parameter within the fragment declaration and body. The generic type parameter may have an optional default value using <code>&lt;? = ...&gt;</code> syntax. See <a href="#generic-types">Generic Types</a> for further details.</p>
</div>
<div class="paragraph">
<p>If a fragment has at least one generic parameter or result type, it <strong>must</strong> be declared as generic. In return, if a fragment is declared as generic, it <strong>must</strong> have at least one generic parameter or result type.</p>
</div>
</div>
<div class="sect4">
<h5 id="fragment-invocation">Invocations</h5>
<div class="paragraph">
<p>Invocations <strong>must</strong> assign a unique argument value to each formal parameter that does not have a default value. Formal parameters with default values can also be assigned a different value.</p>
</div>
<div class="paragraph">
<p>There are two ways to assign values to formal parameters: position based (<em>positional argument</em>) and name based (<em>named argument</em>). Positional arguments correspond to formal parameters in the order of their declaration. Named arguments may occur in any order, independent of the order of declaration of formal parameters. Only tensor arguments may be positional, attribute values <strong>must</strong> be named arguments (see <a href="#type-system">Type System and Type Checking</a>).</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>It is <strong>deprecated</strong> to use named arguments for tensor parameters in an invocation. To clearly separate tensor arguments and non-tensor attributes, tensors should be positional arguments, and attributes should be named arguments.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>A valid invocation <strong>must</strong> satisfy the following criteria:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The operation name in the invocation <strong>must</strong> correspond to a declared operation.</p>
</li>
<li>
<p>An invocation <strong>must not</strong> have more arguments than the number of formal parameters in the declaration.</p>
</li>
<li>
<p>Each formal parameter that does not have a default value <strong>must</strong> be assigned an argument value.</p>
</li>
<li>
<p>Positional arguments <strong>must</strong> precede named arguments.</p>
</li>
<li>
<p>Each named argument <strong>must</strong> have a name that corresponds to a formal parameter declared for the operation.</p>
</li>
<li>
<p>Named arguments <strong>must</strong> be unique, that is, each parameter name may occur only once in an invocation.</p>
</li>
<li>
<p>A named argument <strong>must not</strong> refer to a formal parameter that is also assigned by a positional argument.</p>
</li>
<li>
<p>Generic fragments may be called with an explicit type specifier using <code>name&lt;type&gt;</code> syntax. Furthermore, <code>name&lt;?&gt;</code> syntax may be used when a generic parameter type needs to be propagated inside the body of a generic fragment. If a generic fragment has a default value for its generic type and is invoked without an explicit generic argument, the default value of the generic type is substituted.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In an invocation, formal parameters that have a default value and are not assigned any value get their default values substituted.</p>
</div>
<div class="paragraph">
<p>If a fragment is generic and an explicit generic parameter type is not specified, and has no default value for its generic type, then the value of the generic parameter is deduced from the invocation arguments. The deduction <strong>must</strong> result in a unique value. If it results in multiple or no candidates, the invocation is invalid.</p>
</div>
</div>
<div class="sect4">
<h5 id="assignments">Assignments</h5>
<div class="paragraph">
<p>The right-hand-side of an assignment can be any kind of expression, however, the left-hand-side <strong>must</strong> be an identifier or an array or tuple (or any such combination) of identifiers to which the results are unpacked. That is, an array in a tuple is permitted, but invocations and constant expressions are not allowed on the left-hand-side.</p>
</div>
<div class="paragraph">
<p>In case the right-hand-side is an invocation, the left-hand-side expression <strong>must</strong> be structurally equivalent to the result type of the invoked operation. For example, if the operation results in an array of tensors, the left-hand-side <strong>must</strong> be an array of identifiers. This is required so that the length of the resulting array is known from the assignment without knowing the exact semantics of the operation.</p>
</div>
</div>
<div class="sect4">
<h5 id="identifier-usage">Identifier Usage in the Graph and Fragment Body</h5>
<div class="paragraph">
<p>The rules for valid formal identifier usage are as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Parameters of a <em>fragment</em> <strong>must not</strong> appear on the left-hand side of an assignment within the fragment body, they can only be used as arguments of expressions on the right-hand side of assignments.</p>
</li>
<li>
<p>Parameters of a <em>graph</em> (which are implicitly of type tensor) <strong>must</strong> be defined as the result of an <code>external</code> operation. The <code>external</code> operation <strong>must not</strong> be used in a fragment, and all tensors that are results of <code>external</code> operations <strong>must</strong> be defined as graph parameters (one-to-one mapping between graph parameters and externally defined tensors).</p>
</li>
<li>
<p>Results of a fragment <strong>must</strong> be assigned exactly once within the fragment body.</p>
</li>
<li>
<p>Local identifiers within a fragment or within the main graph <strong>must</strong> first appear on the left-hand side of an assignment before used as arguments in expressions on the right-hand side of subsequent assignments.</p>
</li>
<li>
<p>The same identifier <strong>must not</strong> be used on the left-hand side of assignments more than once (within a fragment, or within the main graph).</p>
</li>
<li>
<p>All individual identifiers in the <em>graph</em> body <strong>must</strong> be of type tensor. That is, identifiers of type tensor array, tensor tuple and primitive types are not allowed. When tensor arrays or tuples are used on the left-hand-side of assignments, they must be explicitly constructed from individual tensor identifiers via array or tuple expressions.</p>
</li>
<li>
<p>The <code>variable</code> and <code>update</code> operations <strong>must not</strong> be used inside a fragment, only inside the graph body (the results of a compound operation should only be defined by its inputs, and it should not have any side effects).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The above rules ensure that the resulting graph is acyclic and the order in which the operations are written in the body results in a valid topological ordering of the graph. However, it is not the only valid ordering, and operations <strong>can</strong> be reordered or executed in parallel as long as the above constraints are satisfied.</p>
</div>
<div class="paragraph">
<p>The purpose of disallowing identifiers of type tensor array or tensor tuple in the graph body, is to let each tensor be identifiable by an individual name.</p>
</div>
</div>
<div class="sect4">
<h5 id="argument-validity">Argument Validity</h5>
<div class="paragraph">
<p>Each operation may further restrict the set of allowed parameters beyond what is generally considered valid according to <a href="#type-system">type checking</a>. For example, operations may restrict their attributes to be in specific ranges. Operations that have multiple tensor parameters may define tensor shape agreement rules, and they also define the shapes of their results. These rules are specific to each operation, and are described separately for each primitive in <a href="#primitives">Operations</a>.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="expression-building">3.3.3. Building Expressions</h4>
<div class="paragraph">
<p>The expressions described here mainly use extended syntax (except for simple array and tuple construction). The purpose of such expressions is two-fold:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>One is to serve as a short-hand notation for certain operations. For example, for tensors <code>a</code> and <code>b</code>, <code>a + b</code> is equivalent to <code>add(a,b)</code>.</p>
</li>
<li>
<p>The other is to build or calculate attribute values of operations. For example, an operation may require an array of integers, and that can be built with an expression such as <code>[1,2,3]</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Thus, expressions are either of frequently used arithmetic, comparison or logical operators, or serve the purpose of manipulating a data structure, such as an array or a tuple to serve as arguments for operations. When manipulating data structures, the typical set of operations that are required are building a data structure from its components or parts,
and dissecting a data structure to its components or parts.</p>
</div>
<div class="paragraph">
<p>The following subsections describe built-in operators and data-structure manipulator notations.</p>
</div>
<div class="sect4">
<h5 id="built-in-operators">Built-in Operators</h5>
<div class="paragraph">
<p>Arithmetic, comparison and logical expressions can be built from identifiers and constants (see <a href="#syntax">Syntax</a>). An expression can be a constant expression, which is built only from primitive types, and it can be non-constant expression, in which case it can be mapped to primitive operations (see <a href="#primitives">Operations</a>). The following table summarizes the allowed argument types and the resulting type for each operator. All operators are also applicable to tensors, hence they are not listed here explicitly. In this case, the result is also a tensor (of the appropriate data-type). If at least one of the arguments is a tensor, the operation is mapped to the appropriate fragment. In this case, argument types <strong>must</strong> be checked in the same way as if the fragment was explicitly called.</p>
</div>
<div class="paragraph">
<p>Arithmetic operators that are applicable to <code>scalar</code> are also applicable to <code>integer</code> arguments (but not mixed), in which case the result is also an <code>integer</code>. Comparison operators that are applicable to any primitive type (marked by <code>?</code> below just like generic types) must have arguments of the same type.</p>
</div>
<table class="tableblock frame-all grid-all stretch">
<caption class="title">Table 1. List of built-in operators; parameter and result types, and mapping to fragments</caption>
<colgroup>
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
<col style="width: 25%;">
</colgroup>
<thead>
<tr>
<th class="tableblock halign-left valign-top">Operator</th>
<th class="tableblock halign-left valign-top">Argument types</th>
<th class="tableblock halign-left valign-top">Result type</th>
<th class="tableblock halign-left valign-top">Fragment</th>
</tr>
</thead>
<tbody>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">+</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">copy</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">neg</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">+</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar, scalar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">add</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">-</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar, scalar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">sub</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">*</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar, scalar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">mul</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">/</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar, scalar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">div</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">^</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar, scalar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">pow</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">&lt;</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar, scalar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">logical</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">lt</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">&lt;=</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar, scalar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">logical</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">le</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">&gt;</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar, scalar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">logical</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">gt</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">&gt;=</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">scalar, scalar</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">logical</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ge</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">==</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">?, ?</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">logical</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">eq</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">!=</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">?, ?</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">logical</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">ne</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">!</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">logical</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">logical</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">not</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">&amp;&amp;</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">logical, logical</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">logical</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">and</p></td>
</tr>
<tr>
<td class="tableblock halign-left valign-top"><p class="tableblock">||</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">logical, logical</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">logical</p></td>
<td class="tableblock halign-left valign-top"><p class="tableblock">or</p></td>
</tr>
</tbody>
</table>
<div class="paragraph">
<p>Uses of the above operators with argument types other than those listed above result in invalid expressions.</p>
</div>
<div class="paragraph">
<p>The semantics of arithmetic operators <code>+</code>, <code>-</code>, <code>*</code>, <code>/</code>, comparison operators <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>==</code>, <code>!=</code> and logical operators <code>&amp;&amp;</code>, <code>||</code>, <code>!</code> is as per their common definitions. Furthermore, the binary operator <code>in</code> tests if an item is contained in an <a href="#arrays">array</a>.</p>
</div>
<div class="paragraph">
<p>The precedence of the operators is as follows, from lowest to highest, equal ones grouped in curly braces: { <code>in</code> }, { <code>&amp;&amp;</code>, <code>||</code> }, { <code>&lt;</code>, <code>&lt;=</code>, <code>&gt;</code>, <code>&gt;=</code>, <code>==</code>, <code>!=</code> }, { <code>+</code>, <code>-</code> }, { <code>*</code>, <code>/</code> }, { <code>^</code> }.</p>
</div>
</div>
<div class="sect4">
<h5 id="arrays">Arrays</h5>
<div class="paragraph">
<p>When building arrays, items <strong>must</strong> be of the same type or it <strong>must</strong> be possible to <a href="#type-casting">cast</a> all items to a common type.</p>
</div>
<div class="paragraph">
<p>The simplest way to build an array is to enumerate its items, such as <code>[a,b,c]</code>. Alternatively, arrays can be built by concatenating two arrays using the <code>+</code> operator, such as <code>[a,b] + [c,d]</code> resulting in <code>[a,b,c,d]</code>, or <code>x + y</code> where <code>x</code> and <code>y</code> are themselves arrays. As a generalization of concatenation, the <code>*</code> operator can be used to duplicate an array several times. For example, <code>[a,b] * 2</code> results in <code>[a,b,a,b]</code>.</p>
</div>
<div class="paragraph">
<p>To access an item or range of items in an array, the subscript operator <code>[]</code> can be used. Array indexing starts from 0, and goes until the length of the array minus 1. There are two types of subscript expressions, depending on the expression inside the <code>[]</code>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If the subscript expression is a single expression of type <code>integer</code>, the result is a single item of the array and the type of the subscript expression is that of the item type of the array. For example if <code>a = [1,2,3]</code> then <code>a[1]</code> equals <code>2</code>.</p>
</li>
<li>
<p>If the subscript expression is a range (two expressions of type <code>integer</code> delimited by <code>:</code>), then the result is a sub-sequence of the array and the type of the subscript expression is the same as the array type. The start of the range is inclusive; the end is exclusive. The range may be open at both ends (by omitting the expressions before or after the <code>:</code>). If it is open at the beginning, it is defined to start from 0. If it is open at the end, the end is defined to be the length of the array. If the beginning is greater than or equal to the end, the result is the empty array. For example let <code>a = [1,2,3]</code>, then <code>a[0:2]</code> is equivalent to <code>a[:2]</code> and equals <code>[1,2]</code>, or <code>a[1:3]</code> is equivalent to <code>a[1:]</code> and equals <code>[2,3]</code>. Furthermore, <code>a[2:2]</code> equals <code>[]</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The <code>in</code> operator can be used to test whether an item is contained in an array, returning a logical value. For example: <code>a = b if i in [2,4,6] else c</code>. The values are matched using deep comparison, i.e. arrays and tuples match if their items match.</p>
</div>
</div>
<div class="sect4">
<h5 id="tuples">Tuples</h5>
<div class="paragraph">
<p>Tuples can be constructed from any expression by enumerating the items separated by the <code>,</code> operator. For example, <code>a,b</code> is a tuple, or optionally, for better legibility, a tuple can be parenthesized, as in <code>(a,b)</code>.</p>
</div>
<div class="paragraph">
<p>If an expression is of tuple type, it can be unpacked by assigning it to a tuple containing identifiers to which the tuple items are assigned. For example, if <code>t</code> is a tuple of three items, then <code>a, b, c = t</code> unpacks the items of the tuple to the identifiers <code>a</code>, <code>b</code> and <code>c</code>. The tuple on the left <strong>must</strong> have the same number of items as the one on the right. Fragments that have multiple results essentially return a tuple.</p>
</div>
<div class="paragraph">
<p>Tuple items can also be accessed by subscripting with limitations: subscripts <strong>must</strong> be integer literals, that is, ranges and index variables or expressions are forbidden. <code>a, b, c = t</code> is equivalent to <code>a = t[0]; b = t[1]; c = t[2]</code>.</p>
</div>
</div>
<div class="sect4">
<h5 id="strings">Strings</h5>
<div class="paragraph">
<p>Although <code>string</code> is a primitive type, strings can be thought of as arrays of characters and some operations can be defined the same way as for arrays. Strings cannot be built from characters as an array expression, however:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Strings can be concatenated using the <code>+</code> operator and duplicated with the  <code>*</code> operator.</p>
</li>
<li>
<p>Sub-strings can be created with range indexing. When the subscript expression is a single index, it also results in a string of a single character.</p>
</li>
</ul>
</div>
</div>
<div class="sect4">
<h5 id="built-in-functions">Built-in Functions</h5>
<div class="paragraph">
<p>Some built-in functions are used to query information from tensors, arrays and strings. To describe the interface of such functions, the same notation is used for convenience as for declarations. However, these functions <strong>must</strong> be called with a single positional argument. The ? in place of the type specifier means that any type may be substituted (note that in these cases, it really means any type, not just any primitive type).</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>shape_of( x: tensor&lt;&gt; ) -&gt; ( shape: integer[] )</code> returns the shape of a tensor <code>x</code>. The length of the resulting array is the rank of the tensor. In case a non-tensor value is substituted in place of tensor <code>x</code>, the empty array corresponding to the singleton shape of rank 0 is returned (<strong>deprecated</strong>, see below).</p>
</li>
<li>
<p><code>length_of( x: ?[] ) -&gt; ( length: integer )</code> returns the length of the array <code>x</code>.</p>
</li>
<li>
<p><code>length_of( x: string ) -&gt; ( length: integer )</code> returns the length of the string <code>x</code>.</p>
</li>
<li>
<p><code>range_of( x: ?[] ) -&gt; ( range: integer[] )</code> returns the range from 0 (inclusive) to the length of array <code>x</code> (exclusive).</p>
</li>
<li>
<p><code>range_of( x: string ) -&gt; ( range: integer[] )</code> returns the range from 0 (inclusive) to the length of string <code>x</code> (exclusive).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Furthermore, explicit type casting can be forced using the type names of primitive types as unary functions (<code>scalar</code>, <code>integer</code>, <code>logical</code>, <code>string</code>):</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>scalar( x: logical ) -&gt; ( y: scalar )</code> returns <code>1.0</code> if the passed value is <code>true</code> and <code>0.0</code> otherwise.</p>
</li>
<li>
<p><code>scalar( x: string ) -&gt; ( y: scalar )</code> returns the scalar representation of a string if it describes a valid scalar literal value (according to the syntax), otherwise the invocation is invalid.</p>
</li>
<li>
<p><code>scalar( x: integer ) -&gt; ( y: scalar )</code> returns the same value as passed in, only the type is changed.</p>
</li>
<li>
<p><code>integer( x: logical ) -&gt; ( y: integer )</code> returns <code>1</code> if the passed value is <code>true</code> and <code>0</code> otherwise.</p>
</li>
<li>
<p><code>integer( x: string ) -&gt; ( y: integer )</code> returns the integer representation of a string if it describes a valid integer literal value (according to the syntax), otherwise the invocation is invalid.</p>
</li>
<li>
<p><code>integer( x: scalar ) -&gt; ( y: integer )</code> returns the passed value truncated to the closest smaller integer value.</p>
</li>
<li>
<p><code>logical( x: integer ) -&gt; ( y: logical )</code> returns <code>false</code> if the passed value is <code>0</code> and <code>true</code> otherwise.</p>
</li>
<li>
<p><code>logical( x: scalar ) -&gt; ( y: logical )</code> returns <code>false</code> if the passed value is <code>0.0</code> and <code>true</code> otherwise.</p>
</li>
<li>
<p><code>logical( x: string ) -&gt; ( y: logical )</code> returns <code>false</code> if the passed string is the empty string (<code>''</code>) and <code>true</code> otherwise.</p>
</li>
<li>
<p><code>string( x: ? ) -&gt; ( y: string )</code> returns the string representation of any value of primitive type according to its literal representation in the syntax.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The <code>shape_of</code> function is <strong>deprecated</strong> and is discouraged from use. The reason is that it provides syntactic means to access a property of tensors that is not defined via the syntax itself. Furthermore, its definition is problematic in cases where the shape of a tensor is not known in graph compilation time. These result in problems with custom operations and operations with results of dynamic shape for a consumer of an NNEF document. By removing support for the <code>shape_of</code> function from NNEF syntax, it becomes possible to de-couple parsing from shape propagation in a consumer of an NNEF document.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect4">
<h5 id="branching">Compile-time Branching</h5>
<div class="paragraph">
<p>Compile-time branching is achieved via the syntax <code>z = x if condition else y</code>. The <code>condition</code> <strong>must</strong> be an expression of type <code>logical</code> (thus its value is known at compile time). Furthermore, the evaluation of <code>x</code> or <code>y</code> <strong>should</strong> be lazy, that is, after the <code>condition</code> is evaluated, only the appropriate one of <code>x</code> and <code>y</code> <strong>should</strong> be evaluated, this way allowing the unevaluated to be invalid as well (for example indexing an array out of bounds), which is necessary when expressing certain constructions such as recursion, as in the <code>add_n</code> operation (defined in <a href="#stdlib">Compound Operations</a>):</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment add_n( x: tensor&lt;scalar&gt;[] ) -&gt; ( y: tensor&lt;scalar&gt; )
{
    y = x[0] + add_n(x[1:]) if length_of(x) &gt; 0 else 0.0;
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the above example of recursive addition, when <code>length_of(x) == 0</code> both expressions <code>x[0]</code> and <code>x[1:]</code> would be invalid, but are not considered because of lazy evaluation.</p>
</div>
</div>
<div class="sect4">
<h5 id="looping">Compile-time Looping</h5>
<div class="paragraph">
<p>An obvious way of looping is to use recursion, as in the above example. However, it is often very cumbersome to write and hard to understand.</p>
</div>
<div class="paragraph">
<p>A simpler way to achieve parallel looping is <em>array comprehension</em>, which generates an array by iterating another one and transforming its items. In general, the expression <code>b = [for i in a yield f(i)]</code> iterates the array <code>a</code> and generates items by applying <code>f</code> to each item <code>i</code>. The length of the resulting array is equal to that of the iterated array. Multiple loop iterators may exist, such as in <code>c = [for i in a, j in b yield i + j]</code>. In this case, the iterated arrays <strong>must</strong> have the same length. The loop iterator <code>i</code> may also be an index running through a range, as in <code>b = [for i in range_of(a) yield f(i,a[i])]</code>. Optionally, a condition can be provided to filter the resulting items: <code>b = [for i in a if c(i) yield f(i)]</code> outputs an item only if condition <code>c(i)</code> evaluates to <code>true</code>. In this case, the length of the output is equal to the number of items for which the condition is met.</p>
</div>
</div>
<div class="sect4">
<h5 id="invocation-chaining">Invocation Chaining</h5>
<div class="paragraph">
<p>Operation invocations can be chained just like function calls in programming languages, in which case the result of an operation is the input to another operation. As an example, the chain <code>z = g(f(x), y)</code> is equivalent to the following sequence of invocations</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>t = f(x);
z = g(t,y);</code></pre>
</div>
</div>
<div class="paragraph">
<p>However, chaining is only allowed if the operation <code>f</code> returns a single tensor, since the left-hand-side of the invocation (the implicit intermediate identifier <code>t</code>) <strong>must</strong> be structurally equivalent to the result type of operation <code>f</code>, and that is only satisfied if it returns a single tensor.</p>
</div>
<div class="paragraph">
<p>Note, that expressions containing multiple operators are also implicitly chained. For example <code>a = b + c * d</code> is equivalent to <code>a = add(b, mul(c, d))</code>.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="exported-ids">3.3.4. Exported Identifiers</h4>
<div class="paragraph">
<p>Certain tensors need to be referenced from outside the main description to be able to attach further information to the graph (such as parameter data or quantization information). There are two types of tensors that are possible to reference: variables and activation tensors.</p>
</div>
<div class="paragraph">
<p>Variables are declared with an explicit string label (see <a href="#variable-op">Variables</a>), and this label must be globally unique (except for shared weights), hence it can be used for referencing a variable. This label is used for example to attach tensor data to variables that are serialized.</p>
</div>
<div class="paragraph">
<p>Activation tensors in the graph body also have a globally unique identifier (the left-hand side of assignments must have previously unused names), hence these can also be used to reference activation tensors, for example to attach quantization information to activations.</p>
</div>
<div class="paragraph">
<p>Note, that variables can also be part of the main graph description, and hence they may be referenced by two mechanisms (the string label of the variable definition, and the identifier name used in the assignment).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>variable42 = variable(label = 'block1/conv1/weights', ...);</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the above example, the names 'variable42' and 'block1/conv1/weights' refer to the same tensor, albeit for different purposes.</p>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="primitives">4. Operations</h2>
<div class="sectionbody">
<div class="paragraph">
<p>This chapter describes the (primitive) operations that can be used to build neural network computations. The description of operations contains</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The declaration of each primitive using the syntax introduced in chapter <a href="#formal-description">Formal Description</a>.</p>
</li>
<li>
<p>The description of parameters in the declaration.</p>
</li>
<li>
<p>Argument validity constraints related to each primitive, including input-output shape relations.</p>
</li>
<li>
<p>The semantics of the operation, that is, mathematical formulae describing how the outputs are calculated from the inputs and the attributes of the operation.</p>
</li>
</ul>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Some operations are described as compounds via other primitives using NNEF syntax. Such a description is purely functional, that is, to describe what the operation <strong>must</strong> compute. It bears no restriction to its actual implementation in a consumer of an NNEF document; it may be treated as an atomic operation by a consumer, or it may be decomposed to primitives in a different but functionally equivalent way.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="paragraph">
<p>Operations can be grouped into a few larger categories, while some operations are one-of-a-kind. The larger groups are:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><a href="#introduction-ops">Tensor Introducing Operations</a></p>
</li>
<li>
<p><a href="#elementwise-ops">Element-wise Operations</a></p>
</li>
<li>
<p><a href="#sliding-ops">Sliding-Window Operations</a></p>
</li>
<li>
<p><a href="#reduce-ops">Reduce Operations</a></p>
</li>
<li>
<p><a href="#shape-ops">Tensor Shape Operations</a></p>
</li>
<li>
<p><a href="#roi-ops">Region-of-Interest Operations</a></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Some operations treat the first two dimensions in a special way. The first dimension (index 0) is considered the <em>batch</em> dimension, the second (index 1) the <em>channel</em> dimension. The rest of the dimensions are called the <em>spatial</em> dimensions.</p>
</div>
<div class="sect2">
<h3 id="introduction-ops">4.1. Tensor Introducing Operations</h3>
<div class="paragraph">
<p>The following operations introduce tensors that are not the result of a calculation, such as external inputs to the computation or parameters like weights and biases.</p>
</div>
<div class="sect3">
<h4 id="external-op">4.1.1. External Data Sources</h4>
<div class="paragraph">
<p>An external data source is a tensor which must be fed to the graph from the outside.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment external&lt;? = scalar&gt;(
    shape: integer[] )          # the shape of the tensor
-&gt; ( output: tensor&lt;?&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Items in <code>shape</code> <strong>must</strong> be strictly positive.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The shape of <code>output</code> is equal to <code>shape</code>. The rank of <code>output</code> is equal to the length of <code>shape</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The content of <code>output</code> is not defined by the operation. The tensor must be fed with input data before each execution of the graph. The content of the tensor is <em>not</em> expected to persist between subsequent invocations of the computational graph.</p>
</div>
</div>
<div class="sect3">
<h4 id="constant-op">4.1.2. Constants</h4>
<div class="paragraph">
<p>A constant is a tensor that has a fixed value.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment constant&lt;? = scalar&gt;(
    shape: integer[],           # the shape of the tensor
    value: ?[] )                # the values to fill the tensor with
-&gt; ( output: tensor&lt;?&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Items in <code>shape</code> <strong>must</strong> be strictly positive.</p>
</li>
<li>
<p>The length of <code>value</code> <strong>must</strong> equal the volume implied by <code>shape</code> or <strong>must</strong> be 1.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The shape of <code>output</code> is equal to <code>shape</code>. The rank of <code>output</code> is equal to the length of <code>shape</code>.</p>
</li>
<li>
<p><code>output</code> is filled with values such that its row-major ordering equals the items in <code>value</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Note, that a constant tensor of singular dimensions can be simply written as a numeric literal directly into expressions, so <code>y = x + constant(shape = [1], value = [3.14])</code> is equivalent to <code>y = x + 3.14</code>, where <code>x</code> is a tensor of arbitrary shape. If the length of <code>value</code> is 1, that single value is repeated, so <code>constant(shape = [1,3], value = [3.14])</code> is equivalent to <code>constant(shape = [1,3], value = [3.14, 3.14, 3.14])</code>.</p>
</div>
</div>
<div class="sect3">
<h4 id="variable-op">4.1.3. Variables</h4>
<div class="paragraph">
<p>A <code>variable</code> is a tensor that may be fed an initial value, may be updated by a computation, and its value persists between consecutive invocations of the computational graph. When a tensor is introduced as a variable, it is possible to update it later (see <a href="#update-op">Variable Updates</a>).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment variable&lt;? = scalar&gt;(
    shape: integer[],           # the shape of the tensor
    label: string )             # a label for referencing the tensor externally
-&gt; ( output: tensor&lt;?&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Items in <code>shape</code> <strong>must</strong> be strictly positive.</p>
</li>
<li>
<p><code>label</code> <strong>must not</strong> be empty. Labels <strong>must</strong> only contain the following characters: <code>[a-z]</code>, <code>[A-Z]</code>, <code>[0-9]</code>, <code>_</code>, <code>-</code>, <code>.</code>, <code>/</code>, <code>\</code>. If a variable operation has the same <code>label</code> as another variable operation in the graph (according to case insensitive comparison), then they share the underlying data, and they <strong>must</strong> have the same <code>shape</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The shape of <code>output</code> is equal to <code>shape</code>. The rank of <code>output</code> is equal to the length of <code>shape</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The content of <code>output</code> is not defined by the operation. The tensor may be filled with data from an external source and may be updated by <a href="#update-op">update</a> operations. The <code>label</code> argument is used to link the result to externally <a href="#storing-data">stored</a> tensor data.</p>
</div>
</div>
</div>
<div class="sect2">
<h3 id="elementwise-ops">4.2. Element-wise Operations</h3>
<div class="paragraph">
<p>Element-wise operations perform the same operation on each element of a tensor, irrespective of tensor dimensions and extents. The operations can be defined by simple mathematical formulae. In what follows, for the sake of simplicity, we will use one-dimensional indexing (index <code>i</code> runs through the whole tensor) and C style pseudo-code to define the result of each operation.</p>
</div>
<div class="sect3">
<h4 id="unary-ops">4.2.1. Unary Operations</h4>
<div class="paragraph">
<p>Unary operations have a single tensor argument and return a single result.</p>
</div>
<div class="paragraph">
<p>Arithmetic operations:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment copy&lt;?&gt;( x: tensor&lt;?&gt; ) -&gt; ( y: tensor&lt;?&gt; )
fragment neg( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
fragment rcp( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
fragment exp( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
fragment log( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
fragment sin( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
fragment cos( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
fragment abs( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
fragment sign( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p>Logical operations:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment not( x: tensor&lt;logical&gt; ) -&gt; ( y: tensor&lt;logical&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p>Rounding operations:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment floor( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
fragment ceil ( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
fragment round( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The shape of <code>y</code> is equal to that of <code>x</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For the operation <code>neg</code>, the output is defined as:</p>
</div>
<div class="stemblock">
<div class="content">
\[y[i] = -x[i]\]
</div>
</div>
<div class="paragraph">
<p>For the operation <code>rcp</code>, the output is defined as:</p>
</div>
<div class="stemblock">
<div class="content">
\[y[i] = \frac{1}{x[i]}\]
</div>
</div>
<div class="paragraph">
<p>For the operation <code>exp</code>, the output is defined as:</p>
</div>
<div class="stemblock">
<div class="content">
\[yi] = e^{x[i]}\]
</div>
</div>
<div class="paragraph">
<p>For the operation <code>log</code>, the output is defined as:</p>
</div>
<div class="stemblock">
<div class="content">
\[y[i] = \ln x[i]\]
</div>
</div>
<div class="paragraph">
<p>For the operation <code>sin</code>, the output is defined as:</p>
</div>
<div class="stemblock">
<div class="content">
\[y[i] = \sin x[i]\]
</div>
</div>
<div class="paragraph">
<p>For the operation <code>cos</code>, the output is defined as:</p>
</div>
<div class="stemblock">
<div class="content">
\[y[i] = \cos x[i]\]
</div>
</div>
<div class="paragraph">
<p>For the operation <code>abs</code>, the output is defined as:</p>
</div>
<div class="stemblock">
<div class="content">
\[y[i] =
\begin{cases}
x[i] &amp; \text{if } x[i] \geq 0 \\
-x[i] &amp; \text{if } x[i] &lt; 0 \\
\end{cases}\]
</div>
</div>
<div class="paragraph">
<p>For the operation <code>sign</code>, the output is defined as:</p>
</div>
<div class="stemblock">
<div class="content">
\[y[i] =
\begin{cases}
1 &amp; \text{if } x[i] &gt; 0 \\
0 &amp; \text{if } x[i] = 0 \\
-1 &amp; \text{if } x[i] &lt; 0 \\
\end{cases}\]
</div>
</div>
<div class="paragraph">
<p>For the operation <code>floor</code>, the output is defined as:</p>
</div>
<div class="stemblock">
<div class="content">
\[y[i] = \textrm{floor} ( x[i] )\]
</div>
</div>
<div class="paragraph">
<p>For the operation <code>ceil</code>, the output is defined as:</p>
</div>
<div class="stemblock">
<div class="content">
\[y[i] = \textrm{ceil} ( x[i] )\]
</div>
</div>
<div class="paragraph">
<p>For the operation <code>round</code>, the output is defined as:</p>
</div>
<div class="stemblock">
<div class="content">
\[y[i] = \textrm{floor} ( x[i] + 0.5 )\]
</div>
</div>
</div>
<div class="sect3">
<h4 id="binary-ops">4.2.2. Binary Operations</h4>
<div class="paragraph">
<p>Binary operations take two tensor arguments and produce a single result. In the basic case of binary operations, by default, the shapes of both input tensors are the same, and the resulting output will also have the same shape. However, binary operations also support the case when the either operand has singleton shape in a dimension but the other operand is non-singleton; in this case the value of the singleton dimension is repeated (broadcast) along that dimension (for example adding a column vector to all columns of a matrix). An extreme case is a scalar operand repeated in all dimensions.</p>
</div>
<div class="paragraph">
<p>Arithmetic operations:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment add( x: tensor&lt;scalar&gt;, y: tensor&lt;scalar&gt; ) -&gt; ( z: tensor&lt;scalar&gt; )
fragment sub( x: tensor&lt;scalar&gt;, y: tensor&lt;scalar&gt; ) -&gt; ( z: tensor&lt;scalar&gt; )
fragment mul( x: tensor&lt;scalar&gt;, y: tensor&lt;scalar&gt; ) -&gt; ( z: tensor&lt;scalar&gt; )
fragment div( x: tensor&lt;scalar&gt;, y: tensor&lt;scalar&gt; ) -&gt; ( z: tensor&lt;scalar&gt; )
fragment pow( x: tensor&lt;scalar&gt;, y: tensor&lt;scalar&gt; ) -&gt; ( z: tensor&lt;scalar&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p>Comparison operations:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment lt( x: tensor&lt;scalar&gt;, y: tensor&lt;scalar&gt; ) -&gt; ( z: tensor&lt;logical&gt; )
fragment gt( x: tensor&lt;scalar&gt;, y: tensor&lt;scalar&gt; ) -&gt; ( z: tensor&lt;logical&gt; )
fragment le( x: tensor&lt;scalar&gt;, y: tensor&lt;scalar&gt; ) -&gt; ( z: tensor&lt;logical&gt; )
fragment ge( x: tensor&lt;scalar&gt;, y: tensor&lt;scalar&gt; ) -&gt; ( z: tensor&lt;logical&gt; )
fragment eq( x: tensor&lt;scalar&gt;, y: tensor&lt;scalar&gt; ) -&gt; ( z: tensor&lt;logical&gt; )
fragment ne( x: tensor&lt;scalar&gt;, y: tensor&lt;scalar&gt; ) -&gt; ( z: tensor&lt;logical&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p>Logical operations:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment and( x: tensor&lt;logical&gt;, y: tensor&lt;logical&gt; ) -&gt; ( z: tensor&lt;logical&gt; )
fragment or ( x: tensor&lt;logical&gt;, y: tensor&lt;logical&gt; ) -&gt; ( z: tensor&lt;logical&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>For each dimension, the extents of <code>x</code> and <code>y</code> <strong>must</strong> either be equal or one of them <strong>must</strong> be singular. Inference APIs are <strong>recommended</strong> to support the case when broadcasting happens in the <em>batch</em> and in the <em>spatial</em> dimensions (<em>channel</em> dimensions equal), and the case when broadcasting happens in all dimensions (one of the arguments is a scalar).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>z</code> is the maximum of the rank of <code>x</code> and <code>y</code>. For each dimension, if the extents of <code>x</code> and <code>y</code> are equal, the extent is carried over to <code>z</code>. Otherwise, the non-singular extent is carried over to <code>z</code>.</p>
</li>
<li>
<p>The computations performed by these operators are as usual (see <a href="#built-in-operators">Built-in Operators</a> for their mapping to mathematical operators).</p>
</li>
</ul>
</div>
</div>
<div class="sect3">
<h4 id="select-op">4.2.3. Select Operation</h4>
<div class="paragraph">
<p>The ternary operation <code>select</code> returns either of two values based on a condition (per element).</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment select&lt;?&gt;(
    condition: tensor&lt;logical&gt;,     # the condition for selecting the result
    true_value: tensor&lt;?&gt;,          # the result when the condition is true
    false_value: tensor&lt;?&gt; )        # the result when the condition is false
-&gt; ( output: tensor&lt;?&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>For each dimension, the extents of <code>condition</code>, <code>true_value</code> and <code>false_value</code> <strong>must</strong> either be equal or some of them <strong>must</strong> be singular.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>For each dimension, if the extents of <code>condition</code>, <code>true_value</code> and <code>false_value</code> are equal, the extent is carried over to <code>output</code>. Otherwise, the non-singular extent is carried over to <code>output</code></p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The selection condition is evaluated independently for each entry (if <code>condition</code> shape is not singular). The <code>output</code> equals <code>true_value</code> where <code>condition</code> is <code>true</code> and <code>false_value</code> otherwise. Arguments of singular shape are broadcast to the shape of <code>output</code>.</p>
</div>
<div class="paragraph">
<p>As a special case, the condition may evaluate to <code>true</code> or <code>false</code> for all items (in runtime), in which case the whole subgraph calculating <code>true_value</code> or <code>false_value</code> <strong>can</strong> be omitted (in runtime), which provides a chance for optimization (conditional execution). This is especially useful if the shape of <code>condition</code> is singular in all dimensions. For example:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment calculate_condition( data: tensor&lt;scalar&gt; )
-&gt; ( condition: tensor&lt;logical&gt; ) { ... }

fragment calculate_more_outputs( data: tensor&lt;scalar&gt; )
-&gt; ( output: tensor&lt;scalar&gt; ) { ... }

data = ...
condition = calculate_condition(data)
output = select(condition, calculate_more_outputs(data), 0.0)</code></pre>
</div>
</div>
<div class="paragraph">
<p>In the above example, if <code>condition</code> evaluates to <code>false</code>, the whole <code>calculate_more_outputs()</code> invocation can be omitted, which may contain an arbitrary large sub-graph.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Although the operation <code>select</code> and the <code>if-else</code> syntactic construct share similarities, they serve different purposes; the <code>select</code> operation serves <em>run-time</em> branching, while the <code>if-else</code> construct serves <em>compile-time</em> branching (in compositional syntax).</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_simplifier_operations">4.2.4. Simplifier Operations</h4>
<div class="paragraph">
<p>Some convenience operations are provided for often-used element-wise operations that can be expressed via other primitives.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment sqr( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
{
    y = pow(x, 2.0);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment sqrt( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
{
    y = pow(x, 0.5);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment rsqr( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
{
    y = pow(x, -2.0);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment rsqrt( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
{
    y = pow(x, -0.5);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment log2( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
{
    y = log(x) / log(2.0);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment min( x: tensor&lt;scalar&gt;, y: tensor&lt;scalar&gt; ) -&gt; ( z: tensor&lt;scalar&gt; )
{
    z = select(x &lt; y, x, y);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment max( x: tensor&lt;scalar&gt;, y: tensor&lt;scalar&gt; ) -&gt; ( z: tensor&lt;scalar&gt; )
{
    z = select(x &gt; y, x, y);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment clamp( x: tensor&lt;scalar&gt;, a: tensor&lt;scalar&gt;, b: tensor&lt;scalar&gt; )
-&gt; ( y: tensor&lt;scalar&gt; )
{
    y = max(min(x, b), a);
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="sliding-ops">4.3. Sliding-Window Operations</h3>
<div class="paragraph">
<p>Sliding-window operations come in pairs, a 'basic' version and a 'reverse' version (it could also be called forward and backward version, however, we would like to avoid confusion with the backward computation of back-propagation, where the backward computation of a 'reverse' operation may be a 'basic' operation). In general, the basic operations either keep the input shape or down-scale the input, while the reverse operations keep the input shape or up-scale it. Therefore, to clearly denote which shape we are talking about, we will denote them <em>down-scaled</em> and <em>up-scaled</em>, respectively. The basic operations map from the up-scaled shape to the down-scaled shape, and the reverse operations map from the down-scaled shape to the up-scaled shape.</p>
</div>
<div class="paragraph">
<p>Most of the parameters of sliding-window operations are common, thus they are summarized here, and for each operation only the additional parameters are described.</p>
</div>
<div id="sliding-params" class="paragraph">
<p><strong><em>Common Parameters</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>input: tensor&lt;scalar&gt;</code> : the tensor to be filtered.</p>
</li>
<li>
<p><code>output: tensor&lt;scalar&gt;</code> : the resulting tensor.</p>
</li>
<li>
<p><code>size: integer[]</code> : the shape of the kernel.</p>
</li>
<li>
<p><code>border: string</code> : the mode by which the borders are handled (see below).</p>
</li>
<li>
<p><code>padding: (integer,integer)[]</code> : the extents of the padding applied on the edges of the input. The values are supplied separately for each dimension and both sides (may be asymmetric). An empty array indicates that the padding extents are calculated automatically, see below.</p>
</li>
<li>
<p><code>stride: integer[]</code> : the amount with which the kernel is displaced in input space when moving the window.</p>
</li>
<li>
<p><code>dilation: integer[]</code> : the amount with which the kernel entries are displaced while matching the kernel to the input in a single input position. Dilation of 1 means that the kernel is continuously matched to the input (no gap).</p>
</li>
<li>
<p><code>output_shape: integer[]</code> : the reference shape for reverse operations, typically the up-scaled shape of the input of a previous basic operation. An empty array indicates no reference shape, so the output shape is calculated as indicated below. If <code>output_shape</code> is not empty, then applying the down-scaled shape calculation to <code>output_shape</code> <strong>must</strong> result in the shape of <code>input</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The parameters <code>size</code>, <code>padding</code>, <code>stride</code> and <code>dilation</code> <strong>must</strong> contain one entry for each relevant dimension. Alternatively, the arrays <code>padding</code>, <code>stride</code> and <code>dilation</code> may be empty (<code>[]</code>). If the arrays <code>stride</code> and <code>dilation</code> are empty, they are considered to be 1s in all relevant dimensions. If the array <code>padding</code> is empty, then padding is calculated automatically, see below.</p>
</div>
<div class="paragraph">
<p>The <code>padding</code>, <code>stride</code> and <code>dilation</code> parameters are always interpreted in the up-scaled space (the input space for the basic operations, the output space for reverse operations). The <code>border</code> parameter however, always effects the input values of each operation, regardless of whether it is basic or reverse operation.</p>
</div>
<div class="paragraph">
<p><strong><em>Argument Validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Items in <code>size</code> <strong>must</strong> be strictly positive. The number of required items depends on the operation.</p>
</li>
<li>
<p>The number of required items in <code>padding</code> depends on the specific operation. Note, that the items in <code>padding</code> <strong>can</strong> be negative. Inference APIs are <strong>recommended</strong> to support the cases when all padding items are less in absolute value than the corresponding items in <code>size</code>.</p>
</li>
<li>
<p>Items in <code>stride</code> <strong>must</strong> be strictly positive. The number of required items depends on the specific operation. Inference APIs are <strong>recommended</strong> to support the cases when all items are less or equal to the corresponding items in <code>size</code>.</p>
</li>
<li>
<p>Items in <code>dilation</code> <strong>must</strong> be strictly positive. The number of required items depends on the specific operation. Inference APIs <strong>must</strong> support at least the case when all items are singular.</p>
</li>
<li>
<p><code>border</code> <strong>must</strong> be one of the modes specified <a href="#border-modes">below</a>.</p>
</li>
</ul>
</div>
<div id="sliding-shape-calculation" class="paragraph">
<p><strong><em>Output Shape Calculation</em></strong></p>
</div>
<div class="paragraph">
<p>For each dimension, the relation between up-scaled extent (\(X\)), down-scaled extent (\(x\)) filter <code>size</code> (\(f\)), <code>padding</code> (\(p,q\), where \(p\) corresponds to the padding that precedes, and \(q\) corresponds to the padding that succeeds the data), <code>stride</code> (\(s\)) and <code>dilation</code> (\(d\)) is as follows. Define dilated filter size as \(f_d = (f - 1) \cdot d + 1\). Note, that when \(d = 1\) then \(f_d = f\). Then the down-scaled extent is</p>
</div>
<div class="stemblock">
<div class="content">
\[x = \textrm{floor} ( \frac{p + X + q - f_d}{s} ) + 1 \ .\]
</div>
</div>
<div class="paragraph">
<p>The nominator <strong>must</strong> be non-negative, that is, \(p + X + q \geq f_d\). Furthermore, the up-scaled extent is</p>
</div>
<div class="stemblock">
<div class="content">
\[X = (x - 1) \cdot s + f_d - (p + q) \ .\]
</div>
</div>
<div class="paragraph">
<p>Note, that because the down-scaling may involve rounding, the up-scaled extent is not necessarily equal to the one which it was down-scaled from.</p>
</div>
<div id="automatic-padding" class="paragraph">
<p><strong><em>Automatic Padding Calculation</em></strong></p>
</div>
<div class="paragraph">
<p>Padding is automatically calculated from the other parameters in all dimensions where it is not specified, such that \(x = \textrm{ceil} \big ( \frac{X}{s} \big )\). To achieve the desired down-scaled extent, define total padding as</p>
</div>
<div class="stemblock">
<div class="content">
\[t = \textrm{max}((x - 1) \cdot s + f_d - X, 0)\]
</div>
</div>
<div class="paragraph">
<p>and let the possibly asymmetric padding be defined as</p>
</div>
<div class="stemblock">
<div class="content">
\[\begin{eqnarray*}
p &amp;=&amp; \textrm{floor} ( \frac{t}{2} ) \\
q &amp;=&amp; \textrm{ceil} ( \frac{t}{2} )
\end{eqnarray*}\]
</div>
</div>
<div class="paragraph">
<p>Note, that for all trailing dimensions, where \(f = 1\) and \(s = 1\) we have \(x = X\) and as a result \(p = 0\) and \(q = 0\).</p>
</div>
<div id="border-modes" class="paragraph">
<p><strong><em>Border Modes</em></strong></p>
</div>
<div class="paragraph">
<p>The allowed border modes are as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>'ignore'</code>: the border values are not considered in the calculations (such as max or average)</p>
</li>
<li>
<p><code>'constant'</code> : the border is considered constant, currently the only supported value is 0</p>
</li>
<li>
<p><code>'replicate'</code>: the border is filled with the edge values replicated</p>
</li>
<li>
<p><code>'reflect'</code>: the values are taken from the input as if it was reflected to the edge (the edge is not duplicated)</p>
</li>
<li>
<p><code>'reflect-even'</code>: similar to <code>'reflect'</code>, but the edge is duplicated</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Suppose for the sake of explanation that the input is one dimensional and has extent \(z\). Let \(\tilde{\textrm{input}}\) denote the extended input, where the values outside the true input range are defined by the <code>border</code> parameter.</p>
</div>
<div class="paragraph">
<p>If <code>border = 'constant'</code>, we have:</p>
</div>
<div class="stemblock">
<div class="content">
\[\tilde{\textrm{input}}[i] =
\begin{cases}
c &amp; \text{if } i &lt; 0 \\
\textrm{input}[i] &amp; \text{if } 0 \leq i &lt; z \\
c &amp; \text{if } i \geq z
\end{cases}\]
</div>
</div>
<div class="paragraph">
<p>For sliding-window operations, only the case of \(c = 0\) is defined.</p>
</div>
<div class="paragraph">
<p>If <code>border = 'replicate'</code>, we have:</p>
</div>
<div class="stemblock">
<div class="content">
\[\tilde{\textrm{input}}[i] =
\begin{cases}
\textrm{input}[0] &amp; \text{if } i &lt; 0 \\
\textrm{input}[i] &amp; \text{if } 0 \leq i &lt; z \\
\textrm{input}[z-1] &amp; \text{if } i \geq z
\end{cases}\]
</div>
</div>
<div class="paragraph">
<p>If <code>border = 'reflect'</code>, we have:</p>
</div>
<div class="stemblock">
<div class="content">
\[\tilde{\textrm{input}}[i] =
\begin{cases}
\textrm{input}[-i] &amp; \text{if } i &lt; 0 \\
\textrm{input}[i] &amp; \text{if } 0 \leq i &lt; z \\
\textrm{input}[z-1-(i-z+1)] &amp; \text{if } i \geq z
\end{cases}\]
</div>
</div>
<div class="paragraph">
<p>If <code>border = 'reflect-even'</code>, we have:</p>
</div>
<div class="stemblock">
<div class="content">
\[\tilde{\textrm{input}}[i] =
\begin{cases}
\textrm{input}[-i-1] &amp; \text{if } i &lt; 0 \\
\textrm{input}[i] &amp; \text{if } 0 \leq i &lt; z \\
\textrm{input}[z-1-(i-z)] &amp; \text{if } i \geq z
\end{cases}\]
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>Dimensionality of operations is not explicitly denoted, instead, it follows from the dimensions of the arguments. Let the padded input shape in each dimension be defined as \(p + X + q\). If the padded input has rank n, then the operation is n-dimensional, and has n-2 <em>spatial</em> dimensions.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="convolution-ops">4.3.1. Convolution and Deconvolution</h4>
<div class="paragraph">
<p>The <code>conv</code> operation correlates a filter with an input, while the <code>deconv</code> operation performs the reverse, which is roughly equivalent to mathematical convolution. Up and down-scaling of tensor shapes happens only in the <em>spatial</em> dimensions. The <em>batch</em> dimension is the same for inputs and outputs, while the <em>channel</em> dimension of the output is derived from the <em>batch</em> dimension of the filters.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment conv(
    input: tensor&lt;scalar&gt;,
    filter: tensor&lt;scalar&gt;,             # the filter that the input is convolved with
    bias: tensor&lt;scalar&gt; = 0.0,         # the bias that is added to the output
    border: string = 'constant',
    padding: (integer,integer)[] = [],
    stride: integer[] = [],
    dilation: integer[] = [],
    groups: integer = 1 )               # the number of convolution groups
-&gt; ( output: tensor&lt;scalar&gt; )

fragment deconv(
    input: tensor&lt;scalar&gt;,
    filter: tensor&lt;scalar&gt;,
    bias: tensor&lt;scalar&gt; = 0.0,
    border: string = 'constant',
    padding: (integer,integer)[] = [],
    stride: integer[] = [],
    dilation: integer[] = [],
    output_shape: integer[] = [],
    groups: integer = 1 )
-&gt; ( output: tensor&lt;scalar&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>filter</code> <strong>must</strong> equalt to that of <code>input</code>.</p>
</li>
<li>
<p>The role of the common parameter <code>size</code> is performed by the shape of <code>filter</code>, therefore the general validity constraints defined <a href="#sliding-params">above</a> for <code>size</code> apply for the shape of <code>filter</code>. Inference APIs are <strong>recommended</strong> to support cases where the filter&#8217;s <em>spatial</em> dimensions are between 1 and 5, inclusive.</p>
</li>
<li>
<p>The length of <code>padding</code>, <code>stride</code> and <code>dilation</code> <strong>must</strong> equal the number of spatial dimensions or 0. Item \(i\) in <code>padding</code>, <code>stride</code> and <code>dilation</code> corresponds to <em>spatial</em> dimension \(i\). Inference APIs are <strong>recommended</strong> to support cases where the <code>stride</code> is 1 or 2.</p>
</li>
<li>
<p><code>groups</code> <strong>must</strong> be non-negative, and if positive, it <strong>must</strong> be a divisor of the <em>batch</em> dimension of the shape of <code>filter</code>. The special value 0 indicates <a href="#grouped-convolution">depth-wise separable convolution</a>.</p>
</li>
<li>
<p>The <em>channel</em> dimension of the <code>filter</code> times the number of <code>groups</code> <strong>must</strong> equal the <em>channel</em> dimension of the <code>input</code>.</p>
</li>
<li>
<p>The <em>channel</em> dimension of <code>bias</code> <strong>must</strong> equal the <em>batch</em> dimension of <code>filter</code> or <strong>must</strong> be singular. In all other dimensions, <code>bias</code> <strong>must</strong> have singular shape.</p>
</li>
<li>
<p><code>border</code> <strong>must</strong> be one of <code>'constant'</code>, <code>'replicate'</code>, <code>'reflect'</code>, <code>'reflect-even'</code>. Inference APIs are <strong>recommended</strong> to support cases where <code>border</code> is <code>'constant'</code>.</p>
</li>
<li>
<p>The length of <code>output_shape</code> <strong>must</strong> equal the rank of <code>input</code> or 0.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>output</code> is the same as the rank of <code>input</code>.</p>
</li>
<li>
<p>The extent of the <em>batch</em> dimension of the shape of <code>output</code> equals that of the <code>input</code>.</p>
</li>
<li>
<p>The extent of the <em>channel</em> dimension of the shape of <code>output</code> equals the extent of the <em>batch</em> dimension of the shape of <code>filter</code>.</p>
</li>
<li>
<p>For each other dimension, the shape of <code>output</code> is calculated as defined <a href="#sliding-shape-calculation">above</a> for sliding window operations (\(x\) in case of <code>conv</code> and \(X\) in case of <code>deconv</code>).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Operation <code>conv</code> maps from the up-sampled space to the down-sampled space, while <code>deconv</code> maps from the down-sampled space to the up-sampled space.</p>
</div>
<div class="paragraph">
<p>For the sake of explanation, suppose that the <code>input</code> and <code>filter</code> tensors are one dimensional (there is only one <em>spatial</em> dimension, <em>batch</em> and <em>channel</em> dimensions are singular). Furthermore, let <code>bias</code> be omitted (as if it was zero). Then for each index \(i \in [0,x)\), the output of <code>conv</code> is calculated as (mathematical correlation):</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output}[i] = \sum_{j=0}^{f-1} \tilde{\textrm{input}}[i \cdot s + j \cdot d - p] \cdot \textrm{filter}[j]\]
</div>
</div>
<div class="paragraph">
<p>The <code>deconv</code> operation implements a calculation as if it was the reverse of correlation (i.e. mathematical convolution), taking stride and dilation into account. For each index \(i \in [0,X)\), the output of <code>deconv</code> is calculated as:</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output}[i] = \sum_{j=0}^{f-1} \tilde{\textrm{input}}[(i + p - j \cdot d) \ / \ s] \cdot \textrm{filter}[j]\]
</div>
</div>
<div class="paragraph">
<p>where the sum only includes terms for which \((i + p - j \cdot d) \mod s \ = \ 0\).</p>
</div>
<div class="paragraph">
<p>In general, the up-scaled space has dimensions \((B,C,X_1,X_2,...)\), the down-scaled space has shape \((B,c,x_1,x_2,...)\), and the filter has dimensions \((c,C,f_1,f_2,...)\). The following equations will suppose two <em>spatial</em> dimensions, but generalization to more dimensions is straightforward.</p>
</div>
<div class="paragraph">
<p>In case of the <code>conv</code> operation, for each batch index \(b \in [0..B)\) and for each \(k_2 \in [0..c)\), the output is calculated as:</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output}[b][k_2][i_1][i_2] = \sum_{k_1=0}^{C-1} \sum_{j_1=0}^{f_1-1} \sum_{j_2=0}^{f_2-1} \tilde{\textrm{input}}[b][k_1][i_1 \cdot s_1 + j_1 \cdot d_1 - p_1][i_2 \cdot s_2 + j_2 \cdot d_2 - p_2] \cdot \textrm{filter}[k_2][k_1][j_1][j_2]\]
</div>
</div>
<div class="paragraph">
<p>and in case of <code>deconv</code>, for each \(k_1 \in [0..C)\):</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output}[b][k_1][i_1][i_2] = \sum_{k_2=0}^{c-1} \sum_{j_1=0}^{f_1-1} \sum_{j_2=0}^{f_2-1} \tilde{\textrm{input}}[b][k_2][(i_1 + p_1 - j_1 \cdot d_1) \ / \ s_1][(i_2 + p_2 - j_2 \cdot d_2) \ / \ s_2] \cdot \textrm{filter}[k_2][k_1][j_1][j_2]\]
</div>
</div>
<div class="paragraph">
<p>When taking <code>bias</code> into account, the output is defined as if <code>bias</code> was added to the result of convolution without bias:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>conv(input, filter, bias, ...) = conv(input, filter, bias = 0.0, ...) + bias</code></pre>
</div>
</div>
<div class="sect4">
<h5 id="grouped-convolution">Grouped Convolutions</h5>
<div class="paragraph">
<p>In case of grouped convolution/deconvolution (<code>groups &gt; 1</code>), conceptually, both the <code>input</code> and <code>output</code> are split into <code>groups</code> number of segments along the <em>channel</em> dimension, and the operations are executed independently on the split segments. Note, that in this case, the <em>channel</em> dimension of the <code>filter</code> must be just the right extent to match one segment&#8217;s <em>channel</em> dimension. Letting \(G\) denote the number of groups, the <code>filter</code> must be of shape \((c,C/G,f_1,f_2,...)\). Note, that the number of output channels <em>per group</em> will be \(c / G\).</p>
</div>
<div class="paragraph">
<p>A special case of grouped convolution is when \(C = G\), in which case the convolution is performed independently for each channel (also called depth-wise separable or plane-wise). The ratio \(c / G\) is also known as <em>channel multiplier</em> in this case. The special value <code>groups = 0</code> is a shorthand for specifying depth-wise separable convolution without requiring to reference the actual number of channels.</p>
</div>
</div>
</div>
<div class="sect3">
<h4 id="box-ops">4.3.2. Box Filter</h4>
<div class="paragraph">
<p>Box filtering performs summation in a local window.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment box(
    input: tensor&lt;scalar&gt;,
    size: integer[],
    border: string = 'constant',
    padding: (integer,integer)[] = [],
    stride: integer[] = [],
    dilation: integer[] = [],
    normalize: logical = false )        # whether to normalize by the kernel volume
-&gt; ( output: tensor&lt;scalar&gt; )

fragment debox(
    input: tensor&lt;scalar&gt;,
    size: integer[],
    border: string = 'constant',
    padding: (integer,integer)[] = [],
    stride: integer[] = [],
    dilation: integer[] = [],
    output_shape: integer[] = [],
    normalize: logical = false )
-&gt; ( output: tensor&lt;scalar&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Inference APIs are <strong>recommended</strong> to support cases where <code>size</code> is between 1 and 3, inclusive.</p>
</li>
<li>
<p>The length of <code>padding</code>, <code>stride</code> and <code>dilation</code> <strong>must</strong> equal the rank of <code>input</code> or 0. Item \(i\) in <code>padding</code>, <code>stride</code> and <code>dilation</code> corresponds to dimension \(i\). Inference APIs are <strong>recommended</strong> to support cases where <code>stride</code> is 1 or 2.</p>
</li>
<li>
<p><code>border</code> <strong>must</strong> be one of <code>'ignore'</code>, <code>'constant'</code>, <code>'replicate'</code>, <code>'reflect'</code>, <code>'reflect-even'</code>. Inference APIs are <strong>recommended</strong> to support the <code>'constant'</code> value.</p>
</li>
<li>
<p>The length of <code>output_shape</code> <strong>must</strong> equal the rank of <code>input</code> or 0.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>output</code> is the same as the rank of <code>input</code>. For each dimension, the shape of <code>output</code> is calculated as defined <a href="#sliding-shape-calculation">above</a> for sliding window operations (\(x\) in case of <code>box</code> and \(X\) in case of <code>debox</code>).</p>
</li>
<li>
<p>When <code>normalize = true</code>, the output is divided with the volume of the kernel (as indicated by the <code>size</code> parameter).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Box filtering in <em>spatial</em> dimensions is equivalent to (depth-wise separable) convolution with a weight tensor of all 1s. Note however, that box filtering does not differentiate <em>batch</em> and <em>channel</em> dimensions, and it may also be applied to those dimensions.</p>
</div>
<div class="paragraph">
<p>For the sake of explanation, suppose that the <code>input</code> tensor is one dimensional. Then in the unnormalized case, for each index \(i \in [0,x)\), the output of <code>box</code> is calculated as:</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output}[i] = \sum_{j=0}^{f-1} \tilde{\textrm{input}}[i \cdot s + j \cdot d - p]\]
</div>
</div>
<div class="paragraph">
<p>The <code>debox</code> operation implements a calculation as if it was the reverse of <code>box</code>, taking stride and dilation into account. For each index \(i \in [0,X)\), the output of <code>debox</code> is calculated as:</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output}[i] = \sum_{j=0}^{f-1} \tilde{\textrm{input}}[(i + p - j \cdot d) \ / \ s]\]
</div>
</div>
<div class="paragraph">
<p>where the sum only includes terms for which \((i + p - j \cdot d) \mod s \ = \ 0\).</p>
</div>
<div class="paragraph">
<p>The box filter is separable in the dimensions, so box filtering in multiple dimensions is equivalent to a sequence of 1-dimensional box filters.</p>
</div>
</div>
<div class="sect3">
<h4 id="_index_based_sampling">4.3.3. Index Based Sampling</h4>
<div class="paragraph">
<p>The operation <code>argmax_pool</code> returns the maximum value positions in the local window.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment argmax_pool(
    input: tensor,
    size: integer[],
    border: string = 'constant',
    padding: (integer,integer)[] = [],
    stride: integer[] = [],
    dilation: integer[] = [] )
-&gt; ( index: tensor&lt;integer&gt; )           # the positions where the maxima are found</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Inference APIs are <strong>recommended</strong> to support cases where <code>size</code> is between 1 and 3, inclusive.</p>
</li>
<li>
<p>The length of <code>padding</code>, <code>stride</code> and <code>dilation</code> <strong>must</strong> equal the rank of <code>input</code> or 0. Item \(i\) in <code>padding</code>, <code>stride</code> and <code>dilation</code> corresponds to dimension \(i\). Inference APIs are <strong>recommended</strong> to support cases where <code>stride</code> is 1 or 2.</p>
</li>
<li>
<p><code>border</code> <strong>must</strong> be one of <code>'ignore'</code>, <code>'constant'</code>, <code>'replicate'</code>, <code>'reflect'</code>, <code>'reflect-even'</code>. Inference APIs are <strong>recommended</strong> to support <code>'constant'</code> and <code>'ignore'</code> values.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>output</code> is the same as the rank of <code>input</code>. For each dimension, the shape of <code>index</code> is calculated as defined <a href="#sliding-shape-calculation">above</a> for sliding window operations (\(x\)).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For each position in the output space, <code>argmax_pool</code> chooses the maximum value in the kernel window, and outputs its index within the window. Indices correspond to row-major ordering of positions within the window. For the sake of explanation, suppose that the <code>input</code> tensor is one dimensional. Then for each index \(i \in [0,x)\), the output of <code>argmax_pool</code> is calculated as:</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{index}[i] = \arg\max_{j=0}^{f-1} \tilde{\textrm{input}}[i \cdot s + j \cdot d - p]\]
</div>
</div>
<div class="paragraph">
<p>If there are multiple equal maximal values, only the first index is stored in <code>index</code>. Note, that the argmax operation is not separable.</p>
</div>
<hr>
<div class="paragraph">
<p>Two related operations are <code>sample</code> and <code>desample</code>. The operation <code>sample</code> performs sampling given the indices, obtained from the <code>argmax_pool</code> operation. That is, it fetches the actual values indicated by the indices. The operation <code>desample</code> performs the reverse, populating the up-scaled space from the values in the down-scaled space using the sampling indices, summing values that are mapped to the same up-scaled position.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment sample(
    input: tensor&lt;scalar&gt;,
    index: tensor&lt;integer&gt;,             # the positions where values are sampled from
    size: integer[],
    border: string = 'constant',
    padding: (integer,integer)[] = [],
    stride: integer[] = [],
    dilation: integer[] = [] )
-&gt; ( output: tensor&lt;scalar&gt; )

fragment desample(
    input: tensor&lt;scalar&gt;,
    index: tensor&lt;integer&gt;,             # the positions where values are sampled to
    size: integer[],
    border: string = 'constant',
    padding: (integer,integer)[] = [],
    stride: integer[] = [],
    dilation: integer[] = [],
    output_shape: integer[] = [] )
-&gt; ( output: tensor&lt;scalar&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Inference APIs are <strong>recommended</strong> to support cases where <code>size</code> is between 1 and 3, inclusive.</p>
</li>
<li>
<p>The shape of <code>index</code> <strong>must</strong> be the same as that of <code>output</code> for <code>sample</code> and that of <code>input</code> for <code>desample</code>.</p>
</li>
<li>
<p>The length of <code>padding</code>, <code>stride</code> and <code>dilation</code> <strong>must</strong> equal the rank of <code>input</code> or 0. Item \(i\) in <code>padding</code>, <code>stride</code> and <code>dilation</code> corresponds to dimension \(i\). Inference APIs are <strong>recommended</strong> to support cases where <code>stride</code> is 1 or 2.</p>
</li>
<li>
<p><code>border</code> <strong>must</strong> be one of <code>'constant'</code>, <code>'replicate'</code>, <code>'reflect'</code>, <code>'reflect-even'</code> in case of <code>sample</code> and <strong>must</strong> be <code>'constant'</code> in case of <code>desample</code>. Inference APIs are <strong>recommended</strong> to support the <code>'constant'</code> value.</p>
</li>
<li>
<p>The length of <code>output_shape</code> <strong>must</strong> equal the rank of <code>input</code> or 0.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>output</code> is the same as the rank of <code>input</code>. For each dimension, the shape of <code>output</code> is calculated as defined <a href="#sliding-shape-calculation">above</a> for sliding window operations (\(x\) in case of <code>sample</code> and \(X\) in case of <code>desample</code>).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For the sake of explanation, suppose that the <code>input</code> and <code>index</code> tensors are one dimensional. Then for each index \(i \in [0,x)\), the output of <code>sample</code> is calculated as:</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output}[i] = \tilde{\textrm{input}}[i \cdot s + \textrm{index}[i] \cdot d - p]\]
</div>
</div>
<div class="paragraph">
<p>For each index \(i \in [0,X)\), the output of <code>desample</code> is calculated as:</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output}[i] = \sum_{\substack{j=0 \\ 0 \ \leq \ (i + p - j \cdot d) \ / \ s \ &lt; \ x \\ \textrm{index}[(i + p - j \cdot d) \ / \ s] \ = \ j}}^{f-1} \textrm{input}[(i + p - j \cdot d) \ / \ s]\]
</div>
</div>
<div class="paragraph">
<p>where the sum only includes terms for which \((i + p - j \cdot d) \mod s \ = \ 0\). Note, that any <code>border</code> value other than <code>'constant'</code> (zeros) loses its utility because the condition \(\textrm{index}[(i + p - j \cdot d) \ / \ s] \ = \ j\) on the sampling indices is never met outside the valid region.</p>
</div>
</div>
<div class="sect3">
<h4 id="up-down-sample">4.3.4. Up and Down-Sampling</h4>
<div class="paragraph">
<p>Tensors may be up and down-sampled along the <em>spatial</em> dimensions. For down-sampling, two methods are given: area interpolation and nearest neighbor interpolation. For up-sampling, also two methods are given: multi-linear interpolation and nearest neighbor interpolation. Only integer scaling factors are allowed, such that these methods are consistent with other sliding window operations. Some of these operations are readily expressible via other primitives.</p>
</div>
<div class="paragraph">
<p>The two down-sampling methods are:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment nearest_downsample( input: tensor&lt;scalar&gt;, factor: integer[] )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    dims = 2 + length_of(factor);       # +2 for batch and channel dimensions
    output = box(input, size = [1] * dims, stride = [1,1] + factor,
                 padding = [(0,0)] * dims);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment area_downsample( input: tensor&lt;scalar&gt;, factor: integer[] )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    dims = 2 + length_of(factor);       # +2 for batch and channel dimensions
    output = box(input, size = [1,1] + factor, stride = [1,1] + factor,
                 padding = [(0,0)] * dims, normalize = true);
}</code></pre>
</div>
</div>
<div class="paragraph">
<p>The two up-sampling methods are:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment nearest_upsample( input: tensor&lt;scalar&gt;, factor: integer[] )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    dims = 2 + length_of(factor);       # +2 for batch and channel dimensions
    output = debox(input, size = [1,1] + factor, stride = [1,1] + factor,
                   padding = [(0,0)] * dims);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment multilinear_upsample(
    input: tensor&lt;scalar&gt;,              # the tensor to up-sample
    factor: integer[],                  # the up-sampling factor
    method: string = 'symmetric',       # the linear interpolation method to use
    border: string = 'replicate' )
-&gt; ( output: tensor&lt;scalar&gt; )           # the up-sampled tensor</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The items in <code>factor</code> <strong>must</strong> be strictly positive. Item \(i\) corresponds to <em>spatial</em> dimension \(i\). Inference APIs are <strong>recommended</strong> to support the case when <code>factor</code> is 2.</p>
</li>
<li>
<p><code>method</code> <strong>must</strong> be one of <code>'asymmetric'</code>, <code>'symmetric'</code>, <code>'aligned'</code> (see below). Inference APIs are <strong>recommended</strong> to support one of values <code>'asymmetric'</code> or <code>'symmetric'</code>.</p>
</li>
<li>
<p><code>border</code> <strong>must</strong> be one of <code>'constant'</code>, <code>'replicate'</code>, <code>'reflect'</code>, <code>'reflect-even'</code>. Inference APIs are <strong>recommended</strong> to support the value <code>'constant'</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>output</code> is the same as the rank of <code>input</code>.</p>
</li>
<li>
<p>For the <em>batch</em> and <em>channel</em> dimensions, the shape of <code>output</code> is the same as the shape of <code>input</code>.</p>
</li>
<li>
<p>For each <em>spatial</em> dimension, the output shape is calculated as follows. Let \(f\) be the corresponding sampling factor. For down-sampling, \(x = \frac{X}{f}\), where the division must result in an integer without modulo. For up-sampling, \(X = x \cdot f\).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Multi-linear up-sampling weighs input values to form an output value. In a single dimension, the weighting happens according to one of various schemes. Let \(f\) be the sampling factor, and let us assume that the input is one dimensional. In the basic scheme, for output index \(i\), let \(j(i) = \textrm{floor} \big ( \frac{i}{f} \big )\) and let \(u(i) = \textrm{frac} \big ( \frac{i}{f} \big )\). Then the output is calculated as:</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output}[i] = (1 - u(i)) \cdot \textrm{input}[j(i)] + u(i) \cdot \textrm{input}[j(i)+1]\]
</div>
</div>
<div class="paragraph">
<p>When \(j(i)+1\) equals the input extent \(x\), the index \(j(i)+1\) is clamped to \(x - 1\), so index \(j(i)\) is used, which results in \(\textrm{output}[i] = \textrm{input}[j(i)]\) for multiple values of \(i\) on the higher end of the output range. For this reason, this method is called <code>'asymmetric'</code>.</p>
</div>
<div class="paragraph">
<p>To correct this asymmetric edge effect, various methods exist. One is to change the weighting such that the indices need not be clamped, by aligning the edges of the input and output intervals. Define \(g(x) = \frac{x \cdot f - 1}{x - 1}\), and define \(j(i) = \textrm{floor} \big ( \frac{i}{g(x)} \big )\) and \(u(i) = \textrm{frac} \big ( \frac{i}{g(x)} \big )\). This way, only the last point of the output interval maps to the last point in the input interval, hence the method is called <code>'aligned'</code>. A potential disadvantage of this method is that the weighting depends on the input extent \(x\).</p>
</div>
<div class="paragraph">
<p>An alternative solution is to make the edge effect symmetric, by shifting the index calculation, resulting in the method called <code>'symmetric'</code>. Let \(f_0 = \frac{f - 1}{2}\), and define \(j(i) = \textrm{floor} \big ( \frac{i - f_0}{f} \big )\) and \(u(i) = \textrm{frac} \big ( \frac{i - f_0}{f} \big )\). When \(j = -1\) or \(j + 1 = x\), the index is clamped to \(0\) or \(x - 1\) respectively.</p>
</div>
<div class="paragraph">
<p>Clamping along the edges corresponds to <code>border = 'replicate'</code>. Alternatively, the values outside the input range may be taken as zeros, which corresponds to <code>border = 'constant'</code>.</p>
</div>
<div class="paragraph">
<p>When <code>method = 'symmetric'</code> or <code>method = 'asymmetric'</code>, multilinear upsampling can be described as a (depth-wise separable) deconvolution with constant weights that only depend on the upsampling factor, and are uniform in the whole input range. As special cases, here are the equivalents for the linear and bilinear case when the sampling factor is uniformly 2. Note, that these definitions do not form part of the standard, they are only shown here for convenience.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment _linear_upsample2x_symmetric( input: tensor&lt;scalar&gt;, border: string )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    channels = shape_of(input)[1];
    filter = constant(shape = [channels, 1, 4],
                      value = [0.25, 0.75, 0.75, 0.25] * channels);
    output = deconv(input, filter, stride = [2], padding = [(1,1)],
                    border = border, groups = channels);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment _linear_upsample2x_asymmetric( input: tensor&lt;scalar&gt;, border: string )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    channels = shape_of(input)[1];
    filter = constant(shape = [channels, 1, 3],
                      value = [0.5, 1.0, 0.5] * channels);
    output = deconv(input, filter, stride = [2], padding = [(0,1)],
                    border = border, groups = channels);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment _bilinear_upsample2x_symmetric( input: tensor&lt;scalar&gt;, border: string )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    channels = shape_of(input)[1];
    weights = [0.0625, 0.1875, 0.1875, 0.0625,
               0.1875, 0.5625, 0.5625, 0.1875,
               0.1875, 0.5625, 0.5625, 0.1875,
               0.0625, 0.1875, 0.1875, 0.0625];
    filter = constant(shape = [channels, 1, 4, 4], value = weights * channels);
    output = deconv(input, filter, stride = [2,2], padding = [(1,1), (1,1)],
                    border = border, groups = channels);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment _bilinear_upsample2x_asymmetric( input: tensor&lt;scalar&gt;, border: string )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    channels = shape_of(input)[1];
    weights = [0.25, 0.5, 0.25,
               0.50, 1.0, 0.50,
               0.25, 0.5, 0.25];
    filter = constant(shape = [channels, 1, 3, 3], value = weights * channels);
    output = deconv(input, filter, stride = [2,2], padding = [(0,1), (0,1)],
                    border = border, groups = channels);
}</code></pre>
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="reduce-ops">4.4. Reduce Operations</h3>
<div class="paragraph">
<p>Reduce operations result in a tensor whose shape is singular along those axes where the input is reduced.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment sum_reduce(
    input: tensor&lt;scalar&gt;,          # the tensor to be reduced
    axes: integer[],                # the axes along which to reduce
    normalize: logical = false )    # whether to normalize with the reduction volume
-&gt; ( output: tensor&lt;scalar&gt; )       # the reduced tensor</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment max_reduce(
    input: tensor&lt;scalar&gt;,
    axes: integer[] )
-&gt; ( output: tensor&lt;scalar&gt; )</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment min_reduce(
    input: tensor&lt;scalar&gt;,
    axes: integer[] )
-&gt; ( output: tensor&lt;scalar&gt; )</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment argmax_reduce(
    input: tensor&lt;scalar&gt;,
    axes: integer[] )
-&gt; ( output: tensor&lt;integer&gt; )</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment argmin_reduce(
    input: tensor&lt;scalar&gt;,
    axes: integer[] )
-&gt; ( output: tensor&lt;integer&gt; )</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment all_reduce(
    input: tensor&lt;logical&gt;,
    axes: integer[] )
-&gt; ( output: tensor&lt;logical&gt; )</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment any_reduce(
    input: tensor&lt;logical&gt;,
    axes: integer[] )
-&gt; ( output: tensor&lt;logical&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Items in <code>axes</code> <strong>must</strong> be unique, non-negative and less than the rank of <code>input</code>. Inference APIs are <strong>recommended</strong> to support the case when the <code>axes</code> parameter includes all dimensions except the <em>channel</em> dimension.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>output</code> is the same as the rank of <code>input</code>. The shape of <code>output</code> is singleton along the dimensions listed in <code>axes</code> and is the same as the shape of <code>input</code> in all other dimensions.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For the sake of argument, reduction operations are described for a single axis. Since these reductions are separable, for multiple axes, it is equivalent to a sequence of single-axis reductions. Furthermore, for the sake of explanation, let the input be of shape \((m, n)\), and we perform the reduction on the second axis, and the result becomes of shape \((m, 1)\). For each index \(i \in [0,m)\), for <code>sum_reduce</code> we have</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output}[i,0] = \sum_{j=0}^{n-1} \textrm{input}[i,j]\]
</div>
</div>
<div class="paragraph">
<p>for <code>max_reduce</code> we have</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output}[i,0] = \max_{j=0}^{n-1} \textrm{input}[i,j]\]
</div>
</div>
<div class="paragraph">
<p>for <code>min_reduce</code> we have</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output}[i,0] = \min_{j=0}^{n-1} \textrm{input}[i,j]\]
</div>
</div>
<div class="paragraph">
<p>for <code>argmax_reduce</code> we have</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output}[i,0] = \arg\max_{j=0}^{n-1} \textrm{input}[i,j]\]
</div>
</div>
<div class="paragraph">
<p>for <code>argmin_reduce</code> we have</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output}[i,0] = \arg\min_{j=0}^{n-1} \textrm{input}[i,j]\]
</div>
</div>
<div class="paragraph">
<p>for <code>all_reduce</code> we have</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output}[i,0] = \bigwedge_{j=0}^{n-1} \textrm{input}[i,j]\]
</div>
</div>
<div class="paragraph">
<p>for <code>any_reduce</code> we have</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output}[i,0] = \bigvee_{j=0}^{n-1} \textrm{input}[i,j]\]
</div>
</div>
<div class="paragraph">
<p>If <code>normalize = true</code> for <code>sum_reduce</code>, the sum is normalized by the volume of the <code>input</code> along the reduction axes (the product of the extents along the dimensions listed in <code>axes</code>), resulting in averaging (taking the mean). A convenience operation is provided for mean reduction:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment mean_reduce( input: tensor&lt;scalar&gt;, axes: integer[] )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    output = sum_reduce(input, axes = axes, normalize = true);
}</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="shape-ops">4.5. Tensor Shape Operations</h3>
<div class="paragraph">
<p>The role of tensor shape manipulating operations is to prepare tensors to serve as parameters to operations that actually perform computations.</p>
</div>
<div class="sect3">
<h4 id="reshape">4.5.1. Reshaping</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment reshape&lt;?&gt;(
    input: tensor&lt;?&gt;,               # the tensor to be reshaped
    shape: integer[],               # the new shape
    axis_start: integer = 0,        # the first axis to be affected
    axis_count: integer = -1 )      # the number of axes to be affected
-&gt; ( output: tensor&lt;?&gt; )            # the reshaped tensor</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>axis_start</code> <strong>must</strong> be non-negative and less than or equal to the rank of <code>input</code>.</p>
</li>
<li>
<p><code>axis_count</code> <strong>must</strong> be non-negative or <code>-1</code>. If it is non-negative, it <strong>must</strong> be such that <code>axis_start + axis_count</code> is less than or equal to the rank of <code>input</code>. A value of <code>-1</code> indicates to take all trailing axes up to the rank of <code>input</code>.</p>
</li>
<li>
<p>Items in the <code>shape</code> array <strong>must</strong> all be non-negative or it <strong>must</strong> contain at most one <code>-1</code> item. If all items are positive, the product of items in <code>shape</code> <strong>must</strong> equal the product of the extents of <code>input</code> in the range [<code>axis_start</code>, <code>axis_start + axis_count</code>). A <code>0</code> item means that the corresponding extent is inherited from the input. In case <code>shape</code> contains a <code>-1</code> item, the product of the positive and inherited items <strong>must</strong> divide the input volume for the given axis range without remainder, and the <code>-1</code> item is replaced with the quotient of the volume of <code>input</code> and the product, therefore meeting the equality constraint.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>output</code> equals the length of <code>shape</code>. The shape of <code>output</code> equals <code>shape</code> (after substituting the <code>0</code> and <code>-1</code> items, if any), in the range [<code>axis_start</code>, <code>axis_start + axis_count</code>) and is the same as the shape of <code>input</code> outside that range.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The reshape operation considers the content of the tensor to be laid out linearly in row-major order. Reshaping does not affect the content of the tensor, instead it only provides a new indexing to the same data. For this reason, the amount of data indexed must be the same under the two (original and new) indexing schemes.</p>
</div>
<hr>
<div class="paragraph">
<p>An often used case of reshaping is when singleton dimensions are removed or inserted, named <code>squeeze</code> and <code>unsqueeze</code> respectively. Specialized fragments for these cases avoid mentioning the explicit shape of tensors:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment squeeze&lt;?&gt;( input: tensor&lt;?&gt;, axes: integer[] ) -&gt; ( output: tensor&lt;?&gt; )
fragment unsqueeze&lt;?&gt;( input: tensor&lt;?&gt;, axes: integer[] ) -&gt; ( output: tensor&lt;?&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p>The <code>axes</code> parameter denotes the list of dimension indices to be removed in case of <code>squeeze</code> and in case of <code>unsqueeze</code>, <code>axes</code> denote the dimension indices as they will appear in the output after insertion.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>For operation <code>squeeze</code>, the rank of <code>output</code> is the rank of <code>input</code> minus the length of <code>axes</code>. Items in <code>axes</code> must be non-negative and less than the rank of <code>input</code>.</p>
</div>
<div class="paragraph">
<p>For operation <code>unsqueeze</code>, the rank of the <code>output</code> is the rank of <code>input</code> plus the length of <code>axes</code>. Items in <code>axes</code> must be non-negative and less than the rank of <code>output</code>.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="transpose-op">4.5.2. Transposing</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment transpose&lt;?&gt;(
    input: tensor&lt;?&gt;,               # the tensor to be transposed
    axes: integer[] )               # the resulting order of axes
-&gt; ( output: tensor&lt;?&gt; )            # the transposed tensor</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>If the <code>axes</code> array has \(n\) items, it <strong>must</strong> be a permutation of the integers from \(0\) to \(n-1\) (inclusive). The length of <code>axes</code> <strong>must</strong> be less than or equal to the rank of <code>input</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>output</code> is the same as the rank of <code>input</code>. Let the shape of <code>input</code> along the first n dimensions be \((x_0,x_1,\dots,x_{n-1})\). Then the shape of <code>output</code> along the first \(n\) dimensions equals \((x_{axes[0]},x_{axes[1]}, \dots, x_{axes[n-1]})\). For the rest of the trailing dimensions, the shape of <code>output</code> is the same as the shape of <code>input</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Transposing a tensor reorganizes its data. It is the n dimensional generalization of matrix transpose, allowing a permutation of the dimensions, and hence changing the ordering of the data.</p>
</div>
<div class="paragraph">
<p>Let \(k \in [0..n)\) and let \(x_k\) denote the extent of the <code>input</code> in dimension \(k\). In \(n\)-dimensional indexing notation, where index \(i_k \in [0..x_k)\), the output can be described as:</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output}[i_{\textrm{axes}[0]},\cdots,i_{\textrm{axes}[n-1]}] = \textrm{input}[i_0,\cdots,i_{n-1}]\]
</div>
</div>
</div>
<div class="sect3">
<h4 id="_splitting_and_concatenation">4.5.3. Splitting and Concatenation</h4>
<div class="paragraph">
<p>Splitting and concatenation convert between a single tensor and sections of it, separated along one of the dimensions. That is, they convert between a (conceptually) single chunk of data and many chunks of data.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment split&lt;?&gt;(
    value: tensor&lt;?&gt;,               # the tensor holding the data in one piece
    axis: integer,                  # the dimension along which to split
    ratios: integer[] )             # the ratios of the chunks along the split axis
-&gt; ( values: tensor&lt;?&gt;[] )          # the list of tensors holding the data in chunks</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>Items in <code>ratios</code> <strong>must</strong> be positive.</p>
</li>
<li>
<p>The sum of <code>ratios</code> <strong>must</strong> divide the extent of <code>value</code> along dimension <code>axis</code> without remainder.</p>
</li>
<li>
<p><code>axis</code> <strong>must</strong> be non-negative and less than the rank of <code>value</code>. Inference APIs are <strong>recommended</strong> to support the case when <code>axis</code> equals 1 (<em>channel</em> dimension).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="paragraph">
<p>Let \(n\) be the number of items in <code>ratios</code>, \(r_i\) equal <code>ratios[i]</code>, and \(R = \sum_{i=0}^n r_i\). Furthermore, let \(S\) equal the shape of <code>value</code> along dimension <code>axis</code>, \(\mu = S / R\), and let \(s_i = \mu \cdot r_i\)</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The number of items in <code>values</code> is equal to the number of items in <code>ratios</code>.</p>
</li>
<li>
<p>The rank of <code>values[i]</code> is the same as the rank of <code>value</code>.</p>
</li>
<li>
<p>The shape of <code>values[i]</code> along dimension <code>axis</code> equals \(s_i\).</p>
</li>
<li>
<p>The shape of <code>values[i]</code> along all other dimensions equals the shape of <code>value</code>.</p>
</li>
</ul>
</div>
<hr>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment concat&lt;?&gt;(
    values: tensor&lt;?&gt;[],            # the list of tensors holding the data in chunks
    axis: integer )                 # the dimension along which to concatenate
-&gt; ( value: tensor&lt;?&gt; )             # the tensor holding the data in one piece</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>values[i] must be the same</code>.</p>
</li>
<li>
<p>The shapes of <code>values[i]</code> for all dimensions other than <code>axis</code> <strong>must</strong> be the same.</p>
</li>
<li>
<p><code>axis</code> <strong>must</strong> be non-negative and less than the rank of <code>values[i]</code>. Inference APIs are <strong>recommended</strong> to support the case when <code>axis</code> equals 1 (<em>channel</em> dimension).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>value</code> is the same as the rank of <code>values[i]</code>.</p>
</li>
<li>
<p>The shape of <code>value</code> along dimension <code>axis</code> is equal to the sum of shapes of <code>values[i]</code> along dimension <code>axis</code>.</p>
</li>
<li>
<p>The shape of <code>value</code> along all other dimensions is equal to the shape of <code>values[i]</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>To precisely define the relation between the split <code>values</code> and the concatenated <code>value</code> for both <code>split</code> and <code>concat</code> operations, let \(\sigma_0 = 0\) and for \(i \in [1..n]\), let \(\sigma_i = \sum_{j=0}^{i-1} s_j\). For the sake of argument, suppose that <code>value</code> and <code>values[i]</code> are of rank 3 and <code>axis = 1</code>. Then, the values in <code>values[i]</code> are equal to the values in the section of <code>value</code> that runs from \(\sigma_{i}\) to \(\sigma_{i+1}\) along dimension <code>axis</code>:</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{values}[i] = \textrm{value}[:,\sigma_i:\sigma_{i+1},:]\]
</div>
</div>
<div class="paragraph">
<p>where \(\sigma_i:\sigma_{i+1}\) means the section of the tensor from index \(\sigma_i\) (inclusive) to index \(\sigma_{i+1}\) (exclusive) along a given dimension, and the sole \(:\) means the whole section of the tensor along the given dimension.</p>
</div>
<hr>
<div class="paragraph">
<p>A special case of concatenation and splitting is when an array of tensors are concatenated/split along a new (singular) dimension. These operations are called stacking and unstacking:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment stack&lt;?&gt;( values: tensor&lt;?&gt;[], axis: integer ) -&gt; ( value: tensor&lt;?&gt; )
fragment unstack&lt;?&gt;( value: tensor&lt;?&gt;, axis: integer ) -&gt; ( values: tensor&lt;?&gt;[] )</code></pre>
</div>
</div>
<div class="paragraph">
<p>Stacking is conceptually equivalent to inserting a singleton dimension to each tensor at position given by <code>axis</code>, and then concatenating them along the axis. Unstacking is its reverse, splitting along the given axis to singleton slices, and then removing the singleton dimension.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>For operation <code>stack</code>, the rank of <code>output</code> is the rank of <code>input</code> plus 1. The <code>axis</code> must be non-negative and less than the rank of <code>output</code>.</p>
</div>
<div class="paragraph">
<p>For operation <code>unstack</code>, the rank of <code>output</code> is the rank of <code>input</code> minus 1. The <code>axis</code> must be non-negative and less than the rank of <code>input</code>.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
<div class="sect3">
<h4 id="_slicing">4.5.4. Slicing</h4>
<div class="paragraph">
<p>Slicing takes a section of a tensor along some specified dimensions.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment slice&lt;?&gt;(
    input: tensor&lt;?&gt;,               # the tensor to be sliced
    axes: integer[],                # axes along which to slice
    begin: integer[],               # beginning of slice along each axis
    end: integer[] )                # and of slice along each axis
-&gt; ( output: tensor&lt;?&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The length of arrays <code>axes</code>, <code>begin</code> and <code>end</code> <strong>must</strong> be the same.</p>
</li>
<li>
<p>Items in <code>axes</code> <strong>must</strong> be non negative and less than the rank of <code>input</code>.</p>
</li>
<li>
<p>For each item <code>axes[i]</code>, the items <code>begin[i]</code> an <code>end[i]</code> <strong>must</strong> be in the range \((-x_i,x_i]\), where \(x_i\) is the extent of <code>input</code> along the dimension <code>axes[i]</code>, and <code>end[i]</code> must be greater than <code>begin[i]</code>. If an item is negative, it is understood as if \(x_i\) was added to it, hence allowing indexing from the end. The special value <code>end[i] == 0</code> means taking the range until \(x_i\) along dimension <code>axes[i]</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>output</code> is the same as the rank of <code>input</code>. The shape of <code>output</code> along dimension \(j\) is <code>end[i] - begin[i]</code> (after accounting for non-positive values) if \(j == axes[i]\). Otherwise, the shape of <code>output</code> along dimension \(j\) is the same as that of <code>input</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>For the sake of explanation, let the input be 3 dimensional, and let <code>axes = [1]</code>. Supposing positive values in <code>begin</code> and <code>end</code>, the output is calculated as:</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output} = \textrm{input}[:,\textrm{begin}[0]:\textrm{end}[0],:]\]
</div>
</div>
</div>
<div class="sect3">
<h4 id="_padding">4.5.5. Padding</h4>
<div class="paragraph">
<p>Padding adds extra border to the tensor contents, filling it according to the specified scheme.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment pad(
    input: tensor&lt;scalar&gt;,
    padding: (integer,integer)[],
    border: string = 'constant',
    value: scalar = 0.0 )
-&gt; ( output: tensor&lt;scalar&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument Validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The number of required items in <code>padding</code> depends on the specific operation. Note, that the items in <code>padding</code> <strong>can</strong> be negative.</p>
</li>
<li>
<p><code>border</code> <strong>must</strong> be one of the modes specified in <a href="#border-modes">Border Modes</a>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>output</code> is the same as the rank of <code>input</code>.</p>
</li>
<li>
<p>The shape of <code>output</code> is calculated as follows. Let \(x_k\) denote the extent of <code>input</code> along dimension \(k\), furthermore, let <code>padding</code> be denoted by (\(p_k,q_k\)), where \(p_k\) corresponds to the padding that precedes, and \(q_k\) corresponds to the padding that succeeds the data. The extent of <code>output</code> along dimension \(k\) is then defined as: \(x_k + p_k + q_k\).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The value of <code>output</code> is the same as that of <code>input</code>, except on the borders of size (\(p_k,q_k\)), where the value depends on the value of <code>border</code> as defined in <a href="#border-modes">Border Modes</a>. If <code>border = 'constant'</code>, the value of the constant \(c\) in the formula is set by the parameter <code>value</code>.</p>
</div>
</div>
<div class="sect3">
<h4 id="_tiling">4.5.6. Tiling</h4>
<div class="paragraph">
<p>Tiling repeats the contents of the tensor along certain dimensions.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment tile&lt;?&gt;(
    input: tensor&lt;?&gt;,
    repeats: integer[] )
-&gt; ( output: tensor&lt;?&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument Validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The length of <code>repeats</code> <strong>must</strong> equal the rank of <code>input</code>. The items in <code>repeats</code> <strong>must</strong> be strictly positive.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>output</code> is the same as the rank of <code>input</code>.</p>
</li>
<li>
<p>The shape of <code>output</code> along dimension is calculated as follows. Let \(x_k\) denote the extent of <code>input</code> along dimension \(k\), and let \(r_k\) denote the \(k\)th item of <code>repeats</code>. The extent of <code>output</code> along dimension \(k\) is then defined as: \(x_k \cdot r_k\).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The value of <code>output</code> is defined as follows:</p>
</div>
<div class="stemblock">
<div class="content">
\[\textrm{output}[\dots, i, \dots] = \textrm{input}[\dots, i\ \textrm{mod}\ x_k, \dots]\]
</div>
</div>
</div>
</div>
<div class="sect2">
<h3 id="roi-ops">4.6. Region-of-Interest Operations</h3>
<div class="paragraph">
<p>RoI operations crop regions of interest out of a feature map and pool them to a fixed size. The actual pooling operation may vary just as in case of regular pooling.</p>
</div>
<div class="sect3">
<h4 id="roi-pool">4.6.1. RoI Pooling</h4>
<div class="paragraph">
<p>RoI pooling generates a fixed size output by pooling regions of variable size.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment avg_roi_pool(
    input: tensor&lt;scalar&gt;,              # the feature maps to pool from
    rois: tensor&lt;scalar&gt;,               # the regions of interest
    batch_index: tensor&lt;integer&gt;,       # batch indices for each RoI
    output_size: integer[] )            # the desired output size
-&gt; ( output: tensor&lt;scalar&gt; )</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment max_roi_pool(
    input: tensor&lt;scalar&gt;,
    rois: tensor&lt;scalar&gt;,
    batch_index: tensor&lt;integer&gt;,
    output_size: integer[] )
-&gt; ( output: tensor&lt;scalar&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="paragraph">
<p>Let \(N\) be the number of RoIs.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>rois</code> must be a tensor of shape \((N, 2 \cdot M)\), where \(M\) is the number of spatial dimensions.</p>
</li>
<li>
<p><code>batch_index</code> must be a tensor of shape \((N,1)\).</p>
</li>
<li>
<p><code>output_size</code> must have an entry for each spatial dimension. The items in <code>output_size</code> <strong>must</strong> be positive.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="paragraph">
<p>Let the shape of <code>input</code> be \((B,C,H,W)\).</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>output</code> is the same as the rank of <code>input</code>. The shape of <code>output</code> is \((N,C,p_H,p_W)\) where <code>output_size</code> = \((p_H,p_W)\).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>RoI coordinates are first rounded to the nearest integer values. Then, each RoI is cropped out from the feature map of the batch item indicated by the corresponding <code>batch_index</code>.</p>
</div>
<div class="paragraph">
<p>Let a row in the <code>rois</code> tensor be denoted by a 4-tuple \((y_0,x_0,y_1,x_1)\). Let the height of the RoI be denoted as \(r_H = y_1 - y_0\), and the width of a RoI be denoted as \(r_W = x_1 - x_0\).</p>
</div>
<div class="paragraph">
<p>Each RoI is divided into \(p_H \times p_W\) cells. Cells have height \(c_H = ceil(\frac{r_H}{p_H})\) and width \(c_W = ceil(\frac{r_W}{p_W})\), and are strided with strides \(s_H = floor(\frac{r_H}{p_H})\) and \(s_W = floor(\frac{r_W}{p_W})\); cell \((i,j)\) starts at position \((c_H + i * s_H, c_W + j * s_W)\), where \(i\) and \(j\) are zero-based indices.</p>
</div>
<div class="paragraph">
<p>For each cell, the average or max values are calculated the same way as for the <code>avg_pool</code> and <code>max_pool</code> operations.</p>
</div>
</div>
<div class="sect3">
<h4 id="roi-resize">4.6.2. RoI Align</h4>
<div class="paragraph">
<p>Enhanced versions of RoI pooling aim to eliminate the quantization effect of rounding RoI coordinates to integer values and the way cells are generated. Instead, each RoI is directly mapped to a fixed intermediate size by resampling. Afterwards, max or average pooling can be applied to arrive to a final size.</p>
</div>
<div class="paragraph">
<p>The <code>roi_resample</code> operation resamples a RoI to a fixed size using multilinear interpolation.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment roi_resample(
    input: tensor&lt;scalar&gt;,              # the feature maps to crop from
    rois: tensor&lt;scalar&gt;,               # the regions of interest
    batch_index: tensor&lt;integer&gt;,       # batch indices for each RoI
    output_size: integer[],             # the desired output size
    method: string = 'symmetric' )      # interpolation method
-&gt; ( output: tensor&lt;scalar&gt; )</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="paragraph">
<p>Let \(N\) be the number of RoIs.</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>rois</code> must be a tensor of shape \((N, 2 \cdot M)\), where \(M\) is the number of spatial dimensions.</p>
</li>
<li>
<p><code>batch_index</code> must be a tensor of shape \((N,1)\).</p>
</li>
<li>
<p><code>size</code> must have an entry for each spatial dimension. The items in <code>size</code> <strong>must</strong> be positive.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="paragraph">
<p>Let the shape of <code>input</code> be \((B,C,H,W)\).</p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>output</code> is the same as the rank of <code>input</code>. The shape of <code>output</code> is \((N,C,h,w)\) where <code>size</code> = \((h,w)\).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The interpolation methods are the same as described in <a href="#up-down-sample">Up and Down-Sampling</a>.</p>
</div>
<div class="paragraph">
<p>Using operation <code>roi_resample</code>, it is possible to describe <code>roi_align</code> operations as compounds:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment avg_roi_align(
    input: tensor&lt;scalar&gt;,
    rois: tensor&lt;scalar&gt;,
    batch_index: tensor&lt;integer&gt;,
    output_size: integer[],
    sampling_rate: integer[],
    resize_method: string = 'symmetric' )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    size = [for i in range_of(output_size) yield output_size[i] * sampling_rate[i]];
    resized = roi_resample(input, rois, batch_index, output_size = size,
                         method = resize_method);
    output = avg_pool(resized, size = sampling_rate, stride = sampling_rate);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment max_roi_align(
    input: tensor&lt;scalar&gt;,
    rois: tensor&lt;scalar&gt;,
    batch_index: tensor&lt;integer&gt;,
    output_size: integer[],
    sampling_rate: integer[],
    resize_method: string = 'symmetric' )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    size = [for i in range_of(output_size) yield output_size[i] * sampling_rate[i]];
    resized = roi_resample(input, rois, batch_index, output_size = size,
                         method = resize_method);
    output = max_pool(resized, size = sampling_rate, stride = sampling_rate);
}</code></pre>
</div>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>The literature contains a similar operation called 'RoI warp'. It can be considered a special case of RoI align, where the RoI coordinates are rounded to integer values before processing. This can be implemented using the <code>round</code> operation on the <code>rois</code> tensor before <code>roi_align</code> is called.</p>
</div>
</td>
</tr>
</table>
</div>
</div>
</div>
<div class="sect2">
<h3 id="matmul-op">4.7. Matrix Multiplication</h3>
<div class="paragraph">
<p>Matrix multiplication is a required operation for implementing fully connected linear layers in neural networks. <code>matmul</code> is a general matrix multiplication operation, which encompasses various special cases such as matrix-vector, vector-matrix and vector-vector operations.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment matmul(
    A: tensor&lt;scalar&gt;,              # the left-hand-side argument
    B: tensor&lt;scalar&gt;,              # the right-hand-side argument
    transposeA: logical = false,    # whether to transpose A
    transposeB: logical = false )   # whether to transpose B
-&gt; ( C: tensor&lt;scalar&gt; )            # the resulting tensor</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>A</code> and <code>B</code> <strong>must</strong> be at least 2 and they <strong>must</strong> be equal. Let \(r = rank(A)\). The first \(r-2\) dimensions of <code>A</code> and <code>B</code> (if any) behave as batch dimensions, resulting in a batch of matrix multiplications (broadcasting may apply if needed, as for <a href="#binary-ops">Binary Operations</a>).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Let the shape of <code>A</code> be \((b, m_A, n_A)\) and the shape of <code>B</code> be \((b, m_B, n_B)\). Then:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>If <code>transposeA = false</code> and <code>transposeB = false</code> then \(n_A\) <strong>must</strong> equal \(m_B\).</p>
</li>
<li>
<p>If <code>transposeA = false</code> and <code>transposeB = true</code> then \(n_A\) <strong>must</strong> equal \(n_B\).</p>
</li>
<li>
<p>If <code>transposeA = true</code> and <code>transposeB = false</code> then \(m_A\) <strong>must</strong> equal \(m_B\).</p>
</li>
<li>
<p>If <code>transposeA = true</code> and <code>transposeB = true</code> then \(m_A\) <strong>must</strong> equal \(n_B\).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank of <code>C</code> will be \(r\).</p>
</li>
<li>
<p>If <code>transposeA = false</code> and <code>transposeB = false</code> then the shape of <code>C</code> will be \((b, m_A, n_B)\).</p>
</li>
<li>
<p>If <code>transposeA = false</code> and <code>transposeB = true</code> then the shape of <code>C</code> will be \((b, m_A, m_B)\).</p>
</li>
<li>
<p>If <code>transposeA = true</code> and <code>transposeB = false</code> then the shape of <code>C</code> will be \((b, n_A, n_B)\).</p>
</li>
<li>
<p>If <code>transposeA = true</code> and <code>transposeB = true</code> then the shape of <code>C</code> will be \((b, n_A, m_B)\).</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Let \(A' = A^T\) if <code>transposeA = true</code> and \(A' = A\) otherwise, and define \(B'\) similarly. Omitting optional batching for simplicity, let the shape of <code>C</code> be \((m_C, n_C)\), and let \(k\) be the agreeing dimension of the matrices \(A'\) and \(B'\). Then, for all \(i \in [1..m_C\)] and \(j \in [1..n_C\)]:</p>
</div>
<div class="stemblock">
<div class="content">
\[C_{i,j} = \sum_{l=1}^k A'_{i,l} B'_{l,j}\]
</div>
</div>
</div>
<div class="sect2">
<h3 id="update-op">4.8. Variable Updates</h3>
<div class="paragraph">
<p>Variables are tensors that can be updated in each graph execution cycle, and the update is performed via a dedicated operation to avoid loops in the graph description and to have a well-defined execution order.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment update&lt;?&gt;(
    variable: tensor&lt;?&gt;,            # the variable to be updated
    value: tensor&lt;?&gt; )              # the new value
-&gt; ( result: tensor&lt;?&gt; )            # the new value returned for convenience</code></pre>
</div>
</div>
<div class="paragraph">
<p><strong><em>Argument validity</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank and shape of <code>value</code> <strong>must</strong> equal the shape of <code>variable</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong><em>Result semantics</em></strong></p>
</div>
<div class="ulist">
<ul>
<li>
<p>The rank and shape of <code>result</code> is equal to the shape of <code>variable</code>.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>After executing all operations in the current graph cycle, the content of <code>variable</code> is replaced with the content of <code>value</code>. The variable itself always refers to the original value at the beginning of the cycle. The new value can be referenced via <code>result</code>. However, <code>result</code> is not a variable tensor, so it cannot be updated again. Thus, each variable can only be updated once in a cycle.</p>
</div>
<div class="paragraph">
<p>Note, that the computational graph that describes a single computation cycle in a potentially recurrent computation is itself acyclic.</p>
</div>
</div>
<div class="sect2">
<h3 id="stdlib">4.9. Compound Operations</h3>
<div class="paragraph">
<p>This section describes a set of compound fragments that are often used to build neural networks. The argument validity and result semantics of these fragments follow from the argument validity and result semantics of the primitives they are built from.</p>
</div>
<div class="admonitionblock note">
<table>
<tr>
<td class="icon">
<i class="fa icon-note" title="Note"></i>
</td>
<td class="content">
<div class="paragraph">
<p>These definitions only provide the semantics for the operations, and do not prescribe the details of how these operations are to be implemented; they may be implemented as atomic operations or as a sequence of primitives that are mathematically equivalent to these definitions.</p>
</div>
</td>
</tr>
</table>
</div>
<div class="sect3">
<h4 id="activation-functions">4.9.1. Activation Functions</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment sigmoid( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
{
    y = 1.0 / (1.0 + exp(-x));
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment relu( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
{
    y = max(x, 0.0);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment prelu( x: tensor&lt;scalar&gt;, alpha: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
{
    y = select(x &lt; 0.0, alpha * x, x);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment leaky_relu( x: tensor&lt;scalar&gt;, alpha: scalar ) -&gt; ( y: tensor&lt;scalar&gt; )
{
    y = prelu(x, alpha);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment elu( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
{
    y = select(x &lt; 0.0, exp(x) - 1.0, x);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment tanh( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
{
    y = (exp(x) - exp(-x)) / (exp(x) + exp(-x));
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment softmax( x: tensor&lt;scalar&gt;, axes: integer[] = [1] ) -&gt; ( y: tensor&lt;scalar&gt; )
{
    m = max_reduce(x, axes = axes);
    e = exp(x - m);
    y = e / sum_reduce(e, axes = axes);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment softplus( x: tensor&lt;scalar&gt; ) -&gt; ( y: tensor&lt;scalar&gt; )
{
    y = log(exp(x) + 1.0);
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="linear-operations">4.9.2. Linear Operations</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment linear(
    input: tensor&lt;scalar&gt;,
    filter: tensor&lt;scalar&gt;,
    bias: tensor&lt;scalar&gt; = 0.0 )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    output = matmul(input, filter, transposeB = true) + bias;
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment separable_conv(
    input: tensor&lt;scalar&gt;,
    plane_filter: tensor&lt;scalar&gt;,
    point_filter: tensor&lt;scalar&gt;,
    bias: tensor&lt;scalar&gt; = 0.0,
    border: string = 'constant',
    padding: (integer,integer)[] = [],
    stride: integer[] = [],
    dilation: integer[] = [],
    groups: integer = 1 )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    filtered = conv(input, plane_filter, border = border, padding = padding,
                    stride = stride, dilation = dilation, groups = 0);
    output = conv(filtered, point_filter, bias, groups = groups);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment separable_deconv(
    input: tensor&lt;scalar&gt;,
    plane_filter: tensor&lt;scalar&gt;,
    point_filter: tensor&lt;scalar&gt;,
    bias: tensor&lt;scalar&gt; = 0.0,
    border: string = 'constant',
    padding: (integer,integer)[] = [],
    stride: integer[] = [],
    dilation: integer[] = [],
    output_shape: integer[] = [],
    groups: integer = 1 )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    filtered = deconv(input, point_filter, groups = groups);
    output = deconv(filtered, plane_filter, bias, border = border,
                    padding = padding, stride = stride, dilation = dilation,
                    output_shape = output_shape, groups = 0);
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="pooling-operations">4.9.3. Pooling Operations</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment max_pool_with_index(
    input: tensor&lt;scalar&gt;,
    size: integer[],
    border: string = 'constant',
    padding: (integer,integer)[] = [],
    stride: integer[] = [],
    dilation: integer[] = [] )
-&gt; ( output: tensor&lt;scalar&gt;, index: tensor&lt;integer&gt; )
{
    index = argmax_pool(input, size = size, border = border, padding = padding,
                        stride = stride, dilation = dilation);
    output = sample(input, index, size = size, border = border, padding = padding,
                    stride = stride, dilation = dilation);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment max_pool(
    input: tensor&lt;scalar&gt;,
    size: integer[],
    border: string = 'constant',
    padding: (integer,integer)[] = [],
    stride: integer[] = [],
    dilation: integer[] = [] )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    output, index = max_pool_with_index(input, size = size, border = border,
                                        padding = padding, stride = stride,
                                        dilation = dilation);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment avg_pool(
    input: tensor&lt;scalar&gt;,
    size: integer[],
    border: string = 'constant',
    padding: (integer,integer)[] = [],
    stride: integer[] = [],
    dilation: integer[] = [] )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    output = box(input, size = size, border = border, padding = padding,
                 stride = stride, dilation = dilation, normalize = true);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment rms_pool(
    input: tensor&lt;scalar&gt;,
    size: integer[],
    border: string = 'constant',
    padding: (integer,integer)[] = [],
    stride: integer[] = [],
    dilation: integer[] = [] )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    output = sqrt(avg_pool(sqr(input), size = size, border = border,
                  padding = padding, stride = stride, dilation = dilation));
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="normalization-operations">4.9.4. Normalization Operations</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment local_response_normalization(
    input: tensor&lt;scalar&gt;,
    size: integer[],
    alpha: scalar = 1.0,
    beta: scalar = 0.5,
    bias: scalar = 1.0 )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    sigma = bias + alpha * box(sqr(input), size = size, normalize = true);
    output = input / (sigma ^ beta);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment local_mean_normalization(
    input: tensor&lt;scalar&gt;,
    size: integer[] )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    mean = box(input, size = size, normalize = true);
    output = input - mean;
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment local_variance_normalization(
    input: tensor&lt;scalar&gt;,
    size: integer[],
    bias: scalar = 0.0,
    epsilon: scalar = 0.0 )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    sigma = box(sqr(input), size = size, normalize = true);
    output = input / max(sqrt(sigma) + bias, epsilon);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment local_contrast_normalization(
    input: tensor&lt;scalar&gt;,
    size: integer[],
    bias: scalar = 0.0,
    epsilon: scalar = 0.0 )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    centered = local_mean_normalization(input, size = size);
    output = local_variance_normalization(centered, size = size, bias = bias,
                                          epsilon = epsilon);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment l1_normalization(
    input: tensor&lt;scalar&gt;,
    axes: integer[],
    bias: scalar = 0.0,
    epsilon: scalar = 0.0 )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    sigma = sum_reduce(abs(input), axes = axes);
    output = input / max(sigma + bias, epsilon);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment l2_normalization(
    input: tensor&lt;scalar&gt;,
    axes: integer[],
    bias: scalar = 0.0,
    epsilon: scalar = 0.0 )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    sigma = sum_reduce(sqr(input), axes = axes);
    output = input / max(sqrt(sigma) + bias, epsilon);
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment batch_normalization(
    input: tensor&lt;scalar&gt;,
    mean: tensor&lt;scalar&gt;,
    variance: tensor&lt;scalar&gt;,
    offset: tensor&lt;scalar&gt;,
    scale: tensor&lt;scalar&gt;,
    epsilon: scalar )
-&gt; ( output: tensor&lt;scalar&gt; )
{
    output = offset + scale * (input - mean) / sqrt(variance + epsilon);
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="quantization-operations">4.9.5. Quantization Operations</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment linear_quantize(
    x: tensor&lt;scalar&gt;,
    min: tensor&lt;scalar&gt;,
    max: tensor&lt;scalar&gt;,
    bits: integer )
-&gt; ( y: tensor&lt;scalar&gt; )
{
    r = scalar(2 ^ bits - 1);
    z = clamp(x, min, max);
    q = round((z - min) / (max - min) * r);
    y = q / r * (max - min) + min;
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment logarithmic_quantize(
    x: tensor&lt;scalar&gt;,
    max: tensor&lt;scalar&gt;,
    bits: integer )
-&gt; ( y: tensor&lt;scalar&gt; )
{
    m = ceil(log2(max));
    r = scalar(2 ^ bits - 1);
    q = round(clamp(log2(abs(x)), m - r, m));
    y = sign(x) * 2.0 ^ q;
}</code></pre>
</div>
</div>
</div>
<div class="sect3">
<h4 id="misc-operations">4.9.6. Miscellaneous Operations</h4>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment copy_n&lt;?&gt;( x: tensor&lt;?&gt;, times: integer ) -&gt; ( y: tensor&lt;?&gt;[] )
{
    y = [x] * times;
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment add_n( x: tensor&lt;scalar&gt;[] ) -&gt; ( y: tensor&lt;scalar&gt; )
{
    y = x[0] + add_n(x[1:]) if length_of(x) &gt; 0 else 0.0;
}</code></pre>
</div>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>fragment moments( input: tensor&lt;scalar&gt;, axes: integer[] )
-&gt; ( mean: tensor&lt;scalar&gt;, variance: tensor&lt;scalar&gt; )
{
    mean = mean_reduce(input, axes = axes);
    variance = mean_reduce(sqr(input - mean), axes = axes);
}</code></pre>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="storing-data">5. Storing Network Data</h2>
<div class="sectionbody">
<div class="paragraph">
<p>The textual description introduced in the previous chapters is capable of capturing the neural network structure. However, to make descriptions of trained neural networks possible, the data parameters of the network (weight tensors) must be stored somehow, furthermore, they must be linked to the graph structure. Since variables, which form the data parameters of the network, are labelled uniquely, these labels can be used to link network data with the structure. The network weights are stored in separate data files that are named and stored into folders according to the labels of variables.</p>
</div>
<div class="paragraph">
<p>Further files may be used to store additional information attached to the basic structure, such as quantization information for tensors. For this reason, the files may be wrapped in a container to form a single data stream. This container may also provide compression and encryption mechanisms. The container format is not part of the NNEF specification, however, it is recommended to use the IEEE 1003.1-2008 tar archive with optional compression.</p>
</div>
<div class="paragraph">
<p>In what follows, the contents of the container and the tensor data files are described.</p>
</div>
<div class="sect2">
<h3 id="container-structure">5.1. Container Organization</h3>
<div class="paragraph">
<p>The contents of the container may consist of several files:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A textual file describing the structure of the network, according to <a href="#syntax">Syntax</a>. The file <strong>must</strong> be named 'graph.nnef'.</p>
</li>
<li>
<p>A binary data file (structured according to <a href="#tensor-data-format">Tensor File Format</a>) for each variable tensor in the structure description, placed into sub-folders according to the labelling of the variables. The files <strong>must</strong> have '.dat' extension. For example, a variable with label 'conv1/filter' is placed into the folder 'conv1' under the name 'filter.dat'. Different versions of the same data (for example with different quantization) may be present starting with the same name, having additional (arbitrary) extensions after '.dat'.</p>
</li>
<li>
<p>An optional quantization file (structured according to <a href="#quantization-format">Quantization File Format</a>) containing quantization algorithm details for <a href="#exported-ids">exported</a> tensors. The file <strong>must</strong> be named 'graph.quant'.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>Note, that vendors may use the container to store vendor specific information in sub-folders and files, such as optimized network data in custom formats. This information is simply omitted by other tools.</p>
</div>
</div>
<div class="sect2">
<h3 id="tensor-data-format">5.2. Tensor File Format</h3>
<div class="paragraph">
<p>Each tensor data file consists of two parts, the header and the data. All data is laid out in little-endian byte order. The header size is fixed to 128 bytes, and consists of the following (in this order):</p>
</div>
<div class="ulist">
<ul>
<li>
<p>A two-byte magic number. The first byte is the ASCII character 'N' (0x4E) and the second byte is 0xEF.</p>
</li>
<li>
<p>A two-byte version info (one byte for major, one for minor version).</p>
</li>
<li>
<p>A 4-byte unsigned integer indicating the length of the actual tensor data in bytes (starting at offset 128).</p>
</li>
<li>
<p>A 4-byte unsigned integer indicating the rank of the tensor. Maximal value is 8.</p>
</li>
<li>
<p>8 \(\times\) 4-byte unsigned integers denoting the extents of the tensor. Only relevant dimensions up to rank must be filed, the rest must be filled with zeros.</p>
</li>
<li>
<p>A 4-byte unsigned integer indicating the number of bits per item, at most 64.</p>
</li>
<li>
<p>A 4-byte code indicating the quantization (compression) algorithm used to store the data. The code is split to two 2-byte parts. The first part is a vendor code, the second part is an algorithm code. See reserved values below.</p>
</li>
<li>
<p>32 bytes for parameters of the quantization algorithm. The interpretation of these 32 bytes depends on the algorithm code.</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>The remaining bytes of the 128 byte header are reserved for future use and should be filled with zeros.</p>
</div>
<div class="paragraph">
<p>The quantization algorithm vendor code 0x0 identifies the Khronos Group. For the algorithm code part, Khronos specifies the following:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>0x00 - uncompressed float values in IEEE format. Valid bits per item is 16, 32, 64.</p>
</li>
<li>
<p>0x01 - uncompressed integer values.</p>
</li>
<li>
<p>0x10 - quantized values with linear quantization as defined by the <code>linear_quantize</code> operation</p>
</li>
<li>
<p>0x11 - quantized values with logarithmic quantization as defined by the <code>logarithmic_quantize</code> operation</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In case of uncompressed integer values (code 0x01), the first 4-byte value of the parameters signal whether the data-type is signed (zero indicates unsigned, non-zero indicates signed).</p>
</div>
<div class="paragraph">
<p>The codes 0x10 and 0x11 have corresponding parameters as follows:</p>
</div>
<div class="ulist">
<ul>
<li>
<p><code>min</code>: 32-bit IEEE floating point value indicating the value to which an item containing all-zero bits is mapped</p>
</li>
<li>
<p><code>max</code>: 32-bit IEEE floating point value indicating the value to which an item containing all-one bits is mapped</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In case of the logarithmic quantization algorithm (0x11), the <code>min</code> parameter must be set to 0 (unsigned) or <code>-max</code> (signed).</p>
</div>
<div class="paragraph">
<p>Following the header, the tensor data is laid out linearly in a continuous row-major order, as a bit-stream packed continuously into a sequence of bytes. If the last group of bits does not fall into a byte boundary, the last byte is padded with zero bits.</p>
</div>
<div class="paragraph">
<p>The extents of the tensors must be equal to the ones calculated from the network description in 'graph.nnef'.</p>
</div>
<div class="paragraph">
<p>In the following the encoding and decoding formulas for quantized representations are detailed. Let \(b\) denote the number of bits per item, and let \(r = 2^b - 1\). The quantized representation will contain integer values in \([0..r]\). Let \(x\) denote the input value, furthermore let \([ x ]_a^b\) denote a value clamped to be within the range \([a,b]\).</p>
</div>
<div class="paragraph">
<p>Encoding with linear quantization is as follows:</p>
</div>
<div class="stemblock">
<div class="content">
\[q = round \bigl( \bigl[ \frac{x - \textrm{min}}{\textrm{max} - \textrm{min}} \bigr]_0^1 \cdot r \bigr)\]
</div>
</div>
<div class="paragraph">
<p>Decoding with linear quantization is as follows:</p>
</div>
<div class="stemblock">
<div class="content">
\[\overline{x} = \frac{q}{r} \cdot (\textrm{max} - \textrm{min}) + \textrm{min}\]
</div>
</div>
<div class="paragraph">
<p>In case of logarithmic quantization, let \(m = ceil(\log_2 max)\), and suppose first that \(x \geq 0\). Encoding with logarithmic quantization is as follows:</p>
</div>
<div class="stemblock">
<div class="content">
\[q = round \bigl( \bigl[\log_2 x \bigr]_{m-r}^{m} \bigr) - (m - r)\]
</div>
</div>
<div class="paragraph">
<p>Note that \(\log_2 0 = -\infty\) in the above equation.</p>
</div>
<div class="paragraph">
<p>Decoding with logarithmic quantization is as follows:</p>
</div>
<div class="stemblock">
<div class="content">
\[\overline{x} = 2^{q + (m - r)} \cdot\]
</div>
</div>
<div class="paragraph">
<p>If \(x\) is signed, then a sign bit must be allocated from the \(b\) bits available for the encoding, and the actual bit width to perform the above computation must be decreased by one. The computation must be performed on \(|x|\) and the sign information must be carried into the code \(q\).</p>
</div>
</div>
<div class="sect2">
<h3 id="quantization-format">5.3. Quantization File Format</h3>
<div class="paragraph">
<p>The quantization file has a simple line based textual format. Each line contains a tensor identifier (string in quotes, <code>''</code> or <code>""</code>) and a quantization algorithm terminated by a semicolon. Identifiers <strong>must</strong> refer to <a href="#exported-ids">exported</a> activation tensor names (not variable labels) in the structure description:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>...
"filter1": linear_quantize(min = -2.0, max = 2.5, bits = 8);
"bias1": linear_quantize(min = -3.0, max = 0.75, bits = 8);
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>The quantization algorithm string <strong>must not</strong> contain an argument for the tensor to be quantized, but all other parameters must be provided as named arguments with compile-time expressions (no tensor ids). The tensor to be quantized is the one identified by the name before the separating colon (see <a href="#quantization">Quantization</a> for more info on interpretation of quantization info).</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="document-validity">6. Document Validity</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Validity and integrity of a document in the NNEF format can be checked independently of an implementation that executes it. Conceptually, a document processor should be able to take in a document, and if it is well-formatted, the processor should be able to output another document, which represents the same graph in a flattened format built only as a sequence of primitives with concrete arguments, without any compound fragment definitions and formal parameter expressions.</p>
</div>
<div class="paragraph">
<p>The process of validity checking consists of a number of steps described in detail in the previous chapters. The goal of this section is to summarize this process. As the result of this process, a document is either accepted as valid, or rejected as illegal. The cause of rejection may be various, depending on the stage in which an integrity problem is encountered during processing. The processing steps and the rejection causes are the following:</p>
</div>
<div class="olist arabic">
<ol class="arabic">
<li>
<p>Syntactic parsing may result in syntax errors when the document does not conform to the grammar defined in <a href="#syntax">Syntax</a>.</p>
</li>
<li>
<p>Semantic analysis may result in semantic errors when the syntactically valid document violates constraints defined in <a href="#semantics">Semantics</a>. The violations may be of the following type:</p>
<div class="olist loweralpha">
<ol class="loweralpha">
<li>
<p>Invalid fragment definition, as defined in <a href="#fragment-definition-invocation">Graph and Fragment Definition</a>.</p>
</li>
<li>
<p>Invalid argument structure in invocation, as defined in <a href="#fragment-definition-invocation">Graph and Fragment Definition</a>.</p>
</li>
<li>
<p>Invalid type agreement, as defined in <a href="#type-system">Type System and Type Checking</a>.</p>
</li>
<li>
<p>Invalid formal identifier usage, as defined in <a href="#identifier-usage">Identifier Usage in the Graph and Fragment Body</a>.</p>
</li>
</ol>
</div>
</li>
<li>
<p>The invocation of operations and the expansion of compound operations to primitive operations may result in invalid arguments in invocations. Validity of arguments is defined individually for each primitive in <a href="#primitives">Operations</a>.</p>
</li>
<li>
<p>Loading serialized tensor data, as defined in <a href="#storing-data">Storing Network Data</a>, may result in conflicting tensor shapes (the stored shape conflicts with the shape in the structure definition).</p>
</li>
</ol>
</div>
<div class="paragraph">
<p>When any of the above errors occur, the processor of the document should report the error and may stop processing. Tensor shape related argument validity checking (3.) and shape propagation may only be performed if no custom operations are defined in the document (for which shape propagation is not possible to define via the syntax). All other validity checks can still be performed in the presence of custom operations.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="quantization">7. Quantization</h2>
<div class="sectionbody">
<div class="paragraph">
<p>Executing neural networks in quantized representations is an important aspect of high performance inference. To achieve best performance, neural networks need to be trained in special ways that takes the expected precision of the target inference hardware into account, therefore when transferring trained networks to inference engines, the quantization method used during training needs to be conveyed by NNEF. However, since NNEF aims to be a platform independent description of network structure and data, the description of quantization also needs to remain independent of the underlying representations of specific training frameworks and inference APIs.</p>
</div>
<div class="paragraph">
<p>To solve this seemingly contradictory issue, NNEF uses the concept of 'pseudo'-quantization. Note that NNEF deals with real valued tensors which are conceptually of infinite precision to avoid reference to machine representations. To keep this status, quantization operations convert tensor data into quantized values while conceptually remaining infinite precision real values. That is, arbitrary values are rounded and clamped to certain finite number of values depending on the exact algorithm. The quantization algorithms are described as compound operations (see <a href="#quantization-operations">Quantization Operations</a>), therefore the set of quantization techniques can easily be extended.</p>
</div>
<div class="paragraph">
<p>Since different inference engines may have different hardware capabilities, quantization information is only meant as a hint for executing networks. It conveys the information that the network was trained with a specific precision and representation in mind, but the actual inference engine that executes the network may use a different representation or quantization level (which may or may not result in performance degradation).</p>
</div>
<div class="sect2">
<h3 id="_incorporating_quantization_info">7.1. Incorporating Quantization Info</h3>
<div class="paragraph">
<p>Quantization can be applied to both network <em>activations</em> and stored tensor <em>parameters</em>. Furthermore, the tensor parameter data itself may be stored in a quantized or non-quantized format. Even if the parameters are not stored in a quantized format, they may be quantized during inference.</p>
</div>
<div class="paragraph">
<p>There are two ways to use the quantization operations in NNEF. First, since they are just like any other operation (input-output mapping, in this case taking a tensor and outputting its quantized version), they can simply be incorporated into a computational graph description.</p>
</div>
<div class="paragraph">
<p>The following example shows an excerpt from a possible quantized network where only activations are quantized:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>input = tensor(shape = [1,3,224,224]);
filter1 = variable(shape = [32,3,5,5], label = 'conv1/filter');
bias1 = variable(shape = [1,32], label = 'conv1/bias');
conv1 = relu(conv(input, filter1) + bias1);
quant1 = linear_quantize(conv1, min = 0.0, max = 1.0, bits = 8);
filter2 = variable(shape = [64,32,3,3], label = 'conv2/filter');
bias2 = variable(shape = [1,64], label = 'conv2/bias');
conv2 = relu(conv(quant1, filter2) + bias2);
quant2 = linear_quantize(conv2, min = 0.0, max = 1.0, bits = 8);
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>If we wanted to quantize parameters such as filters and biases as well by adding further quantization operations for them before the convolution operations, the description would soon become very cluttered. Furthermore, quantization operations are only hints to the inference engine and do not necessarily result in actual computation. To avoid cluttering the network structure, as a second option, quantization info may be placed into a separate file that relies on exported tensor <a href="#exported-ids">identifiers</a> to map tensors to quantization operations. We may keep the original core structure:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>input = tensor(shape = [1,3,224,224]);
filter1 = variable(shape = [32,3,5,5], label = 'conv1/filter');
bias1 = variable(shape = [1,32], label = 'conv1/bias');
conv1 = relu(conv(input, filter1) + bias1);
filter2 = variable(shape = [64,32,3,3], label = 'conv2/filter');
bias2 = variable(shape = [1,64], label = 'conv2/bias');
conv2 = relu(conv(quant1, filter2) + bias2);
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>and indicate quantization separately, which may be applied to external input parameters, constants, variables and activations as well:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>"input": linear_quantize(min = 0.0, max = 1.0, bits = 8);
"filter1": linear_quantize(min = -1.0, max = 1.0, bits = 8);
"bias1": linear_quantize(min = -1.0, max = 1.0, bits = 8);
"conv1": linear_quantize(min = 0.0, max = 1.0, bits = 8);
"filter2": linear_quantize(min = -1.0, max = 1.0, bits = 8);
"bias2": linear_quantize(min = -1.0, max = 1.0, bits = 8);
"conv2": linear_quantize(min = 0.0, max = 1.0, bits = 8);
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>For the sake of explanation, linear quantization is used everywhere in this example, with ranges [0,1] for activations and [-1,1] for variables, but different parameters or algorithms could be used for each tensor. The two descriptions together are interpreted as if the appropriate quantization was applied to each tensor after its value is computed, which is equivalent to the following description:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>input = linear_quantize(tensor(shape = [1,3,224,224]),
                        min = 0.0, max = 1.0, bits = 8);
filter1 = linear_quantize(variable(shape = [32,3,5,5], label = 'conv1/filter'),
                        min = -1.0, max = 1.0, bits = 8);
bias1 = linear_quantize(variable(shape = [1,32], label = 'conv1/bias'),
                        min = -1.0, max = 1.0, bits = 8);
conv1 = linear_quantize(relu(conv(input, filter1) + bias1),
                        min = 0.0, max = 1.0, bits = 8);
filter2 = linear_quantize(variable(shape = [64,32,3,3], label = 'conv2/filter'),
                        min = -1.0, max = 1.0, bits = 8);
bias2 = linear_quantize(variable(shape = [1,64], label = 'conv2/bias'),
                        min = -1.0, max = 1.0, bits = 8);
conv2 = linear_quantize(relu(conv(quant1, filter2) + bias2),
                        min = 0.0, max = 1.0, bits = 8);
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>In case when the parameter data are stored in a quantized format , only the quantization indices are stored, along with the quantization algorithm as a text string (see <a href="#tensor-data-format">Tensor File Format</a>). For example, when an algorithm quantizes to 8 bits, the minimum value is stored as index 0, the next value as index 1, and so on, until the maximum value which has index 255. In this case, a string such as <code>linear_quantize(min = -1.0, max = 1.0, bits = 8)</code> stored in the tensor file 'conv1/filter.dat' provides the interpretation for the 8-bit data. Hence the tensor <code>filter1</code> conceptually contains the real valued data as if it was pseudo-quantized (of course, and implementation need not explicitly perform this conversion). If the quantization file also contains a line for the key 'filter1', the quantization stored in the data file may be overridden for the purpose of actual computation. So, the quantization algorithm in the tensor file is for interpreting the data in that file, while the quantization algorithm in the quantization file is for the actual computation during execution. Quantization info in the quantization file may also be present for a variable if it is not stored in a quantized format in the data file.</p>
</div>
</div>
<div class="sect2">
<h3 id="_dynamic_quantization">7.2. Dynamic Quantization</h3>
<div class="paragraph">
<p>The above examples showcase static quantization, where the quantization ranges are determined <em>a priori</em>, typically from the statistics of a test data set, and are fixed during inference. However, quantization may be dynamic as well, when the quantization range is calculated during inference. Note, that the definition of quantization operations let the quantization range be defined with tensors.</p>
</div>
<div class="paragraph">
<p>Dynamic quantization is only meaningful for activation tensors. In this case, the quantization operations must be part of the structural description, since the calculation of the range arguments must also be part of the computation. In the following example, the maximum of the range is calculated as the actual maximum of the data. Also, it is possible to calculate separate max values for each channel to perform channel-wise quantization:</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>...
conv1 = relu(conv(input, filter1) + bias1);
max1 = max_reduce(conv1, axes = [0,2,3]);
quant1 = linear_quantize(conv1, min = 0.0, max = max1, bits = 8);
...</code></pre>
</div>
</div>
<div class="paragraph">
<p>Note, that even if the quantization operations are part of the description, they may be ignored by an implementation that does not have the capabilities to perform them (of course, at the cost of inaccuracies). In this case, the operations that calculate the range (<code>max_reduce</code> here) may also be ignored.</p>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="grammar-definitions">Appendix A: Grammar Definitions</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="flat-grammar">A.1. Flat Grammar</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;document&gt; ::= &lt;version&gt; &lt;extension&gt;* &lt;graph-definition&gt;
&lt;version&gt; ::= "<strong>version</strong>" &lt;numeric-literal&gt; "<strong>;</strong>"
&lt;extension&gt; ::= "<strong>extension</strong>" &lt;identifier&gt;+ "<strong>;</strong>"

&lt;graph-definition&gt; ::= &lt;graph-declaration&gt; &lt;body&gt;
&lt;graph-declaration&gt; ::= "<strong>graph</strong>" &lt;identifier&gt; "<strong>(</strong>" &lt;identifier-list&gt; "<strong>)</strong>"
                        "<strong>-&gt;</strong>" "<strong>(</strong>" &lt;identifier-list&gt; "<strong>)</strong>"
&lt;identifier-list&gt; ::= &lt;identifier&gt; ("<strong>,</strong>" &lt;identifier&gt;)*

&lt;body&gt; ::= "<strong>{</strong>" &lt;assignment&gt;+ "<strong>}</strong>"
&lt;assignment&gt; ::= &lt;lvalue-expr&gt; "<strong>=</strong>" &lt;invocation&gt; "<strong>;</strong>"

&lt;invocation&gt; ::= &lt;identifier&gt; ["<strong>&lt;</strong>" &lt;type-name&gt; "<strong>&gt;</strong>"] "<strong>(</strong>" &lt;argument-list&gt; "<strong>)</strong>"
&lt;argument-list&gt; ::= &lt;argument&gt; ("<strong>,</strong>" &lt;argument&gt;)*
&lt;argument&gt; ::= &lt;rvalue-expr&gt; | &lt;identifier&gt; "<strong>=</strong>" &lt;rvalue-expr&gt;

&lt;lvalue-expr&gt; ::= &lt;identifier&gt; | &lt;array-lvalue-expr&gt; | &lt;tuple-lvalue-expr&gt;
&lt;array-lvalue-expr&gt; ::= "<strong>[</strong>" [&lt;lvalue-expr&gt; ("<strong>,</strong>" &lt;lvalue-expr&gt;)* ] "<strong>]</strong>"
&lt;tuple-lvalue-expr&gt; ::= "<strong>(</strong>" &lt;lvalue-expr&gt; ("<strong>,</strong>" &lt;lvalue-expr&gt;)+ "<strong>)</strong>" |
                            &lt;lvalue-expr&gt; ("<strong>,</strong>" &lt;lvalue-expr&gt;)+

&lt;rvalue-expr&gt; ::= &lt;identifier&gt; | &lt;literal&gt; | &lt;array-rvalue-expr&gt; | &lt;tuple-rvalue-expr&gt;
&lt;array-rvalue-expr&gt; ::= "<strong>[</strong>" [&lt;rvalue-expr&gt; ("<strong>,</strong>" &lt;rvalue-expr&gt;)* ] "<strong>]</strong>"
&lt;tuple-rvalue-expr&gt; ::= "<strong>(</strong>" &lt;rvalue-expr&gt; ("<strong>,</strong>" &lt;rvalue-expr&gt;)+ "<strong>)</strong>"

&lt;literal&gt; ::= &lt;numeric-literal&gt; | &lt;string-literal&gt; | &lt;logical-literal&gt;</code></pre>
</div>
</div>
</div>
<div class="sect2">
<h3 id="compositional-grammar">A.2. Compositional Grammar</h3>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>&lt;document&gt; ::= &lt;version&gt; &lt;extension&gt;* &lt;fragment-definition&gt;* &lt;graph-definition&gt;
&lt;version&gt; ::= "<strong>version</strong>" &lt;numeric-literal&gt; "<strong>;</strong>"
&lt;extension&gt; ::= "<strong>extension</strong>" &lt;identifier&gt;+ "<strong>;</strong>"

&lt;fragment-definition&gt; ::= &lt;fragment-declaration&gt; (&lt;body&gt; | "<strong>;</strong>")
&lt;fragment-declaration&gt; ::= "<strong>fragment</strong>" &lt;identifier&gt; [&lt;generic-declaration&gt;]
                           "<strong>(</strong>" &lt;parameter-list&gt; "<strong>)</strong>" "<strong>-&gt;</strong>" "<strong>(</strong>" &lt;result-list&gt; "<strong>)</strong>"
&lt;generic-declaration&gt; ::= "<strong>&lt;</strong>" "<strong>?</strong>" ["<strong>=</strong>" &lt;type-name&gt;] "<strong>&gt;</strong>"
&lt;parameter-list&gt; ::= &lt;parameter&gt; ("<strong>,</strong>" &lt;parameter&gt;)*
&lt;parameter&gt; ::= &lt;identifier&gt; "<strong>:</strong>" &lt;type-spec&gt; ["<strong>=</strong>" &lt;literal-expr&gt;]
&lt;result-list&gt; ::= &lt;result&gt; ("<strong>,</strong>" &lt;result&gt;)*
&lt;result&gt; ::= &lt;identifier&gt; "<strong>:</strong>" &lt;type-spec&gt;

&lt;literal-expr&gt; ::= &lt;literal&gt; | &lt;array-literal-expr&gt; | &lt;tuple-literal-expr&gt;
&lt;literal&gt; ::= &lt;numeric-literal&gt; | &lt;string-literal&gt; | &lt;logical-literal&gt;
&lt;array-literal-expr&gt; ::= "<strong>[</strong>" [&lt;literal-expr&gt; ("<strong>,</strong>" &lt;literal-expr&gt;)* ] "<strong>]</strong>"
&lt;tuple-literal-expr&gt; ::= "<strong>(</strong>" &lt;literal-expr&gt; ("<strong>,</strong>" &lt;literal-expr&gt;)+ "<strong>)</strong>"

&lt;type-spec&gt; ::= &lt;type-name&gt; | &lt;tensor-type-spec&gt; |
                &lt;array-type-spec&gt; | &lt;tuple-type-spec&gt;
&lt;type-name&gt; ::= "<strong>integer</strong>" | "<strong>scalar</strong>" | "<strong>logical</strong>" | "<strong>string</strong>" | "<strong>?</strong>"
&lt;tensor-type-spec&gt; ::= "<strong>tensor</strong>" "<strong>&lt;</strong>" [&lt;type-name&gt;] "<strong>&gt;</strong>"
&lt;array-type-spec&gt; ::= &lt;type-spec&gt; "<strong>[]</strong>"
&lt;tuple-type-spec&gt; ::= "<strong>(</strong>" &lt;type-spec&gt; ("<strong>,</strong>" &lt;type-spec&gt;)+ "<strong>)</strong>"

&lt;graph-definition&gt; ::= &lt;graph-declaration&gt; &lt;body&gt;
&lt;graph-declaration&gt; ::= "<strong>graph</strong>" &lt;identifier&gt; "<strong>(</strong>" &lt;identifier-list&gt; "<strong>)</strong>"
                        "<strong>-&gt;</strong>" "<strong>(</strong>" &lt;identifier-list&gt; "<strong>)</strong>"
&lt;identifier-list&gt; ::= &lt;identifier&gt; ("<strong>,</strong>" &lt;identifier&gt;)*

&lt;body&gt; ::= "<strong>{</strong>" &lt;assignment&gt;+ "<strong>}</strong>"
&lt;assignment&gt; ::= &lt;lvalue-expr&gt; "<strong>=</strong>" &lt;rvalue-expr&gt; "<strong>;</strong>"

&lt;lvalue-expr&gt; ::= &lt;identifier&gt; | &lt;array-lvalue-expr&gt; | &lt;tuple-lvalue-expr&gt;
&lt;array-lvalue-expr&gt; ::= "<strong>[</strong>" [&lt;lvalue-expr&gt; ("<strong>,</strong>" &lt;lvalue-expr&gt;)* ] "<strong>]</strong>"
&lt;tuple-lvalue-expr&gt; ::= "<strong>(</strong>" &lt;lvalue-expr&gt; ("<strong>,</strong>" &lt;lvalue-expr&gt;)+ "<strong>)</strong>" |
                            &lt;lvalue-expr&gt; ("<strong>,</strong>" &lt;lvalue-expr&gt;)+

&lt;invocation&gt; ::= &lt;identifier&gt; ["<strong>&lt;</strong>" &lt;type-name&gt; "<strong>&gt;</strong>"] "<strong>(</strong>" &lt;argument-list&gt; "<strong>)</strong>"
&lt;argument-list&gt; ::= &lt;argument&gt; ("<strong>,</strong>" &lt;argument&gt;)*
&lt;argument&gt; ::= &lt;rvalue-expr&gt; | &lt;identifier&gt; "<strong>=</strong>" &lt;rvalue-expr&gt;

&lt;rvalue-expr&gt; ::= &lt;identifier&gt;
                | &lt;literal&gt;
                | &lt;binary-expr&gt;
                | &lt;unary-expr&gt;
                | &lt;paren-expr&gt;
                | &lt;array-rvalue-expr&gt;
                | &lt;tuple-rvalue-expr&gt;
                | &lt;subscript-expr&gt;
                | &lt;if-else-expr&gt;
                | &lt;comprehension-expr&gt;
                | &lt;builtin-expr&gt;
                | &lt;invocation&gt;

&lt;array-rvalue-expr&gt; ::= "<strong>[</strong>" [&lt;rvalue-expr&gt; ("<strong>,</strong>" &lt;rvalue-expr&gt;)* ] "<strong>]</strong>"
&lt;tuple-rvalue-expr&gt; ::= "<strong>(</strong>" &lt;rvalue-expr&gt; ("<strong>,</strong>" &lt;rvalue-expr&gt;)+ "<strong>)</strong>"

&lt;unary-expr&gt; ::= &lt;unary-operator&gt; &lt;rvalue-expr&gt;
&lt;binary-expr&gt; ::= &lt;rvalue-expr&gt; &lt;binary-operator&gt; &lt;rvalue-expr&gt;
&lt;paren-expr&gt; ::= "<strong>(</strong>" &lt;rvalue-expr&gt; "<strong>)</strong>"

&lt;binary-operator&gt; ::= &lt;comparison-operator&gt;
                    | &lt;binary-arithmetic-operator&gt;
                    | &lt;binary-logical-operator&gt;
&lt;comparison-operator&gt; ::= "<strong>&lt;</strong>" | "<strong>&lt;=</strong>" | "<strong>&gt;</strong>" | "<strong>&gt;=</strong>" | "<strong>==</strong>" | "<strong>!=</strong>" | "<strong>in</strong>"
&lt;binary-arithmetic-operator&gt; ::= "<strong>+</strong>" | "<strong>-</strong>" | "<strong>*</strong>" | "<strong>/</strong>" | "<strong>^</strong>"
&lt;binary-logical-operator&gt; ::= "<strong>&amp;&amp;</strong>" | "<strong>||</strong>"

&lt;unary-operator&gt; ::= &lt;unary-arithmetic-operator&gt;
                   | &lt;unary-logical-operator&gt;
&lt;unary-arithmetic-operator&gt; ::= "<strong>+</strong>" | "<strong>-</strong>"
&lt;unary-logical-operator&gt; ::= "<strong>!</strong>"

&lt;if-else-expr&gt; ::= &lt;rvalue-expr&gt; "<strong>if</strong>" &lt;rvalue-expr&gt; "<strong>else</strong>" &lt;rvalue-expr&gt;

&lt;loop-iter&gt; ::= &lt;identifier&gt; "<strong>in</strong>" &lt;rvalue-expr&gt;
&lt;loop-iter-list&gt; ::= &lt;loop-iter&gt; ("<strong>,</strong>" &lt;loop-iter&gt;)*
&lt;comprehension-expr&gt; ::= "<strong>[</strong>" "<strong>for</strong>" &lt;loop-iter-list&gt; ["<strong>if</strong>" &lt;rvalue-expr&gt;]
                         "<strong>yield</strong>" &lt;rvalue-expr&gt; "<strong>]</strong>"

&lt;subscript-expr&gt; ::= &lt;rvalue-expr&gt; "<strong>[</strong>" (&lt;rvalue-expr&gt; |
                     [&lt;rvalue-expr&gt;] "<strong>:</strong>" [&lt;rvalue-expr&gt;]) "<strong>]</strong>"

&lt;builtin-expr&gt; ::= &lt;builtin-name&gt; "<strong>(</strong>" &lt;rvalue-expr&gt; "<strong>)</strong>"
&lt;builtin-name&gt; ::= "<strong>shape_of</strong>" | "<strong>length_of</strong>" | "<strong>range_of</strong>"
                 | "<strong>integer</strong>" | "<strong>scalar</strong>" | "<strong>logical</strong>" | "<strong>string</strong>"</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="example-exports">Appendix B: Example Export</h2>
<div class="sectionbody">
<div class="sect2">
<h3 id="_alexnet">B.1. AlexNet</h3>
<div class="paragraph">
<p>The following example shows a complete description of AlexNet as exported from TensorFlow. Note, that the example uses only flat NNEF syntax.</p>
</div>
<div class="listingblock">
<div class="content">
<pre class="highlight"><code>graph AlexNet( input ) -&gt; ( output )
{
    input = external(shape = [1, 3, 224, 224]);
    kernel1 = variable(shape = [64, 3, 11, 11], label = 'alexnet_v2/conv1/kernel');
    bias1 = variable(shape = [1, 64], label = 'alexnet_v2/conv1/bias');
    conv1 = conv(input, kernel1, bias1, padding = [(0,0), (0,0)],
                 border = 'constant', stride = [4, 4], dilation = [1, 1]);
    relu1 = relu(conv1);
    pool1 = max_pool(relu1, size = [1, 1, 3, 3], stride = [1, 1, 2, 2],
                     border = 'ignore', padding = [(0,0), (0,0), (0,0), (0,0)]);
    kernel2 = variable(shape = [192, 64, 5, 5], label = 'alexnet_v2/conv2/kernel');
    bias2 = variable(shape = [1, 192], label = 'alexnet_v2/conv2/bias');
    conv2 = conv(pool1, kernel2, bias2, padding = [(2,2), (2,2)],
                 border = 'constant', stride = [1, 1], dilation = [1, 1]);
    relu2 = relu(conv2);
    pool2 = max_pool(relu2, size = [1, 1, 3, 3], stride = [1, 1, 2, 2],
                     border = 'ignore', padding = [(0,0), (0,0), (0,0), (0,0)]);
    kernel3 = variable(shape = [384, 192, 3, 3], label = 'alexnet_v2/conv3/kernel');
    bias3 = variable(shape = [1, 384], label = 'alexnet_v2/conv3/bias');
    conv3 = conv(pool2, kernel3, bias3, padding = [(1,1), (1,1)],
                 border = 'constant', stride = [1, 1], dilation = [1, 1]);
    relu3 = relu(conv3);
    kernel4 = variable(shape = [384, 384, 3, 3], label = 'alexnet_v2/conv4/kernel');
    bias4 = variable(shape = [1, 384], label = 'alexnet_v2/conv4/bias');
    conv4 = conv(relu3, kernel4, bias4, padding = [(1,1), (1,1)],
                 border = 'constant', stride = [1, 1], dilation = [1, 1]);
    relu4 = relu(conv4);
    kernel5 = variable(shape = [256, 384, 3, 3], label = 'alexnet_v2/conv5/kernel');
    bias5 = variable(shape = [1, 256], label = 'alexnet_v2/conv5/bias');
    conv5 = conv(relu4, kernel5, bias5, padding = [(1,1), (1,1)],
                 border = 'constant', stride = [1, 1], dilation = [1, 1]);
    relu5 = relu(conv5);
    pool3 = max_pool(relu5, size = [1, 1, 3, 3], stride = [1, 1, 2, 2],
                     border = 'ignore', padding = [(0,0), (0,0), (0,0), (0,0)]);
    kernel6 = variable(shape = [4096, 256, 5, 5], label = 'alexnet_v2/fc6/kernel');
    bias6 = variable(shape = [1, 4096], label = 'alexnet_v2/fc6/bias');
    conv6 = conv(pool3, kernel6, bias6, padding = [(0,0), (0,0)],
                 border = 'constant', stride = [1, 1], dilation = [1, 1]);
    relu6 = relu(conv6);
    kernel7 = variable(shape = [4096, 4096, 1, 1], label = 'alexnet_v2/fc7/kernel');
    bias7 = variable(shape = [1, 4096], label = 'alexnet_v2/fc7/bias');
    conv7 = conv(relu6, kernel7, bias7, padding = [(0,0), (0,0)],
                 border = 'constant', stride = [1, 1], dilation = [1, 1]);
    relu7 = relu(conv7);
    kernel8 = variable(shape = [1000, 4096, 1, 1], label = 'alexnet_v2/fc8/kernel');
    bias8 = variable(shape = [1, 1000], label = 'alexnet_v2/fc8/bias');
    conv8 = conv(relu7, kernel8, bias8, padding = [(0,0), (0,0)],
                 border = 'constant', stride = [1, 1], dilation = [1, 1]);
    output = softmax(conv8);
}</code></pre>
</div>
</div>
</div>
</div>
</div>
<div class="sect1">
<h2 id="credits">Appendix C: Credits</h2>
<div class="sectionbody">
<div class="paragraph">
<p>NNEF 1.0 is the result of contributions from many people and companies participating in the Khronos NNEF Working Group, as well as input from the NNEF Advisory Panel.
Members of the Working Group, including the company that they represented at the time of their contributions, are listed below. Some specific contributions made by individuals are listed together with their name.</p>
</div>
<div class="ulist">
<ul>
<li>
<p>Peter McGuinness, independent (Working Group Chair)</p>
</li>
<li>
<p>Frank Brill, Cadence</p>
</li>
<li>
<p>Maciej Urbanski, Intel</p>
</li>
<li>
<p>Ofer Rosenberg, Qualcomm</p>
</li>
<li>
<p>Radha Giduthuri, AMD</p>
</li>
<li>
<p>Sandip Parikh, Cadence</p>
</li>
<li>
<p>Viktor Gyenes, AImotive (Specification Editor, original format proposal)</p>
</li>
<li>
<p>Xin Wang, Verisilicon</p>
</li>
</ul>
</div>
<div class="paragraph">
<p>In addition to the Working Group, the NNEF Advisory Panel members provided important real-world usage information and advice that helped guide design decisions.</p>
</div>
<div class="paragraph">
<p>Administrative support to the Working Group was provided by members of Khronos Group, including Kathleen Mattson. Technical support was provided by James Riordon, webmaster of Khronos.org.</p>
</div>
</div>
</div>
<div class="sect1">
<h2 id="changes">Appendix D: List of Changes</h2>
<div class="sectionbody">
<div class="paragraph">
<p><strong>Version 1.0</strong> (since Provisional):</p>
</div>
<div class="ulist">
<ul>
<li>
<p>renamed <code>extent</code> type to <code>integer</code></p>
</li>
<li>
<p>made semicolon after assignments mandatory</p>
</li>
<li>
<p>depth-wise convolution can be expressed with <code>groups = 0</code></p>
</li>
<li>
<p>added new operations: <code>squeeze</code>, <code>unsqueeze</code>, <code>stack</code>, <code>unstack</code>, <code>slice</code>, <code>argmax_reduce</code>, <code>prelu</code>, <code>RoI</code> operations</p>
</li>
<li>
<p><code>matmul</code> operation generalized to batched version, <code>trA</code> and <code>trB</code> parameters renamed to <code>transposeA</code> and <code>transposeB</code></p>
</li>
<li>
<p>renamed <code>perm</code> parameter of <code>transpose</code> operation to <code>axes</code></p>
</li>
<li>
<p>added <code>epsilon</code> parameter to normalization operations</p>
</li>
<li>
<p>variable labels are restricted to contain limited set of characters allowed in file names, case insensitive comparison</p>
</li>
<li>
<p>added missing negative numeric literals in case of flat syntax</p>
</li>
<li>
<p>changed syntax of array comprehension to let the loop variable be declared before the loop core expression</p>
</li>
<li>
<p>added the <code>in</code> operator for testing containment of items in arrays</p>
</li>
<li>
<p>made parentheses mandatory for tuple literals in rvalue expressions</p>
</li>
<li>
<p>added appendix that contains both grammars in one place</p>
</li>
<li>
<p>added syntax to specify extensions in the structure description</p>
</li>
<li>
<p>revised tensor binary header (fixed size, aligned fields, quantization info is represented with binary fields instead of text)</p>
</li>
<li>
<p>introduced syntax for explicit tensor data-type specification for operations</p>
</li>
<li>
<p>introduced explicit generic tensor data-type syntax</p>
</li>
<li>
<p>clarifications about type system and casting</p>
</li>
<li>
<p>clarified tensor rank definition, added explicit tensor rank pre and post conditions for operations</p>
</li>
<li>
<p>added <code>output_shape</code> parameter to <code>deconv</code>-like operations</p>
</li>
<li>
<p>removed the option of using 0s in the <code>external</code> operation to indicate unknown shapes</p>
</li>
<li>
<p>enhancement of formulas and wording in various places</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Version 1.0.1</strong>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>deprecation of <code>shape_of</code> builtin function</p>
</li>
<li>
<p>deprecation of the use of keyword arguments for tensor arguments in operation invocations (only positional)</p>
</li>
<li>
<p>prohibiting the use of <code>variable</code> and <code>update</code> operation inside fragments</p>
</li>
<li>
<p>prohibiting the declaration of fragments that do not have tensor results</p>
</li>
<li>
<p>restriction of the left-hand-side of assignments to be structurally equivalent to the result type of invoked operations (to let result array sizes be well defined)</p>
</li>
<li>
<p>allow output tensors to be assigned a constant value inside fragments</p>
</li>
<li>
<p>removed appendix of layer and network examples (depended on <code>shape_of</code> function)</p>
</li>
<li>
<p>replaced <code>stack</code>, <code>unstack</code>, <code>squeeze</code>, <code>unsqueeze</code> compound operation definitions with primitives</p>
</li>
<li>
<p>removed appendix that listed the operation mappings (content moved to GitHub)</p>
</li>
<li>
<p>added change-log appendix</p>
</li>
<li>
<p>typo fixes and wording clarifications</p>
</li>
</ul>
</div>
<div class="paragraph">
<p><strong>Version 1.0.2</strong>:</p>
</div>
<div class="ulist">
<ul>
<li>
<p>added unary operations <code>sin</code> and <code>cos</code></p>
</li>
<li>
<p>added reduce operations <code>any_reduce</code> and <code>all_reduce</code></p>
</li>
<li>
<p>added tensor shape manipulator operations <code>pad</code> and <code>tile</code></p>
</li>
<li>
<p>added axis range attributes to <code>reshape</code> operation</p>
</li>
<li>
<p>fixed formula for auto-padding to prevent negative values</p>
</li>
</ul>
</div>
</div>
</div>
</div>
<div id="footer">
<div id="footer-text">
Version 1.0.2, Revision 2<br>
Last updated 2019-09-24 20:20:10 UTC
</div>
</div>
<script type="text/x-mathjax-config">
MathJax.Hub.Config({
  messageStyle: "none",
  tex2jax: {
    inlineMath: [["\\(", "\\)"]],
    displayMath: [["\\[", "\\]"]],
    ignoreClass: "nostem|nolatexmath"
  },
  asciimath2jax: {
    delimiters: [["\\$", "\\$"]],
    ignoreClass: "nostem|noasciimath"
  },
  TeX: { equationNumbers: { autoNumber: "none" } }
})
MathJax.Hub.Register.StartupHook("AsciiMath Jax Ready", function () {
  MathJax.InputJax.AsciiMath.postfilterHooks.Add(function (data, node) {
    if ((node = data.script.parentNode) && (node = node.parentNode) && node.classList.contains('stemblock')) {
      data.math.root.display = "block"
    }
    return data
  })
})
</script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_HTMLorMML"></script>
</body>
</html>